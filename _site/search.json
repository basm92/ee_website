[
  {
    "objectID": "lectureslides.html",
    "href": "lectureslides.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "The lecture slides can be found below.\n\nLecture 1: Probability\nLecture 2: Statistics\nLecture 3: The Linear Model\nLecture 4: Time Series\nLecture 5: Panel Data\nLecture 6: Binary Outcomes\nLecture 7: Potential Outcomes and DiD\nLecture 8: Hand-on Econometrics"
  },
  {
    "objectID": "tutorialslides.html",
    "href": "tutorialslides.html",
    "title": "Tutorial Slides",
    "section": "",
    "text": "The tutorial slides can be found below:\n\nTutorial 1\nTutorial 2\nTutorial 3\nTutorial 8"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#introduction",
    "href": "lectures/lecture1/lecture1.html#introduction",
    "title": "Empirical Economics",
    "section": "Introduction",
    "text": "Introduction\n\nEmpirical Economics\nTwo central aspects:\n\nEconometrics and Econometric Theory\nEmpirical Practice in the form of programming\n\nCentral course objective: to make you understand enough econometric theory, and have you obtain enough experience to :\n\nWrite a succesful thesis (in particular)\nConduct an empirical economics research project (in general)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#course-layout-and-schedule",
    "href": "lectures/lecture1/lecture1.html#course-layout-and-schedule",
    "title": "Empirical Economics",
    "section": "Course Layout and Schedule",
    "text": "Course Layout and Schedule\n\nSimple course organization: 8 Lectures and 8 Tutorials\n\nAlways: 1 lecture (focused on theory) followed by 1 tutorial (recap and practice)\nTutorials: Wednesday and Thursday, 13:15-15:15-17:15 (see mytimetable)\n\nOne mid-term (7 Oct) and one end-term exam (7 Nov)\nOrganized on Canvas\nFeaturing both multiple choice and open questions akin to the tutorials"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#lecture-schedule",
    "href": "lectures/lecture1/lecture1.html#lecture-schedule",
    "title": "Empirical Economics",
    "section": "Lecture Schedule",
    "text": "Lecture Schedule\n\nEverything is in your timetable, but..\nLectures: On Friday\n\nOne Exception: No Lecture on Friday 17 October\nInstead: Tuesday 11 October 11:00\n\nLectures start at:\n\n13:15 (12 Sept, 19 Sept, 3 Oct, 24 October)\n15:15 (5 Sept, 26 Sept, 10 October)\n\nTwo Q&A Sessions Before Mid-term and End-term Exams:\n\nFriday 3 October 15:00 (following lecture)\nFriday 24 October 15:00 (following lecture)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#course-overview",
    "href": "lectures/lecture1/lecture1.html#course-overview",
    "title": "Empirical Economics",
    "section": "Course Overview",
    "text": "Course Overview\n\nStatistics and Probability - Basic Concepts\nStatistics and Probability - Hypothesis Testing\nThe Linear Regression Model\nTime Series Data\nPanel Data (FE) and Control Variables\nBinary Outcome Data\nPotential Outcomes and Difference-in-differences\nHands-on Econometrics in Practice"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#what-do-we-need-to-learn",
    "href": "lectures/lecture1/lecture1.html#what-do-we-need-to-learn",
    "title": "Empirical Economics",
    "section": "What do we need to learn?",
    "text": "What do we need to learn?\n\nFirst two/three lectures devoted to Probability & Statistics\nHow do we model the processes that might have generated our data?\n\nProbability\n\nHow do we summarize and describe data, and try to uncover what process may have generated it?\n\nStatistics\n\nHow do we uncover patterns between variables?\n\nEconometrics"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#correlation-vs.-causation",
    "href": "lectures/lecture1/lecture1.html#correlation-vs.-causation",
    "title": "Empirical Economics",
    "section": "Correlation vs. Causation",
    "text": "Correlation vs. Causation\n\nBanerjee and Duflo (2015) examined the causal effect of a comprehensive anti-poverty program.\n\nCan a “big push” program, which provides a combination of a productive asset, training, and support, have a lasting causal impact on the lives of the ultra-poor?\n\nTo answer this, the researchers used a Randomized Controlled Trial (RCT) across six countries.\n\nA large number of villages were randomly selected to either receive the program (the “treatment group”) or not (the “control group”).\nThis random assignment helps ensure that, on average, the two groups were similar in all other aspects before the program began.\nTherefore, any significant differences observed between the two groups after the program can be causally attributed to the program itself, rather than other factors.\n\nThe study found that, even years after the program ended, the treatment group had:\n\nSignificantly higher consumption levels.\nIncreased income and assets.\nImproved psychological well-being.\n\n\nBecause of the RCT design, the researchers could confidently conclude that the program caused these improvements.\n:::"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#correlation",
    "href": "lectures/lecture1/lecture1.html#correlation",
    "title": "Empirical Economics",
    "section": "Correlation",
    "text": "Correlation\n\nA significant amount of modern finance research focuses on the relationship between a company’s Environmental, Social, and Governance (ESG) scores and its financial performance.\nStudies have documented a positive correlation between high ESG scores and strong financial performance.\n\nCompanies that score well on environmental and social metrics also tend to be more profitable.\n\nIt does not necessarily mean that high ESG scores cause better financial performance. The relationship could be driven by other factors:\n\nReverse Causality: It might be that more profitable and successful firms have the resources to invest in improving their environmental and social impact, which in turn leads to higher ESG scores.\nConfounding Variables: A third factor, such as high-quality management, could be responsible for both high ESG scores and strong financial performance. A well-managed company is likely to be both profitable and attentive to its social and environmental responsibilities.\n\nCorrelations can be spurious, illustrating a key principle in economics: correlation does not imply causation."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#experiments-outcomes-and-sample-spaces",
    "href": "lectures/lecture1/lecture1.html#experiments-outcomes-and-sample-spaces",
    "title": "Empirical Economics",
    "section": "Experiments, Outcomes, and Sample Spaces",
    "text": "Experiments, Outcomes, and Sample Spaces\n\nExperiment: A process or action whose result is uncertain.\n\nExample: Rolling a six-sided die.\nExample: Surveying a household to ask about their income.\nExample: Observing next year’s GDP growth rate.\n\nOutcome (\\(X\\)): A single possible result of an experiment.\n\nExample: The die shows a \\(4\\).\nExample: The household’s income is \\(52,000\\).\nExample: GDP growth is \\(2.3%\\).\n\nSample Space (\\(S\\)): The set of all possible outcomes of an experiment.\n\nExample (Die Roll): \\(S = {1, 2, 3, 4, 5, 6}\\)\nExample (Household Income): \\(S \\in [0, \\infty]\\)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#events",
    "href": "lectures/lecture1/lecture1.html#events",
    "title": "Empirical Economics",
    "section": "Events",
    "text": "Events\n\nEvent: any collection of outcomes (including individual outcomes, the entire sample space, and the null set)\nUsing the die roll example where \\(S = {1, 2, 3, 4, 5, 6}\\), and each outcome is equally likely:\n\n\n\n\n\n\n\n\nExample\n\n\n\nEvent A: The outcome is an even number.\n\n\\(A = {2, 4, 6}\\)\nThe probability of Event A is \\(P(A) = \\frac{3}{6} = 0.5\\)\n\nEvent B: The outcome is greater than 4.\n\n\\(B = {5, 6}\\)\nThe probability of Event B is \\(P(B) = \\frac{2}{6} \\approx 0.33\\)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#intersections-and-unions",
    "href": "lectures/lecture1/lecture1.html#intersections-and-unions",
    "title": "Empirical Economics",
    "section": "Intersections and Unions",
    "text": "Intersections and Unions\n\nThe union of two events \\(A\\) and \\(B\\), denoted \\(A \\cup B\\) is the event that either \\(A\\) occurs, \\(B\\) occurs, or both occur.\n\\[\n  P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\n\nThis accounts for the overlap between \\(A\\) and \\(B\\).\n\nThe intersection of two events \\(A\\) and \\(B\\), denoted \\(P(A \\cap B)\\), is the event that both \\(A\\) and \\(B\\) occur simultaneously.\n\n\n\n\n\n\n\n\nExample\n\n\n\nContinuing the dice example:\nThe intersection of A and B (\\(A \\cap B\\)): The outcome is even AND greater than 4.\n\n\\(A \\cap B = {6}\\)\n\\(P(A \\cap B) = 1 / 6\\)\n\nWhat is the probability of the union of A and B?\n\n\\(A \\cup B\\): The outcome is even OR greater than 4"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#conditional-probability",
    "href": "lectures/lecture1/lecture1.html#conditional-probability",
    "title": "Empirical Economics",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditional Probability is the probability of an event occurring, given that another event has already occurred.\nThe probability of event A occurring given that event B has occurred is written as \\(P(A|B)\\).\n\n\n\n\n\n\n\n\nDefinition: Conditional Probability\n\n\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\n\n\n\n\nIntuition: We are restricting our sample space. We know \\(B\\) happened, so the “universe” of possible outcomes is now just \\(B\\).\nWithin that new universe, we want to know the chance that \\(A\\) also happens."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#conditional-probability-example",
    "href": "lectures/lecture1/lecture1.html#conditional-probability-example",
    "title": "Empirical Economics",
    "section": "Conditional Probability: Example",
    "text": "Conditional Probability: Example\n\nLet’s use our die roll example again:\n\n\n\n\n\n\n\n\nExample\n\n\n\\(S = {1, 2, 3, 4, 5, 6}\\), \\(A = \\{\\text{Outcome is an even number}\\} = \\{2, 4, 6\\}\\), \\(B = \\{\\text{Outcome is greater than 3}\\} = \\{4, 5, 6\\}\\)\nQuestion: What is the probability that the number is even, given that we know it is greater than 3? We want to find \\(P(A|B)\\).\n\nFind \\(P(B)\\): The probability of rolling a number greater than 3 is \\(P(B) = 3/6\\).\nFind \\(P(A \\cap B)\\): The probability of rolling a number that is even AND greater than 4 is \\(P({5, 6}) = 2/6\\).\nApply the formula: \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{2/6}{3/6} = \\frac{2}{3} \\approx 0.67\\]"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#conditional-probability-graphically",
    "href": "lectures/lecture1/lecture1.html#conditional-probability-graphically",
    "title": "Empirical Economics",
    "section": "Conditional Probability (Graphically)",
    "text": "Conditional Probability (Graphically)\n\nIntuition Check: If we know the outcome is in \\(B = {4, 5, 6}\\), there are only two possibilities. Of these, only two (\\(4, 6\\)) are even. So the probability is \\(2/3\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#random-variables",
    "href": "lectures/lecture1/lecture1.html#random-variables",
    "title": "Empirical Economics",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom Variable (RV): A variable whose value is a numerical outcome of an experiment. We use capital letters (e.g., \\(X\\), \\(Y\\)) to denote a random variable.\nThere are two main types of random variables:\nDiscrete Random Variable: A variable that can only take on a finite or countably infinite number of distinct values.\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe number of heads in three coin flips (\\(X\\) can be 0, 1, 2, 3).\nThe number of defaults in a portfolio of 100 loans (\\(X\\) can be 0, 1, …, 100)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#random-variables-1",
    "href": "lectures/lecture1/lecture1.html#random-variables-1",
    "title": "Empirical Economics",
    "section": "Random Variables",
    "text": "Random Variables\n\nContinuous Random Variable: A variable that can take on any value within a given range.\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe exact price of a stock tomorrow.\nThe annual percentage growth in GDP (\\(Y\\) could be 2.1%, 2.11%, 2.113%…)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#probability-mass-function",
    "href": "lectures/lecture1/lecture1.html#probability-mass-function",
    "title": "Empirical Economics",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\nFor a discrete random variable \\(X\\), the Probability Mass Function (PMF) gives the probability that \\(X\\) is exactly equal to some value \\(x\\).\n\n\n\n\n\n\n\n\nDefinition: Probability Mass Function\n\n\n\\[\nf(x) = P(X = x)\n\\]\n\n\n\n\n\nA PMF has two key properties:\n\n\\(0 \\leq f(x) \\leq 1\\) for all \\(x\\).\n\\(\\sum f(x) = 1\\) (The sum of probabilities over all possible values is 1)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#pmf-example",
    "href": "lectures/lecture1/lecture1.html#pmf-example",
    "title": "Empirical Economics",
    "section": "PMF Example",
    "text": "PMF Example\n\n\n\n\n\n\n\nExample Probability Mass Function\n\n\nLet \\(X\\) be the outcome of a fair die roll. The PMF is: \\(f(1) = 1/6\\), \\(f(2) = 1/6\\), …, \\(f(6) = 1/6\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#expected-value",
    "href": "lectures/lecture1/lecture1.html#expected-value",
    "title": "Empirical Economics",
    "section": "Expected Value",
    "text": "Expected Value\n\nThe Expected Value of a discrete random variable \\(X\\), denoted \\(E[X]\\) or \\(\\mu\\), is the long-run average value of the variable.\n\nIt’s a weighted average of the possible outcomes, where the weights are the probabilities.\n\n\n\n\n\n\n\n\n\nDefinition: Expected Value\n\n\n\\[\nE[X] = \\mu = \\sum_x x \\cdot P(X=x)\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample: Expected value of a fair die roll\n\n\n\\(E[X] = (1 \\times {1 \\over 6}) + (2 \\times {1 \\over 6}) + (3 \\times {1 \\over 6}) + (4 \\times {1 \\over 6}) + (5 \\times {1 \\over 6}) + (6 \\times {1 \\over 6})\\) \\(\\hspace{2.5em} = (1+2+3+4+5+6) / 6 = 21 / 6 = 3.5\\)\nNote: The expected value doesn’t have to be a possible outcome!"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#variance-and-standard-deviation",
    "href": "lectures/lecture1/lecture1.html#variance-and-standard-deviation",
    "title": "Empirical Economics",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nVariance, denoted \\(Var(X)\\) or \\(\\sigma^2\\), measures the spread or dispersion of a random variable around its mean. A larger variance means the outcomes are more spread out.\n\n\n\n\n\n\n\n\nDefinition: Variance\n\n\n\\[\nVar(X) = \\sigma^2 = E[(X - \\mu)^2] = \\sum_x (x-\\mu)^2 \\cdot P(X=x)\n\\]\n\n\n\n\n\nStandard Deviation, \\(SD(X)\\) or \\(\\sigma\\), is the square root of the variance. It’s often easier to interpret because it’s in the same units as the random variable itself.\n\n\\[SD(X) = \\sigma = \\sqrt{Var(X)}\\]"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#example-the-bernoulli-distribution",
    "href": "lectures/lecture1/lecture1.html#example-the-bernoulli-distribution",
    "title": "Empirical Economics",
    "section": "Example: The Bernoulli Distribution",
    "text": "Example: The Bernoulli Distribution\n\n\n\n\n\n\n\nExample: Bernoulli\n\n\nThe Bernoulli distribution is a fundamental discrete distribution for any experiment with two outcomes, typically labeled “success” (1) and “failure” (0).\nLet \\(X\\) be a Bernoulli random variable where \\(P(X=1) = p\\) and \\(P(X=0) = 1-p\\). A Bernoulli distribution models binary outcomes like employed/unemployed, default/no-default, buy/don’t-buy.\n\nExpected Value: \\(E[X] = (1 \\times p) + (0 \\times (1-p)) = p\\)\nVariance: \\[\\begin{align*}\nVar(X) &= (1-p)^2 \\times p + (0-p)^2 \\times (1-p) \\\\\n\\qquad &= (1-p)^2 \\times p + p^2 \\times (1-p) \\\\\n\\qquad &= p(1-p) \\times [(1-p) + p] = p(1-p)\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#independence",
    "href": "lectures/lecture1/lecture1.html#independence",
    "title": "Empirical Economics",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\n\nIndependence\n\n\nTwo events A and B are independent if:\n\\[\nP(A \\cap B) = P(A) \\times P(B)\n\\]\nor equivalently:\n\\[\nP(A|B) = P(A)\n\\]\n\n\n\n\n\nIndependence means one event doesn’t affect the other’s probability\nMutually exclusive \\(\\neq\\) independent (actually, mutually exclusive events with positive probability are dependent)\nIndependence can extend to more than two events"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#independence-example",
    "href": "lectures/lecture1/lecture1.html#independence-example",
    "title": "Empirical Economics",
    "section": "Independence: Example",
    "text": "Independence: Example\n\n\n\n\n\n\n\nExample: Coin Flip\n\n\n\nConsider flipping a fair coin twice:\n\nLet A = “First toss is Heads”, \\(P(A) = \\frac{1}{2}\\)\nLet B = “Second toss is Tails” \\(P(B) = \\frac{1}{2}\\)\n\\(P(A \\cap B) = P(HT) = 0.25\\)\n\nSince \\({1 \\over 4} ={1 \\over 2} \\times {1 \\over 2}\\), A and B are independent\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Coin Flip (2)\n\n\n\nConsider flipping a fair coin twice:\n\nLet \\(A\\) = “First flip is Heads”, \\(\\{HH, HT\\}\\), \\(P(A)=\\frac{1}{2}\\).\nLet \\(C\\) = “At least one Head” \\(\\{HH,HT,TH\\}\\), \\(P(C)=\\frac{3}{4}\\)\n\n\\(P(A \\cap C)=P({HH, HT})=\\frac{1}{2}\\). But \\(P(A) \\cdot P(C)=\\frac{1}{2} \\cdot \\frac{3}{4}=\\frac{3}{8}\\). Since \\(\\frac{1}{2}\\neq\\frac{3}{8}\\), \\(A\\) and \\(C\\) are dependent."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#probability-density-function-pdf",
    "href": "lectures/lecture1/lecture1.html#probability-density-function-pdf",
    "title": "Empirical Economics",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\n\nFor a continuous random variable, the probability of it taking on any single specific value is zero! \\(P(X = x) = 0\\).\nWhy? Because there are infinitely many possible values.\nInstead, we use a Probability Density Function (PDF), \\(f(x)\\).\nKey Idea: Probability is represented by the area under the curve of the PDF.\n\n\n\n\n\n\n\n\nDefinition: PDF (Informal)\n\n\n\\[\nP(a \\leq X \\leq b) = \\text{ Area under $f(x)$ between $a$ and $b$}\n\\]"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#example-pdf",
    "href": "lectures/lecture1/lecture1.html#example-pdf",
    "title": "Empirical Economics",
    "section": "Example PDF",
    "text": "Example PDF\n\n\n\n\n\n\n\nExample PDF"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#cumulative-distribution-function-cdf",
    "href": "lectures/lecture1/lecture1.html#cumulative-distribution-function-cdf",
    "title": "Empirical Economics",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\n\nThe Cumulative Distribution Function (CDF), \\(F(x)\\), gives the probability that a random variable \\(X\\) is less than or equal to a certain value \\(x\\).\n\nIt’s a unifying concept for both discrete and continuous variables.\n\n\n\n\n\n\n\n\n\nDefinition: CDF\n\n\n\\[F(x) = P(X \\le x)\\]\n\n\n\n\n\nProperties:\n\n\\(F(x)\\) is non-decreasing.\n\\(F(x)\\) ranges from 0 to 1.\nFor continuous RVs, \\(P(a \\leq X \\leq b) = F(b) - F(a)\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#example-cdf",
    "href": "lectures/lecture1/lecture1.html#example-cdf",
    "title": "Empirical Economics",
    "section": "Example CDF",
    "text": "Example CDF\n\n\n\n\n\n\n\nExample: CDF"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#quantiles",
    "href": "lectures/lecture1/lecture1.html#quantiles",
    "title": "Empirical Economics",
    "section": "Quantiles",
    "text": "Quantiles\n\nQuantiles are cut points that divide the range of a probability distribution into continuous intervals with equal probabilities.\n\n\n\n\n\n\n\n\nDefinition: Quantile\n\n\nThe \\(\\tau\\)-th quantile (where \\(0 &lt; \\tau &lt; 1\\)) of a random variable \\(X\\), is the value \\(x\\) such that the probability of the random variable being less than or equal to \\(x\\) is \\(\\tau\\).\nMathematically, this is expressed using the Cumulative Distribution Function (CDF), \\(F(x)\\):\n\\[\nP(X \\leq Q(\\tau)) = F(Q(\\tau)) = \\tau\n\\] This means the quantile function, \\(Q(\\tau)\\), is the inverse of the CDF: \\(Q(\\tau)=F^{-1}(\\tau)\\)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#quantiles-visualization",
    "href": "lectures/lecture1/lecture1.html#quantiles-visualization",
    "title": "Empirical Economics",
    "section": "Quantiles: Visualization",
    "text": "Quantiles: Visualization"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#quantiles-via-the-cdf",
    "href": "lectures/lecture1/lecture1.html#quantiles-via-the-cdf",
    "title": "Empirical Economics",
    "section": "Quantiles via the CDF",
    "text": "Quantiles via the CDF"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#the-normal-distribution-1",
    "href": "lectures/lecture1/lecture1.html#the-normal-distribution-1",
    "title": "Empirical Economics",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\n\nThe Normal Distribution is the most important probability distribution in statistics and econometrics.\nIt is defined by its mean \\(\\mu\\) and its variance \\(\\sigma^2\\).\nWe write \\(X \\sim N(\\mu, \\sigma^2)\\).\nProperties:\n\nBell-shaped and symmetric around its mean \\(\\mu\\).\nMean = Median = Mode.\nThe curve is completely determined by \\(\\mu\\) (center) and \\(\\sigma\\) (spread)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#example-normal-distribution",
    "href": "lectures/lecture1/lecture1.html#example-normal-distribution",
    "title": "Empirical Economics",
    "section": "Example Normal Distribution",
    "text": "Example Normal Distribution\n\n\n\n\n\n\n\nExample: Normal Distributions"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#linear-combinations-of-normal-variables",
    "href": "lectures/lecture1/lecture1.html#linear-combinations-of-normal-variables",
    "title": "Empirical Economics",
    "section": "Linear Combinations of Normal Variables",
    "text": "Linear Combinations of Normal Variables\n\nAn important property of the normal distribution is that linear combinations of independent normal variables are also normally distributed.\n\n\n\n\n\n\n\n\nTheorem: Transformations of Normal Variables\n\n\n\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then the new variable \\(Y = aX + b\\) is also normally distributed:\n\n\\[Y \\sim N(a\\mu + b, a^2\\sigma^2)\\] Note that the new standard deviation is \\(|a|\\sigma\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#linear-combinations-of-normal-variables-1",
    "href": "lectures/lecture1/lecture1.html#linear-combinations-of-normal-variables-1",
    "title": "Empirical Economics",
    "section": "Linear Combinations of Normal Variables",
    "text": "Linear Combinations of Normal Variables\n\nIn the special case of independent normal variables:\n\n\n\n\n\n\n\n\nSum/Difference of Independent Variables\n\n\nIf \\(X \\sim N(\\mu_X, \\sigma_X^2)\\) and \\(Y \\sim N(\\mu_Y, \\sigma_Y^2)\\) are independent, then their sum and difference are also normally distributed: \\[ X + Y \\sim N(\\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2) \\] \\[ X - Y \\sim N(\\mu_X - \\mu_Y, \\sigma_X^2 + \\sigma_Y^2) \\]\n\n\n\n\n\nKey point: Variances always add, even when subtracting the random variables."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#example",
    "href": "lectures/lecture1/lecture1.html#example",
    "title": "Empirical Economics",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nExample: Normally Distributed Tasks\n\n\n\nLet the time to complete Task A be \\(T_A \\sim N(20, 3^2)\\) minutes and Task B be \\(T_B \\sim N(15, 4^2)\\) minutes. What is the distribution of the total time \\(T_{total} = T_A + T_B\\)?\n\nNew Mean: \\(\\mu_{total} = \\mu_A + \\mu_B = 20 + 15 = 35\\)\nNew Variance: \\(\\sigma^2_{total} = \\sigma_A^2 + \\sigma_B^2 = 3^2 + 4^2 = 9 + 16 = 25\\)\nNew Standard Deviation: \\(\\sigma_{total} = \\sqrt{25} = 5\\)\n\nSo, \\(T_{total} \\sim N(35, 5^2)\\). We can now calculate probabilities for the total time, e.g., the probability the total time is less than 45 minutes:\n\n\nRPythonStata\n\n\n\npnorm(45, mean = 35, sd = 5)\n\n[1] 0.9772499\n\n\n\n\n\nfrom scipy.stats import norm\nresult = norm.cdf(45, loc=35, scale=5)\nprint(result)\n\n0.9772498680518208\n\n\n\n\n\ndisplay normal((45 - 35)/5)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#the-standard-normal-distribution-z",
    "href": "lectures/lecture1/lecture1.html#the-standard-normal-distribution-z",
    "title": "Empirical Economics",
    "section": "The Standard Normal Distribution (Z)",
    "text": "The Standard Normal Distribution (Z)\n\nThe Standard Normal Distribution is a special case of the normal distribution with a mean of 0 and a variance of 1. \\(Z \\sim N(0, 1)\\).\n\n\n\n\n\n\n\n\nDefinition: Standardization\n\n\nWe can convert any normally distributed random variable \\(X \\sim N(\\mu, \\sigma^2)\\) into a standard normal variable \\(Z\\) using the formula:\n\\[ Z = \\frac{X - \\mu}{\\sigma} \\]\n\n\n\n\n\nIt allows us to use a single table (or software function) to find probabilities for any normal distribution.\nThe Z-score tells us how many standard deviations an observation \\(X\\) is away from its mean \\(\\mu\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#finding-probabilities",
    "href": "lectures/lecture1/lecture1.html#finding-probabilities",
    "title": "Empirical Economics",
    "section": "Finding Probabilities",
    "text": "Finding Probabilities\n\nHistorically, probabilities for the standard normal distribution were found using Z-tables, which provide \\(P(Z \\leq z)\\).\nToday, we use software like R, Stata, or Python.\n\n\n\n\n\n\n\n\nFinding Normal Probabilities\n\n\nExample: Suppose annual returns on a mutual fund are normally distributed with a mean of 8% and a standard deviation of 10%. \\(X \\sim N(0.08, 0.01)\\). What’s the probability of a negative return, \\(P(X &lt; 0)\\)?\n\nStandardize the value: \\(Z = (0 - 0.08) / 0.10 = -0.8\\)\nFind the probability: We need to find \\(P(X &lt; 0) = P(\\frac{X - 0.08}{0.01} &lt; \\frac{0-0.08}{0.01}) = P(Z &lt; -0.8)\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#finding-probabilities-cont.",
    "href": "lectures/lecture1/lecture1.html#finding-probabilities-cont.",
    "title": "Empirical Economics",
    "section": "Finding Probabilities (Cont.)",
    "text": "Finding Probabilities (Cont.)\n\n\n\n\n\n\n\nFinding Normal Probabilities with Software\n\n\nUsing R, Python or Stata: The pnorm() (R) and norm.cdf (Python) functions give the area to the left of the provided value (the CDF).\n\nRPythonStata\n\n\n\npnorm(-0.8, mean = 0, sd = 1)\n\n[1] 0.2118554\n\n\n\n\n\nfrom scipy.stats import norm\nnorm.cdf(-0.8)\n\n0.2118553985833967\n\n\n\n\n\n\n\n\nSo, there is about a 21.2% chance of experiencing a negative return."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#conditional-probability-1",
    "href": "lectures/lecture1/lecture1.html#conditional-probability-1",
    "title": "Empirical Economics",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditional probability is about how the probability of an event \\(A\\) changes when we know an event \\(B\\) has occurred.\nDiscrete Case: We update probabilities for specific outcomes. \\[\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nContinuous Case: What if we want to condition on a continuous random variable \\(Y\\) taking a specific value \\(y\\)?\nSince \\(P(Y=y) = 0\\) for any continuous variable, the formula above is undefined.\nWe need to shift our thinking from the probability of events to the probability density functions (PDFs) of random variables."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#conditional-probability-density-functions-pdfs",
    "href": "lectures/lecture1/lecture1.html#conditional-probability-density-functions-pdfs",
    "title": "Empirical Economics",
    "section": "Conditional Probability Density Functions (PDFs)",
    "text": "Conditional Probability Density Functions (PDFs)\n\n\n\n\n\n\n\nDefinition: Conditional PDF\n\n\nThe conditional PDF of a random variable \\(X\\) given that \\(Y=y\\) is defined as the ratio of the joint PDF to the marginal PDF of \\(Y\\).\n\\[\nf_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]\nProvided that the marginal density \\(f_Y(y) &gt; 0\\).\nSimilarly, \\(f_{X,Y}(x,y)\\), the joint PDF, describes the probability density of \\((X, Y)\\) occurring together.\n\\(f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx\\): The marginal PDF of \\(Y\\) found by “integrating out” \\(X\\). It represents the distribution of \\(Y\\) on its own."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#intuition-slicing-the-joint-distribution",
    "href": "lectures/lecture1/lecture1.html#intuition-slicing-the-joint-distribution",
    "title": "Empirical Economics",
    "section": "Intuition: Slicing the Joint Distribution",
    "text": "Intuition: Slicing the Joint Distribution\n\nThink of the joint PDF \\(f_{X,Y}(x,y)\\) as a 3D surface.\n\nFix a value \\(y\\) for the variable \\(Y\\). This is like taking a 2D “slice” of the 3D surface at that \\(y\\).\nThe shape of this slice gives the relative likelihood of \\(X\\)’s values, given that \\(Y=y\\).\nNormalize the slice: The area under this slice might not be 1. Dividing by \\(f_Y(y)\\) (the area of the slice) scales it to become a valid probability density function.\n\nThis normalized slice is the conditional PDF, \\(f_{X|Y}(x|y)\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#example-visualization",
    "href": "lectures/lecture1/lecture1.html#example-visualization",
    "title": "Empirical Economics",
    "section": "Example: Visualization",
    "text": "Example: Visualization\n\n\n\n\n\n\n\nVisualization of the Conditional PDF"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#conditional-expectation",
    "href": "lectures/lecture1/lecture1.html#conditional-expectation",
    "title": "Empirical Economics",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\nConditional expectation, \\(E[X | Y=y]\\), is simply the expected value (the mean) of X calculated using its conditional distribution.\n\nIt’s the “center of mass” of that conditional slice we just discussed.\n\n\n\n\n\n\n\n\n\nDefinition: Conditional Expectation\n\n\nDiscrete case: if X and Y are discrete random variables, the expectation of X given Y=y is a weighted average using conditional probabilities:\n\\[\nE[X | Y=y] = \\sum_{x} x \\cdot P(X=x | Y=y)\n\\]\nContinuous case: if X and Y are continuous random variables, the expectation is an integral using the conditional PDF:\n\\[\nE[X | Y=y] = \\int_{-\\infty}^{\\infty} x \\cdot f_{X|Y}(x|y) \\, dx\n\\]"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#a-crucial-distinction-exyy-vs.-exy",
    "href": "lectures/lecture1/lecture1.html#a-crucial-distinction-exyy-vs.-exy",
    "title": "Empirical Economics",
    "section": "A Crucial Distinction: \\(E[X|Y=y]\\) vs. \\(E[X|Y]\\)",
    "text": "A Crucial Distinction: \\(E[X|Y=y]\\) vs. \\(E[X|Y]\\)\n\n\\(E[X|Y=y]\\) is a function\nThe expression \\(E[X|Y=y]\\) produces a value that depends on the specific, fixed y we conditioned on. We can think of this as a function of \\(y\\).\n\n\n\n\n\n\n\n\nExample \\(E[X|Y=y]\\)\n\n\n\nLet’s define a function \\(g(y)\\): \\[ g(y) = E[X|Y=y] \\]\nIf we know Y=2, we calculate \\(g(2) = E[X|Y=2]\\).\nIf we know Y=5, we calculate \\(g(5) = E[X|Y=5]\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#exy-is-a-random-variable",
    "href": "lectures/lecture1/lecture1.html#exy-is-a-random-variable",
    "title": "Empirical Economics",
    "section": "\\(E[X|Y]\\) is a Random Variable",
    "text": "\\(E[X|Y]\\) is a Random Variable\n\nThe notation \\(E[X|Y]\\) (without specifying a value for \\(Y\\)) represents a new random variable.\nIt is the random variable you get by taking the function \\(g(y)\\) and plugging in the random variable \\(Y\\) itself. \\[ E[X|Y] = g(Y) \\]\nThe value of this new random variable is not known until the value of \\(Y\\) is revealed.\n\n\n\n\n\n\n\n\nExample \\(E[X|Y]\\)\n\n\nSuppose we find that the expected height of a child \\(X\\) given the mother’s height \\(Y\\) is \\(E[X|Y=y] = 40 + 0.8y\\).\n\n\\(g(y) = 40 + 0.8y\\) is the function.\n\\(E[X|Y] = 40 + 0.8Y\\) is a random variable whose outcome depends on the randomly selected mother’s height \\(Y\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#covariance",
    "href": "lectures/lecture1/lecture1.html#covariance",
    "title": "Empirical Economics",
    "section": "Covariance",
    "text": "Covariance\n\nCovariance measures the joint variability of two random variables. It tells us the direction of the linear relationship.\n\n\n\n\n\n\n\n\nDefinition: Covariance\n\n\n\\[\nCov(X, Y) = \\sigma_{XY} = E[(X-\\mu_X)(Y-\\mu_Y)]\n\\]\n\n\\(Cov(X, Y) &gt; 0\\): \\(X\\) and \\(Y\\) tend to move in the same direction. When \\(X\\) is above its mean, \\(Y\\) tends to be above its mean.\n\\(Cov(X, Y) &lt; 0\\): \\(X\\) and \\(Y\\) tend to move in opposite directions.\n\\(Cov(X, Y) = 0\\): No linear relationship between \\(X\\) and \\(Y\\).\n\n\n\n\n\n\nDrawback: The magnitude of covariance is hard to interpret because it depends on the units of \\(X\\) and \\(Y\\). (e.g., Cov(GDP, Consumption) will be a huge number)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#correlation-1",
    "href": "lectures/lecture1/lecture1.html#correlation-1",
    "title": "Empirical Economics",
    "section": "Correlation",
    "text": "Correlation\n\nThe Correlation Coefficient (\\(\\rho\\) or \\(r\\)) is a standardized version of covariance that measures both the strength and direction of the linear relationship between two variables.\n\n\n\n\n\n\n\n\nDefinition: Correlation\n\n\n\\[\n\\rho_{XY} = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\n\n\n\n\n\nProperties:\n\nRanges from -1 to +1.\n\\(\\rho = +1\\): Perfect positive linear relationship.\n\\(\\rho = -1\\): Perfect negative linear relationship.\n\\(\\rho = 0\\): No linear relationship.\nIt is unitless, making it easy to interpret and compare."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#example-correlation",
    "href": "lectures/lecture1/lecture1.html#example-correlation",
    "title": "Empirical Economics",
    "section": "Example: Correlation",
    "text": "Example: Correlation\n\n\n\n\n\n\n\nExample: Correlations"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#correlation-neq-causation",
    "href": "lectures/lecture1/lecture1.html#correlation-neq-causation",
    "title": "Empirical Economics",
    "section": "Correlation \\(\\neq\\) Causation",
    "text": "Correlation \\(\\neq\\) Causation\n\nReminder: a strong correlation between two variables does not mean that one causes the other. There could be:\n\nReverse Causality: \\(Y\\) causes \\(X\\).\nOmitted Variable Bias (Lurking Variable): A third variable \\(Z\\) causes both \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\nExample: Correlation \\(\\neq\\) Causation\n\n\n\nIce cream sales (\\(X\\)) and drowning deaths (\\(Y\\)) are highly positively correlated.\n\nDoes eating ice cream cause drowning? No.\nThe omitted variable is hot weather (\\(Z\\)), which causes people to both buy more ice cream and swim more (leading to more drownings)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#expected-value-1",
    "href": "lectures/lecture1/lecture1.html#expected-value-1",
    "title": "Empirical Economics",
    "section": "Expected Value",
    "text": "Expected Value\n\nThese rules are fundamental and apply to all random variables, discrete or continuous.\n\n\n\n\n\n\n\n\nTheorem: Linearity of Expectation\n\n\nLet \\(X\\) and \\(Y\\) be two random variables with means \\(\\mu_X\\) and \\(\\mu_Y\\), and variances \\(\\sigma_X^2\\) and \\(\\sigma_Y^2\\). Let \\(a\\) and \\(b\\) be constants.\nThe expectation of a linear combination is the linear combination of the expectations.\n\\[\nE[aX + bY] = aE[X] + bE[Y]\n\\]\n\n\n\n\n\nThis rule holds regardless of whether \\(X\\) and \\(Y\\) are independent. Expectations are always additive/subtractive in this straightforward way."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#variance",
    "href": "lectures/lecture1/lecture1.html#variance",
    "title": "Empirical Economics",
    "section": "Variance",
    "text": "Variance\n\nThe rule for variance includes a covariance term.\n\n\n\n\n\n\n\n\nTheorem: Sum of Variances\n\n\nThe sum of the variances equals:\n\\[\n\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2ab\\,\\text{Cov}(X, Y)\n\\]\nIf \\(X\\) and \\(Y\\) are independent random variables, then their covariance is zero (\\(\\text{Cov}(X, Y) = 0\\)). The formula simplifies significantly:\n\\[\n\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y)\n\\]\n\n\n\n\n\nSimple Cases (for Independent Variables):\n\nSum: \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\nDifference: \\(\\text{Var}(X - Y) = \\text{Var}(X) + \\text{Var}(Y)\\)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#covariance-1",
    "href": "lectures/lecture1/lecture1.html#covariance-1",
    "title": "Empirical Economics",
    "section": "Covariance",
    "text": "Covariance\n\nCovariance of sums can also be decomposed as sums of covariances.\n\n\n\n\n\n\n\n\nSums of Covariances\n\n\nThe covariance can be decomposed as: \\[\n\\text{Cov}(aX + bY, Z) = a\\,\\text{Cov}(X, Z) + b\\,\\text{Cov}(Y, Z)\n\\]\nIn addition, the covariance of \\(X\\) with itself equals the variance:\n\\[\n\\text{Cov}(X, X) = \\text{Var}(X)\n\\]"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#covariance-example",
    "href": "lectures/lecture1/lecture1.html#covariance-example",
    "title": "Empirical Economics",
    "section": "Covariance Example",
    "text": "Covariance Example\n\n\n\n\n\n\n\nExample: Proof that \\(\\text{Var}(X+Y)=\\text{Cov}(X+Y, X+Y)\\)\n\n\nWe know that \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2 \\text{Cov}(X,Y)\\) by definition.\nNow observe that:\n\\[\\begin{align*}\n\\text{Cov}(X+Y, X+Y) &= \\text{Cov}(X,X+Y)+\\text{Cov}(Y,X+Y) \\newline\n  &= \\text{Var}(X)+\\text{Cov}(X,Y)+\\text{Cov}(Y,X)+\\text{Var}(Y) \\newline\n  &= \\text{Var}(X)+\\text{Var}(Y)+2\\text{Cov}(X,Y)\n\\end{align*}\\]\n..which is equal to \\(\\text{Var}(X+Y)\\)."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#summary-table",
    "href": "lectures/lecture1/lecture1.html#summary-table",
    "title": "Empirical Economics",
    "section": "Summary Table",
    "text": "Summary Table\nThe following table outlines the results previously discussed:\n\n\n\n\n\n\n\n\n\n\nProperty\nLinear Combination\nGeneral Case\nIndependent Case (\\(Cov(X,Y)=0\\))\n\n\n\n\nExpectation\n\\(E[aX \\pm bY]\\)\n\\(a\\mu_X \\pm b\\mu_Y\\)\n(Same as General)\n\n\n\n\\(E[X + Y]\\)\n\\(\\mu_X + \\mu_Y\\)\n(Same as General)\n\n\n\n\\(E[X - Y]\\)\n\\(\\mu_X - \\mu_Y\\)\n(Same as General)\n\n\nVariance\n\\(\\text{Var}(aX + bY)\\)\n\\(a^2\\sigma_X^2 + b^2\\sigma_Y^2 + 2ab\\,\\text{Cov}(X,Y)\\)\n\\(a^2\\sigma_X^2 + b^2\\sigma_Y^2\\)\n\n\n\n\\(\\text{Var}(X + Y)\\)\n\\(\\sigma_X^2 + \\sigma_Y^2 + 2\\,\\text{Cov}(X,Y)\\)\n\\(\\sigma_X^2 + \\sigma_Y^2\\)\n\n\n\n\\(\\text{Var}(X - Y)\\)\n\\(\\sigma_X^2 + \\sigma_Y^2 - 2\\,\\text{Cov}(X,Y)\\)\n\\(\\sigma_X^2 + \\sigma_Y^2\\)"
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#what-did-we-do",
    "href": "lectures/lecture1/lecture1.html#what-did-we-do",
    "title": "Empirical Economics",
    "section": "What did we do?",
    "text": "What did we do?\n\nProbability Concepts:\n\nThe lecture began by introducing foundational probability concepts, including experiments, sample spaces, events, and the rules for calculating probabilities of unions, intersections, and conditional events (\\(P(A|B)\\)).\n\nDicsrete and Continuous Distributions:\n\nWe defined discrete and continuous random variables and their key descriptive functions: the Probability Mass/Density Function (PMF/PDF) and the Cumulative Distribution Function (CDF). This section also covered how to calculate the expected value (\\(E[X]\\)) and variance (\\(Var(X)\\)) of a random variable.\n\nThe Normal Distribution:\n\nA significant portion was dedicated to the Normal distribution, explaining its properties, the importance of its parameters (mean and variance), and how any normal variable can be standardized into a Z-score. It also covered the properties of linear combinations of normal variables."
  },
  {
    "objectID": "lectures/lecture1/lecture1.html#what-did-we-do-cont.",
    "href": "lectures/lecture1/lecture1.html#what-did-we-do-cont.",
    "title": "Empirical Economics",
    "section": "What did we do? (Cont.)",
    "text": "What did we do? (Cont.)\n\nConditional Distributions:\n\nThe relationship between variables was explored through multiple lenses, including conditional probability density functions (\\(f_{X|Y}(x|y)\\)), conditional expectation (\\(E[X|Y]\\)), and measures of linear association like covariance and correlation, while reiterating the crucial distinction that correlation does not imply causation.\n\nRules for Expected Value and Variance:\n\nFinally, the lecture established the mathematical rules for the expectation and variance of sums of random variables. It detailed the linearity of expectation and the formula for the variance of a sum, emphasizing the simplification that occurs when the variables are independent and their covariance is zero."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#course-overview",
    "href": "lectures/lecture2/lecture2.html#course-overview",
    "title": "Empirical Economics",
    "section": "Course Overview",
    "text": "Course Overview\n\nStatistics and Probability - Basic Concepts\nStatistics and Probability - Hypothesis Testing\nThe Linear Regression Model\nTime Series Data\nPanel Data (FE) and Control Variables\nBinary Outcome Data\nPotential Outcomes and Difference-in-differences\nHands-on Econometrics in Practice"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#what-do-we-do-today",
    "href": "lectures/lecture2/lecture2.html#what-do-we-do-today",
    "title": "Empirical Economics",
    "section": "What do we do today?",
    "text": "What do we do today?\n\nFirst two/three lectures devoted to Probability & Statistics\nLecture 1:\n\nHow do we model the processes that might have generated our data?\nProbability\n\nLecture 2:\n\nHow do we summarize and describe data, and try to uncover what process may have generated it?\nStatistics\n\nRemaining lectures:\n\nHow do we uncover patterns between variables?\nEconometrics"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#population-vs.-sample",
    "href": "lectures/lecture2/lecture2.html#population-vs.-sample",
    "title": "Empirical Economics",
    "section": "Population vs. Sample",
    "text": "Population vs. Sample\n\n\nIn the previous lecture, we have seen concepts like expected values, \\(E[X]\\) or \\(\\mu_X\\), and variances \\(\\sigma^2\\).\nThese are parameters that belong to a population: the entire group of individuals, objects, or data points that we are interested in studying.\nIn reality, we usually only have a sample, a subset of the population from which we actually collect data, at our disposal.\n\nIt’s often impossible or too expensive to collect data on the entire population. We use samples to make inferences about the population.\n\n\n\n\n\n\n\n\n\n\nExample Populations and Samples\n\n\n\nPopulations:\n\nAll households in the United States.\nAll firms listed on the New York Stock Exchange.\n\nSamples:\n\nA survey of 2,000 U.S. households.\nThe stock prices of 50 firms from the NYSE."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#parameters-vs.-statistics",
    "href": "lectures/lecture2/lecture2.html#parameters-vs.-statistics",
    "title": "Empirical Economics",
    "section": "Parameters vs. Statistics",
    "text": "Parameters vs. Statistics\n\nThe core idea of inference is to use a sample statistic to learn about a population parameter.\n\n\n\n\n\n\n\n\nDefinition: Parameter\n\n\nA parameter is a numerical characteristic of a population. These are typically unknown and what we want to estimate. They are considered fixed values.\n\n\n\n\n\n\n\n\n\n\n\nExample: Parameters\n\n\nThe population mean (\\(\\mu\\)), population variance (\\(\\sigma^2\\)), population correlation (\\(\\rho\\))."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#statistics",
    "href": "lectures/lecture2/lecture2.html#statistics",
    "title": "Empirical Economics",
    "section": "Statistics",
    "text": "Statistics\n\nThe core idea of inference is to use a sample statistic to learn about a population parameter.\n\n\n\n\n\n\n\n\nDefinition: Statistic\n\n\nA numerical characteristic of a sample. We calculate statistics from our data. A statistic is a random variable, as its value depends on the particular sample drawn.\n\n\n\n\n\n\n\n\n\n\n\nExample: Statistics\n\n\nSample mean (\\(\\bar{x}\\)), sample variance (\\(s^2\\)), sample correlation (\\(r\\))."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#simple-random-sampling",
    "href": "lectures/lecture2/lecture2.html#simple-random-sampling",
    "title": "Empirical Economics",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\n\nIn statistics, we usually want to compute a statistic and derive its distribution to say something about a corresponding parameter in the population. To do so, we need to assume things about the way our data is sampled.\nSimple Random Sampling (SRS) is the most basic sampling method.\n\n\n\n\n\n\n\n\nDefintion: Simple Random Sampling\n\n\nA sample of size \\(n\\) where every possible sample of that size has an equal chance of being selected, and every individual in the population has an equal chance of being included.\n\n\n\n\n\nSRS is the ideal. Statistical methods (like the ones we’re learning) are built on the assumption of random sampling.\nIf a sample is not drawn randomly, our inferences may be biased and incorrect."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#the-distribution-of-a-statistic",
    "href": "lectures/lecture2/lecture2.html#the-distribution-of-a-statistic",
    "title": "Empirical Economics",
    "section": "The Distribution of a Statistic",
    "text": "The Distribution of a Statistic\n\nThis is a crucial, but sometimes tricky, concept.\n\n\n\n\n\n\n\n\nThought Experiment\n\n\n\nThere is a population with an unknown mean \\(\\mu\\).\nWe take a random sample of size \\(n\\) (e.g., n=100) and calculate its sample mean, \\(\\bar{x}_1\\).\nWe take a different random sample of size \\(n\\) and get a different sample mean, \\(\\bar{x}_2\\).\nWe repeat this process 10,000 times, getting \\(\\bar{x}_1\\), \\(\\bar{x}_2\\), \\(\\dots\\), \\(\\bar{x}_{10000}\\).\n\n\n\n\n\n\nThe Sampling Distribution of the a statistic (in this case the sample mean) is the probability distribution of all these possible \\(\\bar{x}\\) values. It’s the distribution of a statistic, not of the original data."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#sampling-distribution-simulation",
    "href": "lectures/lecture2/lecture2.html#sampling-distribution-simulation",
    "title": "Empirical Economics",
    "section": "Sampling Distribution Simulation",
    "text": "Sampling Distribution Simulation\n\nRPythonStata\n\n\n\n\nCode\nset.seed(42)\npopulation_mean &lt;- 50\npopulation_sd &lt;- 10\nsample_size &lt;- 100\nnum_samples &lt;- 10000\n\nsample_means &lt;- replicate(num_samples, mean(rnorm(sample_size, mean = population_mean, sd = population_sd)))\n\nhist(sample_means, \n     breaks = 50,\n     main = \"Distribution of Sample Means\",\n     xlab = \"Sample Mean\",\n     ylab = \"Frequency\",\n     col = \"lightblue\",\n     border = \"black\")\nabline(v = population_mean, col = \"red\", lty = 2, lwd = 2)\ngrid()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\npopulation_mean, population_std, sample_size, num_samples = 50, 10, 100, 10000\nsample_means = [np.mean(np.random.normal(loc=population_mean, scale=population_std, size=sample_size)) for _ in range(num_samples)]\n\nplt.figure(figsize=(10, 6))\nplt.hist(sample_means, bins=50, edgecolor='black', alpha=0.7)\nplt.axvline(x=population_mean, color='red', linestyle='--', linewidth=2)\nplt.title('Distribution of Sample Means')\nplt.xlabel('Sample Mean')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nclear all\nset seed 42\nset obs 10000\n\n* Parameters\nscalar population_mean = 50\nscalar population_sd = 10\nscalar sample_size = 100\n\n* Generate sample means\ngen sample_mean = .\nquietly {\n    forvalues i = 1/10000 {\n        drawnorm x, n(`=sample_size') mean(`=population_mean') sd(`=population_sd') clear\n        summarize x\n        replace sample_mean = r(mean) in `i'\n    }\n}\n\n* Plot histogram\nhistogram sample_mean, ///\n    bin(50) ///\n    title(\"Distribution of Sample Means\") ///\n    xtitle(\"Sample Mean\") ///\n    ytitle(\"Frequency\") ///\n    fcolor(\"lightblue\") ///\n    addplot(pci 0 0 `=population_mean' `=population_mean', lcolor(red) lpattern(dash))"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#sampling-distribution-visualization",
    "href": "lectures/lecture2/lecture2.html#sampling-distribution-visualization",
    "title": "Empirical Economics",
    "section": "Sampling Distribution Visualization",
    "text": "Sampling Distribution Visualization\n\nOn the left: the distribution of \\(X\\) in the population\nOn the right: the distribution of \\(\\bar{X}\\)"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#sampling-distribution-example",
    "href": "lectures/lecture2/lecture2.html#sampling-distribution-example",
    "title": "Empirical Economics",
    "section": "Sampling Distribution Example",
    "text": "Sampling Distribution Example\n\n\n\n\n\n\n\nNumerical Example Sampling Distribution\n\n\nImagine a tiny population that contains only four numbers. These are all the values that exist in our entire population.\n\n\n[1] \"The population is: 2, 4, 6, 10\"\n\n\n[1] \"The true population mean (mu) is: 5.5\"\n\n\nThe true mean of our population is 5.5. In a real research problem, this is the value we want to estimate, but we don’t know it.\nNow, let’s list every single possible sample of size \\(n = 2\\) that we can draw from this population without replacement.\nThe number of combinations is “4 choose 2”, which is 6. We can use R to list them all.\n\n\n[1] \"All 6 possible samples of size n=2:\"\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    2    2    2    4    4    6\n[2,]    4    6   10    6   10   10"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#sampling-distribution-example-cont.",
    "href": "lectures/lecture2/lecture2.html#sampling-distribution-example-cont.",
    "title": "Empirical Economics",
    "section": "Sampling Distribution Example (Cont.)",
    "text": "Sampling Distribution Example (Cont.)\n\n\n\n\n\n\n\nNumerical Example Sampling Distribution (Cont.)\n\n\nFor each of the 6 possible samples, we will now calculate its sample mean (\\(\\bar{x}\\)).\n\n\n[1] \"The mean of each of the 6 possible samples:\"\n\n\n[1] 3 4 6 5 7 8\n\n\nThe list of all possible sample means we just calculated (3, 4, 6, 5, 7, 8) is the sampling distribution of the sample mean. This gives the distribution of all possible values the sample mean can take for a sample of size \\(n=2\\) from our population. Since we assume SRS, each outcome is equally likely.\nLet’s organize it into a frequency table and visualize it.\n\n\n[1] \"The Sampling Distribution (as a frequency table):\"\n\n\n  Sample_Mean Frequency\n1           3         1\n2           4         1\n3           5         1\n4           6         1\n5           7         1\n6           8         1"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#visualization-the-sampling-distribution",
    "href": "lectures/lecture2/lecture2.html#visualization-the-sampling-distribution",
    "title": "Empirical Economics",
    "section": "Visualization the Sampling Distribution",
    "text": "Visualization the Sampling Distribution\n\nWe can again visualize this distribution with a histogram.\n\n\n\n\n\n\n\n\n\n\n\nThe explicit PMF of the sampling distribution is the normalized version of this histogram: \\(P(\\bar{X}=3)=\\frac{1}{6}, P(\\bar{X}=4)=\\frac{1}{6}, \\dots, P(\\bar{X}=8)=\\frac{1}{6}\\)."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#the-central-limit-theorem",
    "href": "lectures/lecture2/lecture2.html#the-central-limit-theorem",
    "title": "Empirical Economics",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\n\nThe Central Limit Theorem (CLT) states:\n\n\n\n\n\n\n\n\nTheorem: Central Limit Theorem\n\n\nIf you take a sufficiently large random sample (\\(n \\geq 30\\) is a common rule of thumb) from a population distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of the sample mean \\(\\bar{x}\\) will be approximately normally distributed, regardless of the original population’s distribution.\nFurthermore, the mean of this sampling distribution will be the population mean \\(\\mu\\), and its standard deviation (called the standard error) will be \\(\\sigma / \\sqrt n\\).\n\\[\\bar{X} \\approx N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#why-is-the-clt-important",
    "href": "lectures/lecture2/lecture2.html#why-is-the-clt-important",
    "title": "Empirical Economics",
    "section": "Why Is The CLT Important?",
    "text": "Why Is The CLT Important?\n\nThe implications of the CLT are profound:\nWe can use the normal distribution for inference on the mean even if the underlying data is not normal. Many economic variables (like income) are highly skewed, but the CLT lets us work with their sample means.\nIt provides a precise formula for the variance of the sample mean (\\(\\sigma^2/n\\)). This shows that as our sample size \\(n\\) increases, the sample mean \\(\\bar{x}\\) becomes a more precise estimator of the population mean \\(\\mu\\) (its sampling distribution gets narrower).\nThe CLT is the foundation that allows us to build confidence intervals and conduct hypothesis tests for the mean. And later, also for estimators that are functions of the mean."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#example-of-the-clt",
    "href": "lectures/lecture2/lecture2.html#example-of-the-clt",
    "title": "Empirical Economics",
    "section": "Example of the CLT",
    "text": "Example of the CLT\n\n\n\n\n\n\n\nExample: Poisson-distributed RV’s\n\n\nLet \\(X_1, \\dots, X_n\\) be independent, identically distributed (i.i.d.) random variables following a Poisson distribution with parameter \\(\\lambda\\), i.e., \\(X_i \\sim \\text{Poisson}(\\lambda)\\). The Poisson distribution has mean \\(E[X_i]=\\lambda\\) and Variance \\(\\text{Var}(X_i)=\\lambda\\).\nBy the CLT, \\(\\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X}_n \\sim N(\\lambda, \\frac{\\lambda}{n})\\).\nSuppose that \\(\\lambda=5\\) and \\(N=100\\), then \\(\\bar{X}_n \\sim N(5,\\frac{5}{100})=N(5,0.05).\\) In the simulation below, where we plot 10000 simulations of \\(\\bar{X}_n\\), we should find that the empirical mean is 5 and the empirical variance is 0.05.\n\nRPythonStata\n\n\n\n\nCode\nset.seed(123)  # For reproducibility\nlambda &lt;- 5    # Poisson parameter\nn &lt;- 100       # Sample size\nn_sim &lt;- 10000 # Number of simulations\n\n# Simulate 10,000 sample means (each from n=100 Poisson(5) samples)\nx_bars &lt;- replicate(n_sim, mean(rpois(n, lambda)))\n\n# Compute empirical mean and variance of x_bars\nempirical_mean &lt;- mean(x_bars)\nempirical_var &lt;- var(x_bars)\n\ncat(\"Empirical Mean:\", empirical_mean, \"Empirical Var:\", empirical_var)\n\n\nEmpirical Mean: 4.996629 Empirical Var: 0.04879321\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Parameters\nlambda_ = 5     # Poisson parameter\nn = 100         # Sample size\nn_sim = 10000   # Number of simulations\n\n# Simulate 10,000 sample means (each from n=100 Poisson(5) samples)\nx_bars = np.array([np.random.poisson(lambda_, n).mean() for _ in range(n_sim)])\n\n# Empirical mean and variance\nempirical_mean = np.mean(x_bars)\nempirical_var = np.var(x_bars, ddof=1)  # ddof=1 for unbiased estimator\n\n# Print results\nprint(f\"Empirical Mean: {empirical_mean:.6f}, Empirical Var: {empirical_var:.6f}\")\n\n\nEmpirical Mean: 5.000215, Empirical Var: 0.048941\n\n\n\n\n\n\nCode\nclear\nset seed 123\nset obs 10000\n\nlocal lambda = 5\nlocal n = 100\nlocal n_sim = 10000\n\n// Generate 10,000 sample means\ngen x_bar = .\nforvalues i = 1/`n_sim' {\n    qui drawnorm x, n(`n') means(`lambda') sds(sqrt(`lambda')) clear\n    qui replace x_bar = r(mean) in `i'\n}\n\n// Calculate empirical mean and variance\nsum x_bar\ndi \"Empirical Mean: \" r(mean) \", Empirical Var: \" r(Var)"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#point-estimators",
    "href": "lectures/lecture2/lecture2.html#point-estimators",
    "title": "Empirical Economics",
    "section": "Point Estimators",
    "text": "Point Estimators\n\n\n\n\n\n\n\nDefinitions: Estimator, Estimate, Point Estimate\n\n\nAn estimator is a rule (a formula) for calculating an estimate of a population parameter based on sample data. The value it produces is called an estimate.\nA point Estimator is a formula that produces a single value/number as the estimate.\n\n\n\n\n\nCommon Point Estimators:\n\nThe sample mean \\(\\bar{x}\\) is a point estimator for the population mean \\(\\mu\\).\nThe sample proportion \\(\\hat{p}\\) is a point estimator for the population proportion \\(p\\).\nThe sample variance \\(s^2\\) is a point estimator for the population variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#desirable-properties-of-estimators",
    "href": "lectures/lecture2/lecture2.html#desirable-properties-of-estimators",
    "title": "Empirical Economics",
    "section": "Desirable Properties of Estimators",
    "text": "Desirable Properties of Estimators\n\nHow do we know if an estimator is “good”? We look for three properties (conceptually):\n\n\n\n\n\n\n\n\nDefinition: Unbiasedness\n\n\nAn estimator is unbiased if its expected value is equal to the true population parameter. \\(E[\\theta]=\\theta\\).\nAnalogy: On average, the shots hit the center of the target. There’s no systematic over- or under-estimation.\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Consistency\n\n\nAn estimator is consistent if, as the sample size \\(n\\) approaches infinity, the value of the estimator converges to the true parameter value.\nAnalogy: The more information you get, the closer you get to the truth."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#desirable-properties-of-estimators-cont.",
    "href": "lectures/lecture2/lecture2.html#desirable-properties-of-estimators-cont.",
    "title": "Empirical Economics",
    "section": "Desirable Properties of Estimators (Cont.)",
    "text": "Desirable Properties of Estimators (Cont.)\n\nHow do we know if an estimator is “good”? We look for three properties (conceptually):\n\n\n\n\n\n\n\n\nDefinition: Consistency\n\n\nAmong all unbiased estimators, the most efficient one is the one with the smallest variance.\nAnalogy: The shots are tightly clustered around the center. It’s a precise estimator."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#hypothesis-testing",
    "href": "lectures/lecture2/lecture2.html#hypothesis-testing",
    "title": "Empirical Economics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nHypothesis Testing is a formal procedure for checking whether our sample data provides convincing evidence against a preconceived claim about the population.\n\n\n\n\n\n\n\n\nThe Logic: Proof by Contradiction\n\n\n\nWe start by assuming something is true about the population (the Null Hypothesis).\nWe then look at our sample data.\nWe ask: “If the null hypothesis were true, how likely is it that we would get sample data like this?”\nIf our sample data is very unlikely under the null hypothesis, we reject our initial assumption in favor of an alternative."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#null-and-alternative-hypotheses",
    "href": "lectures/lecture2/lecture2.html#null-and-alternative-hypotheses",
    "title": "Empirical Economics",
    "section": "Null and Alternative Hypotheses",
    "text": "Null and Alternative Hypotheses\n\nEvery hypothesis test has two competing hypotheses:\nNull Hypothesis (\\(H_0\\)): The claim being tested. It’s the “status quo” or “no effect” hypothesis. It always contains an equality sign (\\(=\\), \\(\\leq\\), or \\(\\geq\\)).\n\n\n\n\n\n\n\n\nExamples: Null Hypothesis \\(H_0\\)\n\n\nThe new drug has no effect on blood pressure (\\(\\mu_{change} = 0\\)).\nThe mean income in a region is \\(50,000\\) (\\(\\mu = 50000\\))."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#alternative-hypothesis",
    "href": "lectures/lecture2/lecture2.html#alternative-hypothesis",
    "title": "Empirical Economics",
    "section": "Alternative Hypothesis",
    "text": "Alternative Hypothesis\n\nAlternative Hypothesis (\\(H_A\\) or \\(H_1\\)): The claim we are trying to find evidence for. It’s what we conclude if we reject the null hypothesis. It never contains an equality sign (\\(\\neq\\), \\(&lt;\\), or \\(&gt;\\)).\n\n\n\n\n\n\n\n\nExamples: Null Hypothesis \\(H_A\\)\n\n\nThe new drug does have an effect (\\(\\mu_{change} \\neq 0\\)).\nThe mean income is not \\(50,000\\) (\\(\\mu \\neq 50000\\))."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#test-statistics-and-p-values-conceptual",
    "href": "lectures/lecture2/lecture2.html#test-statistics-and-p-values-conceptual",
    "title": "Empirical Economics",
    "section": "Test Statistics and \\(p\\)-values (Conceptual)",
    "text": "Test Statistics and \\(p\\)-values (Conceptual)\n\nHow do we decide whether our data is “unlikely”?\nTest Statistic: A value calculated from sample data that measures how far our sample statistic (e.g., \\(\\bar{x}\\)) is from the parameter value claimed by the null hypothesis (\\(\\mu_0\\)). It’s often standardized, like a Z-score.\n\n\n\n\n\n\n\n\nDefinition: Test Statistic (Informal)\n\n\n\\[\n\\text{Test Statistic} = \\frac{\\text{Sample Statistic} - \\text{Null Hypothesis Value}}{\\text{Standard Error}}\n\\]"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#p-values",
    "href": "lectures/lecture2/lecture2.html#p-values",
    "title": "Empirical Economics",
    "section": "\\(p\\)-values",
    "text": "\\(p\\)-values\n\n\n\n\n\n\n\nDefinition: \\(p\\)-value\n\n\nThe \\(p\\)-value is the probability of observing a test statistic as extreme (or more extreme) than the one calculated, assuming the null hypothesis is true.\n\n\n\n\n\nSmall P-Value (e.g., &lt; 0.05): The observed data is very unlikely if \\(H_0\\) were true. We reject \\(H_0\\). The evidence supports \\(H_A\\).\nLarge P-Value (e.g., &gt; 0.05): The observed data is plausible if \\(H_0\\) were true. We fail to reject \\(H_0\\). There is not enough evidence to support \\(H_A\\)."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#a-crucial-ingredient-the-standard-error",
    "href": "lectures/lecture2/lecture2.html#a-crucial-ingredient-the-standard-error",
    "title": "Empirical Economics",
    "section": "A Crucial Ingredient: The Standard Error",
    "text": "A Crucial Ingredient: The Standard Error\n\nHow do we measure if a sample result is “surprising”? We use the Standard Error (SE).\n\n\n\n\n\n\n\n\nDefinition: Standard Error\n\n\nThe Standard Error of a statistic (like the sample mean) is the standard deviation of its sampling distribution. In simpler terms, it measures the typical or average distance between the sample statistic and the true population parameter.\n\n\n\n\n\nThe SE tells us how much we expect e.g. a sample mean (\\(\\bar{x}\\)) to naturally vary from the true population mean (\\(\\mu\\)).\n\nA small SE means our sample means will be tightly clustered around the true mean.\nA large SE means they will be more spread out."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#one-sided-vs.-two-sided-hypotheses",
    "href": "lectures/lecture2/lecture2.html#one-sided-vs.-two-sided-hypotheses",
    "title": "Empirical Economics",
    "section": "One-Sided vs. Two-Sided Hypotheses",
    "text": "One-Sided vs. Two-Sided Hypotheses\n\nTwo-Sided Hypotheses tests are the most common type of test.\nIs the population parameter different from a specific value? (We don’t care if it’s higher or lower, just that it’s not the same).\n\nNull Hypothesis: \\(H_0: \\mu = \\mu_0\\)\nAlternative Hypothesis: \\(H_A: \\mu \\neq \\mu_0\\)\nWe are looking for an extreme result in either direction.\n\nThe significance level (\\(\\alpha\\), e.g., 0.05) is split between the two tails of the distribution.\n\n\n\n\n\n\n\n\nDefinition: Significance level\n\n\nThe significance level, denoted as \\(\\alpha\\), is the probability we are willing to incur of rejecting the null hypothesis when it is actually true."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#rejection-region-two-sided-test",
    "href": "lectures/lecture2/lecture2.html#rejection-region-two-sided-test",
    "title": "Empirical Economics",
    "section": "Rejection Region Two-Sided Test",
    "text": "Rejection Region Two-Sided Test\n\nWe are looking for an extreme result in either direction.\nThe significance level (\\(\\alpha\\), e.g., 0.05) is split between the two tails of the distribution."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#one-sided-hypothesis-testing-right-tailed",
    "href": "lectures/lecture2/lecture2.html#one-sided-hypothesis-testing-right-tailed",
    "title": "Empirical Economics",
    "section": "One-Sided Hypothesis Testing (Right-Tailed)",
    "text": "One-Sided Hypothesis Testing (Right-Tailed)\n\nThe Question: Is the population parameter greater than a specific value?\nThis is used when we have a strong reason to believe the effect can only go in one direction, or we are only interested in an effect in one direction.\n\nNull Hypothesis: \\(H_0: \\mu \\le \\mu_0\\)\nAlternative Hypothesis: \\(H_A: \\mu &gt; \\mu_0\\)"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#rejection-region-one-sided-test-right-tailed",
    "href": "lectures/lecture2/lecture2.html#rejection-region-one-sided-test-right-tailed",
    "title": "Empirical Economics",
    "section": "Rejection Region One-Sided Test (Right-Tailed)",
    "text": "Rejection Region One-Sided Test (Right-Tailed)\n\nThe entire significance level (\\(\\alpha\\)) is placed in the upper (right) tail."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#one-sided-hypothesis-testing-left-tailed",
    "href": "lectures/lecture2/lecture2.html#one-sided-hypothesis-testing-left-tailed",
    "title": "Empirical Economics",
    "section": "One-Sided Hypothesis Testing (Left-Tailed)",
    "text": "One-Sided Hypothesis Testing (Left-Tailed)\n\nIn the left-tailed case, we ask whether the population parameter is less than a specific value?\n\nNull Hypothesis: \\(H_0: \\mu \\ge \\mu_0\\)\nAlternative Hypothesis: \\(H_A: \\mu &lt; \\mu_0\\)"
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#rejection-region-one-sided-test-left-tailed",
    "href": "lectures/lecture2/lecture2.html#rejection-region-one-sided-test-left-tailed",
    "title": "Empirical Economics",
    "section": "Rejection Region One-Sided Test (Left-Tailed)",
    "text": "Rejection Region One-Sided Test (Left-Tailed)\n\nThe entire significance level (\\(\\alpha\\)) is placed in the lower (left) tail."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#example-hypothesis-testing",
    "href": "lectures/lecture2/lecture2.html#example-hypothesis-testing",
    "title": "Empirical Economics",
    "section": "Example Hypothesis Testing",
    "text": "Example Hypothesis Testing\n\n\n\n\n\n\n\nExample: Fertilizer\n\n\nWe are testing if a new fertilizer increases or decreases crop yield. We have a dataset of land units that either use fertilizer or not, and their associated crop yields. We have a sample size \\(\\geq\\) 30 in both groups, meaning we can assume the CLT holds.\nA good estimator of the crop yield in either group (fertilizer, \\(\\mu_F\\) and no fertilzer \\(\\mu_N\\)) is the group’s sample mean, \\(\\overline{\\mu_F}\\) and \\(\\overline{\\mu_N}\\). Suppose we also know that both crop yields are distributed around their population means with variance \\(\\sigma^2=5\\).\nAccording to the CLT, we know that \\(\\overline{\\mu_F} \\sim N(\\mu_F, \\frac{5}{30})\\) and \\(\\overline{\\mu_N} \\sim N(\\mu_N, \\frac{5}{30})\\).\nSuppose we want to know whether the group yields differ per group. This is a two-sided hypothesis: \\(H_0: \\mu_F = \\mu_N\\), and \\(H_A: \\mu_F \\neq \\mu_N\\)."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#example-hypothesis-testing-cont.",
    "href": "lectures/lecture2/lecture2.html#example-hypothesis-testing-cont.",
    "title": "Empirical Economics",
    "section": "Example Hypothesis Testing (Cont.)",
    "text": "Example Hypothesis Testing (Cont.)\n\n\n\n\n\n\n\nExample: Fertilizer (Cont.)\n\n\nSuppose our test statistic is \\(T = \\overline{\\mu_F} - \\overline{\\mu_N}\\). Under the null hypothesis, its expected value is 0. According to the law of variances, its variance is \\(\\frac{5}{30} + \\frac{5}{30} = \\frac{1}{3}\\). Since both components are normally distributed, under the null hypothesis our test statistic \\(T \\sim N(0, \\frac{1}{3})\\).\nSuppose our data show that \\(\\overline{\\mu_F}=5\\) and \\(\\overline{\\mu_N}=1\\). Hence \\(T=4\\). How likely is that under the null hypothesis?\n\\[\\begin{align*}\nP(|T|&gt; 4) &= P(T&gt;4) + P(T&lt;-4) \\newline\n&= P(Z &gt; \\frac{4}{\\sqrt{\\frac{1}{3}}}) + P(Z &lt; \\frac{-4}{\\sqrt{\\frac{1}{3}}})\n&\\approx 0\n\\end{align*}\\]\nThis probability is also the \\(p\\)-value. This means that it is very unlikely to observe this difference between the two groups given the null hypothesis. Therefore, with e.g. \\(\\alpha=0.05\\), we reject the null hypothesis."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#example-hypothesis-testing-cont.-1",
    "href": "lectures/lecture2/lecture2.html#example-hypothesis-testing-cont.-1",
    "title": "Empirical Economics",
    "section": "Example Hypothesis Testing (Cont.)",
    "text": "Example Hypothesis Testing (Cont.)\n\n\n\n\n\n\n\nExample: Fertilizer (Cont.)\n\n\nSuppose now that \\(\\overline{\\mu_F}=1.5\\) and \\(\\overline{\\mu_N}=1\\), such that \\(T=0.5\\), and we’re interested in whether fertilizer increases crop yield. This is a one-sided hypothesis test with \\(H_0: \\mu_F = \\mu_N\\) against \\(H_A: \\mu_F &gt; \\mu_N\\). We again use a significance level \\(\\alpha=0.05\\).\nUnder the null hypothesis, \\(T \\sim N(0, \\frac{1}{3})\\). We now calculate the \\(p\\)-value as \\(P(T &gt; 0.5)\\) = \\(P(Z &gt; \\frac{0.5}{\\sqrt{\\frac{1}{3}}}) = P(Z &gt; 0.86)\\), which evaluates to 0.19. Since we have a significance-level of 5%, we do not reject the null hypothesis: the test statistic we observed is not unlikely enough to reject it."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#which-test-to-use",
    "href": "lectures/lecture2/lecture2.html#which-test-to-use",
    "title": "Empirical Economics",
    "section": "Which Test to Use?",
    "text": "Which Test to Use?\n\nGolden Rule: Unless you have a very strong, justifiable, pre-specified reason for expecting an effect in only one direction, you should use a two-sided test.\n\nTwo-sided tests define the \\(p\\)-value as the probability of observing something more extreme than the observed test statistic on both sides, i.e. \\(P(|T|&gt;t)\\)\nOne-sided tests define the \\(p\\)-value as the probability of observing something more extreme than the observed test statistic on one side, i.e. \\(P(T&gt;t)\\) or \\(P(T&lt;t)\\).\n\n\n\n\n\n\n\n\n\n\n\nFeature\nTwo-Sided Test\nOne-Sided Test\n\n\n\n\nKey Question\nIs there a difference?\nIs it greater than or less than?\n\n\nAlternative (\\(H_A\\))\n\\(\\mu \\neq \\mu_0\\)\n\\(\\mu &gt; \\mu_0\\) or \\(\\mu &lt; \\mu_0\\)\n\n\nRejection Region\nSplit into two tails\nAll in one tail (left or right)\n\n\nWhen to Use\nThe default, conservative choice.\nOnly when there is a strong prior reason or you only care about one direction.\n\n\nPower\nLess powerful.\nMore powerful (if the effect is in the hypothesized direction)."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#confidence-intervals-1",
    "href": "lectures/lecture2/lecture2.html#confidence-intervals-1",
    "title": "Empirical Economics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\n\n\n\n\n\nDefinition: Confidence Interval\n\n\nA confidence interval (CI) is a range of values, derived from sample data, that is likely to contain the value of an unknown population parameter (e.g., the true population mean \\(\\mu\\) or proportion \\(p\\)).\n\n\n\n\n\nIngredients:\n\nPoint Estimate: A single value calculated from the sample that estimates the population parameter (e.g., sample mean \\(\\bar{x}\\)). It’s our “best guess,” but it’s almost certainly wrong.\nInterval Estimate: The confidence interval provides a range around the point estimate, acknowledging the uncertainty inherent in sampling.\nConfidence Level: The probability that the method used to construct the interval will capture the true population parameter. Common levels are 90%, 95%, and 99%."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#interpretation",
    "href": "lectures/lecture2/lecture2.html#interpretation",
    "title": "Empirical Economics",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe confidence level refers to the long-run success rate of the method, not the probability of a single interval being correct.\nCorrect Interpretation: “We are 95% confident that the method used to construct this interval from our sample captures the true population mean.”\n\nAnalogy: Imagine throwing rings at a post (the true parameter). The 95% confidence level means that if you were to take many samples and throw many “ring” intervals, 95% of them would land on the post. You don’t know if the one ring you just threw is a success or a miss.\n\nIncorrect Interpretation: It is wrong to say, “There is a 95% probability that the true population mean lies within this specific interval \\([A, B]\\).” Once an interval is calculated, the true mean is either in it or it isn’t; there is no probability involved for that specific interval."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#construction-of-a-confidence-interval",
    "href": "lectures/lecture2/lecture2.html#construction-of-a-confidence-interval",
    "title": "Empirical Economics",
    "section": "Construction of a Confidence Interval",
    "text": "Construction of a Confidence Interval\n\nMost confidence intervals share a common structure.\n\n\n\n\n\n\n\n\nConstruction of a Confidence Interval\n\n\n\\[\nCI= \\text{Point Estimate} \\pm \\text{Margin of Error}\n\\]\n\n\n\n\n\nThe Margin of Error (ME) quantifies the uncertainty of our estimate and is built from two pieces.."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#margin-of-error",
    "href": "lectures/lecture2/lecture2.html#margin-of-error",
    "title": "Empirical Economics",
    "section": "Margin of Error",
    "text": "Margin of Error\n\n\nCritical Value:\n\n\nA number from a probability distribution (typically a Z or t distribution).\nIt determines how many standard errors to go out from the point estimate to achieve the desired confidence level.\nFor a 95% CI, the Z-critical value is \\(Z_{\\alpha/2} = 1.96\\). For a t-distribution, it also depends on the sample size (degrees of freedom).\n\n\nStandard Error of the Estimate (SE):\n\n\nAn estimate of the standard deviation of the sampling distribution of the point estimate.\nIt measures the typical amount of variability we expect in our point estimate from sample to sample.\nExample for a mean: \\(SE = s / \\sqrt{n}\\) (where \\(s\\) is the sample standard deviation and \\(n\\) is the sample size)."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#common-examples",
    "href": "lectures/lecture2/lecture2.html#common-examples",
    "title": "Empirical Economics",
    "section": "Common Examples",
    "text": "Common Examples\n\n\n\n\n\n\n\nExample: CI for a Population Mean\n\n\nSuppose we observe \\(n\\) samples of a population, which is i.i.d. distributed with some unknown mean \\(\\mu_X\\) and some known variance \\(\\sigma^2\\). A confidence interval for a population mean (when population \\(\\sigma\\) is known) is:\n\\[\n\\bar{x} \\pm z_{\\alpha/2} \\left( \\frac{\\sigma}{\\sqrt{n}} \\right).\n\\] Where \\(\\bar{x}\\) is the sample mean, \\(\\sigma\\) is the standard deviation, and \\(z\\) is the critical value from the normal distribution at the \\(\\alpha/2\\)’th quantile.\nThe confidence interval is constructed on the basis of the central limit theorem, i.e. the normal distribution of \\(\\bar{x}\\) with variance \\(\\sigma^2/n\\), hence standard error \\(\\sigma/\\sqrt{n}\\)."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#common-examples-cont.",
    "href": "lectures/lecture2/lecture2.html#common-examples-cont.",
    "title": "Empirical Economics",
    "section": "Common Examples (Cont.)",
    "text": "Common Examples (Cont.)\n\n\n\n\n\n\n\nExample: CI for a Proportion\n\n\nSuppose we have \\(n\\) samples of a Bernoulli-distributed variable, i.e. \\(X_i \\sim \\text{Ber}(p)\\) with unknown \\(p\\). Our objective is to provide a CI for this \\(p\\) on the basis of our observed data.\nIn lecture one, we have seen that the variance of a Bernoulli distribution is \\(p(1-p)\\). According to the CLT, \\(\\hat{p}=\\frac{1}{n}\\sum x_i \\sim N(p, \\frac{p(1-p)}{n})\\).\nA \\((1-\\alpha)\\)% CI can then be constructed as follows:\n\\[\n\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}.\n\\]\nWhere \\(\\hat{p}\\) is the sample proportion and \\(z_{\\alpha/2}\\) is the \\(\\alpha/2\\)’th quantile from the standard normal distribution."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#what-did-we-do",
    "href": "lectures/lecture2/lecture2.html#what-did-we-do",
    "title": "Empirical Economics",
    "section": "What did we do?",
    "text": "What did we do?\n\nDistinguished between populations and samples:\n\nThe lecture established the core idea of inferential statistics, which is to use a sample statistic (e.g., the sample mean \\(\\bar{x}\\)) calculated from observed data to learn about an unknown population parameter (e.g., the true population mean \\(\\mu\\)).\n\nIntroduced the Sampling Distribution:\n\nIt explained that any statistic calculated from a random sample is itself a random variable. The sampling distribution is the probability distribution of this statistic, showing all its possible values and their likelihoods across many different potential samples.\n\nExplained the Central Limit Theorem (CLT):\n\nWe highlighted the CLT as a cornerstone of statistics. It states that for a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normal, even if the original population data is not normally distributed."
  },
  {
    "objectID": "lectures/lecture2/lecture2.html#what-did-we-do-cont.",
    "href": "lectures/lecture2/lecture2.html#what-did-we-do-cont.",
    "title": "Empirical Economics",
    "section": "What did we do? (Cont.)",
    "text": "What did we do? (Cont.)\n\nOutlined the framework for Hypothesis Testing:\n\nWe presented hypothesis testing as a formal procedure to evaluate a claim about a population. This involves setting up a null (\\(H_0\\)) and alternative (\\(H_A\\)) hypothesis, calculating a test statistic from the sample, and using a p-value to determine if the data provides statistically significant evidence to reject the null hypothesis.\n\nDifferentiated between One-Sided and Two-Sided Tests:\n\nThe lecture clarified how the research question determines the type of test. A two-sided test checks for any difference (\\(\\neq\\)), while a one-sided test checks for a specific direction (greater than &gt; or less than &lt;), which affects how the p-value and rejection region are determined.\n\nDefined Confidence Intervals:\n\nFinally, the lecture introduced confidence intervals as a method for estimation. A confidence interval provides a range of plausible values for an unknown population parameter, constructed as a point estimate plus or minus a margin of error that reflects the level of sampling uncertainty."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#course-overview",
    "href": "lectures/lecture6/lecture6.html#course-overview",
    "title": "Empirical Economics",
    "section": "Course Overview",
    "text": "Course Overview\n\nStatistics and Probability - Basic Concepts\nStatistics and Probability - Hypothesis Testing\nThe Linear Regression Model\nTime Series Data\nPanel Data (FE) and Control Variables\nBinary Outcome Data\nPotential Outcomes and Difference-in-differences\nHands-on Econometrics in Practice"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#what-do-we-do-today",
    "href": "lectures/lecture6/lecture6.html#what-do-we-do-today",
    "title": "Empirical Economics",
    "section": "What do we do today?",
    "text": "What do we do today?\n\nWe have seen cross sectional and time series data\nThis lecture, we will talk about methods used when we can combine features of cross-sectional and time-series data\nWe will introduce two workhorse econometrics models, the Fixed Effects (FE) model, and the Random Effects (RE) model\nWe discuss a special case of panel data in the form of event studies."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#binary-outcomes",
    "href": "lectures/lecture6/lecture6.html#binary-outcomes",
    "title": "Empirical Economics",
    "section": "Binary Outcomes",
    "text": "Binary Outcomes\n\nMany interesting economic and social questions have a binary (0/1) outcome. We need special tools to model these.\n\n\n\n\n\n\n\n\nExamples: Binary Outcomes\n\n\n\nLabor Economics: Does a person participate in the labor force? (Yes=1, No=0)\nFinance: Does a company default on its loan? (Default=1, No Default=0)\nMarketing: Does a consumer purchase a product after seeing an ad? (Purchase=1, No Purchase=0)\nHealth Economics: Does a patient’s insurance status affect whether they receive a certain treatment? (Treatment=1, No Treatment=0)\nPolitical Science: Does a person vote for a specific candidate? (Vote=1, Don’t Vote=0)\n\n\n\n\n\n\nOur dependent variable, \\(y\\), can only take two values: 0 and 1."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#linear-probability-model-lpm",
    "href": "lectures/lecture6/lecture6.html#linear-probability-model-lpm",
    "title": "Empirical Economics",
    "section": "Linear Probability Model (LPM)",
    "text": "Linear Probability Model (LPM)\n\nWhat if we just use what we know? Ordinary Least Squares (OLS).\nWhen we apply OLS to a binary dependent variable, we call it the Linear Probability Model (LPM).\n\n\n\n\n\n\n\n\nDefinition: Linear Probability Model\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\epsilon_i\\]\nWhere \\(y_i\\) is either 0 or 1.\n\n\n\n\n\nKey Insight: The expected value of a binary variable is the probability that it equals 1. \\(E[y_i | X_i] = 1 \\cdot P(y_i=1 | X_i) + 0 \\cdot P(y_i=0 | X_i) = P(y_i=1 | X_i)\\)\nThis makes interpretation very appealing…"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#lpm-interpretation",
    "href": "lectures/lecture6/lecture6.html#lpm-interpretation",
    "title": "Empirical Economics",
    "section": "LPM Interpretation",
    "text": "LPM Interpretation\n\nSince \\(E[y_i | X_i] = P(y_i=1 | X_i)\\), the LPM becomes: \\(P(y_i=1 | X_i) = \\beta_0 + \\beta_1 x_{1i} + ...\\)\nInterpretation of Coefficients:\n\n\\(\\beta_k\\) is the change in the probability that \\(y=1\\) for a one-unit change in \\(x_k\\), holding other factors constant.\nSimplicity: Easy to estimate (just OLS) and coefficients are incredibly easy to interpret as changes in probability.\n\n\n\n\n\n\n\n\n\nExample: Interpretation of LPM\n\n\nSuppose we have a model like: employed = 0.20 + 0.15 * education_years\nEach additional year of education is associated with a 0.15 (or 15 percentage point) increase in the probability of being employed."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#problems-with-lpm-out-of-bounds-predictions",
    "href": "lectures/lecture6/lecture6.html#problems-with-lpm-out-of-bounds-predictions",
    "title": "Empirical Economics",
    "section": "Problems with LPM: Out-of-Bounds Predictions",
    "text": "Problems with LPM: Out-of-Bounds Predictions\n\nThe model is linear, but probability is bounded by \\([0, 1]\\). The LPM doesn’t know this.\n\\[P(y=1|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\]\nNothing in the OLS mechanics prevents the predicted value, \\(\\hat{y}\\), from being less than 0 or greater than 1 for certain values of X.\n\nInterpretation of a predicted probability of 1.2 or -0.1 is nonsensical."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#illustration",
    "href": "lectures/lecture6/lecture6.html#illustration",
    "title": "Empirical Economics",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#problems-with-lpm-the-error-term",
    "href": "lectures/lecture6/lecture6.html#problems-with-lpm-the-error-term",
    "title": "Empirical Economics",
    "section": "Problems with LPM: The Error Term",
    "text": "Problems with LPM: The Error Term\n\nThe assumptions of the Classical Linear Model are violated.\nInherent Heteroskedasticity:\n\nThe variance of the error term depends on the values of the independent variables.\n\\(Var(\\epsilon_i | X_i) = p_i(1-p_i)\\), where \\(p_i = P(y_i=1|X_i)\\)\nSince \\(p_i\\) depends on X, the variance is not constant. This is heteroskedasticity.\nConsequence: OLS standard errors are biased. We could use robust standard errors to fix this problem."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#lpm-summary",
    "href": "lectures/lecture6/lecture6.html#lpm-summary",
    "title": "Empirical Economics",
    "section": "LPM Summary",
    "text": "LPM Summary\n\nPros: Simple to interpret. It is also easy to incorporate fixed effects in an LPM.\nCons: Nonsensical predictions, violates key OLS assumptions.\nWe need a model that constrains the predicted probability to be between 0 and 1. We need a non-linear model.\nThis requires a different way of thinking about the choice process."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#the-latent-variable-framework",
    "href": "lectures/lecture6/lecture6.html#the-latent-variable-framework",
    "title": "Empirical Economics",
    "section": "The Latent Variable Framework",
    "text": "The Latent Variable Framework\n\nLet’s imagine the binary choice is driven by an unobserved, underlying continuous variable, \\(y^*\\).\n\\(y^*\\) can be thought of as the “net utility,” “propensity,” or “tendency” to choose 1.\n\n\n\n\n\n\n\n\nDefinition: Latent Variable Framework\n\n\n\\(y_i^* = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\nWe don’t observe \\(y^*\\). We only observe the outcome, y, based on a threshold (usually normalized to 0):\n\nIf \\(y_i^* &gt; 0\\), then we choose Yes (\\(y_i = 1\\))\nIf \\(y_i^* \\le 0\\), then we choose No (\\(y_i = 0\\))\n\nThe probability that \\(y_i=1\\) is the probability that \\(y_i^*\\) is greater than 0.\n\\[\n    P(y_i=1) = P(y_i^* &gt; 0) = P(\\beta_0 + \\beta_1 x_i + \\epsilon_i &gt; 0) = P(\\epsilon_i &gt; -(\\beta_0 + \\beta_1 x_i))\n  \\]"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#from-latent-variables-to-probit-logit",
    "href": "lectures/lecture6/lecture6.html#from-latent-variables-to-probit-logit",
    "title": "Empirical Economics",
    "section": "From Latent Variables to Probit & Logit",
    "text": "From Latent Variables to Probit & Logit\n\nThis links the probability of the observed outcome to the distribution of the unobserved error term, \\(\\epsilon_i\\).\nThe final step depends on what we assume about the distribution of \\(\\epsilon_i\\).\n\\[\n  P(y_i=1) = P(\\epsilon_i &gt; -X_i'\\beta) = 1 - F(-X_i'\\beta)\n\\]\nwhere F is the Cumulative Distribution Function (CDF) of \\(\\epsilon_i\\)."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#probit-and-logit",
    "href": "lectures/lecture6/lecture6.html#probit-and-logit",
    "title": "Empirical Economics",
    "section": "Probit and Logit",
    "text": "Probit and Logit\n\nTwo common choices for \\(F\\) are the normal distribution (Probit model) and the logistic distribution (Logit model)\n\n\n\n\n\n\n\n\nDefinition: Probit and Logit Models\n\n\nProbit Model: Assumes \\(\\epsilon_i\\) follows a Standard Normal distribution.\n\n\\(P(y_i=1 | X_i) = 1 - \\Phi (-\\beta x_i) =  \\Phi(\\beta x_i)\\)\nwhere \\(\\Phi(\\cdot)\\) is the Standard Normal CDF.\n\nLogit Model: Assumes \\(\\epsilon_i\\) follows a Standard Logistic distribution.\n\n\\(P(y_i=1 | X_i) = 1 - \\Lambda(-\\beta x_i) = \\Lambda(\\beta x_i) = \\frac{e^{\\beta x_i}}{1 + e^{\\beta x_i}}\\)\nwhere \\(\\Lambda(\\cdot)\\) is the Standard Logistic CDF.\n\nIn both cases, \\(1-F(-\\beta x_i) = F(\\beta x_i)\\), since both distributions are symmetric. Both CDFs produce the S-shaped curve we need."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#visualization-logit-and-probit-models",
    "href": "lectures/lecture6/lecture6.html#visualization-logit-and-probit-models",
    "title": "Empirical Economics",
    "section": "Visualization Logit and Probit Models",
    "text": "Visualization Logit and Probit Models"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#estimation-maximum-likelihood-mle",
    "href": "lectures/lecture6/lecture6.html#estimation-maximum-likelihood-mle",
    "title": "Empirical Economics",
    "section": "Estimation: Maximum Likelihood (MLE)",
    "text": "Estimation: Maximum Likelihood (MLE)\n\nRemember that in OLS, we take \\(\\sum (y_i - \\hat{y_i} (\\beta, x_i))^2\\) and set the derivative to zero to express the optimal \\(\\beta\\) coefficients.\n\nThese conditions do not lead to a unique expression for the \\(\\beta\\)-coefficients in Logit/Probit.\nHence we can’t use OLS. Instead, we use Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\n\nDefinition: Maximum Likelihood Estimation\n\n\nMLE finds the parameter values (\\(\\beta\\)) that maximize the probability of observing the actual data we collected.\n\n\n\n\n\nIn other words: “Given our data, what are the most likely parameter values that could have generated it?”"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#simple-mle-example-a-biased-coin",
    "href": "lectures/lecture6/lecture6.html#simple-mle-example-a-biased-coin",
    "title": "Empirical Economics",
    "section": "Simple MLE Example: A Biased Coin",
    "text": "Simple MLE Example: A Biased Coin\n\n\n\n\n\n\n\nExample: A Biased Coin\n\n\nImagine you flip a coin 10 times and get 7 Heads (H) and 3 Tails (T).\n\nData: {H, H, T, H, H, T, H, H, T, H}\nQuestion: What is your best guess for p, the probability of getting a Head?\n\nThe Likelihood Function \\(L(p | \\text{Data})\\): The probability of observing this specific sequence is: \\[L(p) = p \\cdot p \\cdot (1-p) \\cdot p \\cdot p \\cdot (1-p) \\cdot p \\cdot p \\cdot (1-p) \\cdot p = p^7 (1-p)^3\\]\nThe goal is to find the value of p that maximizes this function.\n\nIntuition: Your gut says p = 0.7.\nOptimization: We can take lots of values of \\(p\\) and calculate the likelihood, and see which value of \\(p\\) gives us the highest likelihood.\nThe result is indeed \\(\\hat{p}_{MLE} = 0.7\\).\n\nThis is the value of p that makes the data we saw “most likely.”"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#mle-visualization",
    "href": "lectures/lecture6/lecture6.html#mle-visualization",
    "title": "Empirical Economics",
    "section": "MLE Visualization",
    "text": "MLE Visualization\n\n\n\n\n\n\n\nMLE Visualization: Biased Coin"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#mle-for-probitlogit-models",
    "href": "lectures/lecture6/lecture6.html#mle-for-probitlogit-models",
    "title": "Empirical Economics",
    "section": "MLE for Probit/Logit Models",
    "text": "MLE for Probit/Logit Models\n\nFor our regression models, the principle is the same but more complex.\nThe Likelihood Function is the product of the probabilities of each individual observation: \\(L(\\beta) = \\prod_{i=1}^{N} [P(y_i=1|X_i)]^{y_i} \\cdot [1 - P(y_i=1|X_i)]^{1-y_i}\\)\n\nIf \\(y_i=1\\), we use \\(P(y_i=1|X_i)\\)\nIf \\(y_i=0\\), we use \\(1-P(y_i=1|X_i) = P(y_i=0|X_i)\\)\n\nWe plug in the Probit or Logit formula for \\(P(y_i=1|X_i)\\) and use a computer to find the \\(\\beta\\) vector that maximizes this function (or more commonly, the log of this function, the Log-Likelihood)."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#mle-probit-example",
    "href": "lectures/lecture6/lecture6.html#mle-probit-example",
    "title": "Empirical Economics",
    "section": "MLE: Probit Example",
    "text": "MLE: Probit Example\n\n\n\n\n\n\n\nExample: MLE for Probit\n\n\nLikelihood Function: \\[\n\\mathcal{L}(\\beta) = \\prod_{i=1}^n \\Phi(X_i\\beta)^{Y_i} [1 - \\Phi(X_i\\beta)]^{1-Y_i}\n\\]\nLog-Likelihood Function: \\[\n\\ln\\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left\\{ Y_i \\ln\\Phi(X_i\\beta) + (1-Y_i)\\ln[1-\\Phi(X_i\\beta)] \\right\\}\n\\]\nFirst-Order Condition (FOC): \\[\n\\frac{\\partial \\ln\\mathcal{L}(\\beta)}{\\partial \\beta} = \\sum_{i=1}^n \\left[ \\frac{Y_i \\phi(X_i\\beta)}{\\Phi(X_i\\beta)} - \\frac{(1-Y_i)\\phi(X_i\\beta)}{1-\\Phi(X_i\\beta)} \\right] X_i = 0\n\\]"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#interpreting-coefficients",
    "href": "lectures/lecture6/lecture6.html#interpreting-coefficients",
    "title": "Empirical Economics",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\nIn Probit and Logit models, the estimated coefficients (\\(\\hat{\\beta}\\)) are NOT marginal effects.\n\\[\n  P(y=1|X) = F(\\beta x_i )\n\\]\nA one-unit change in \\(x_k\\) changes the argument \\(\\beta x_i\\) by \\(\\beta_k\\).\nBut the change in the probability depends on the slope of the S-curve, which depends on the values of all X variables:\n\\[\n  \\frac{\\partial P(y=1 | x_i)}{\\partial x_i} = F'(\\beta x_i) \\cdot \\beta = f(\\beta x_i) \\cdot \\beta\n\\]\nThis means that the change in the probability does not only depend on \\(\\beta\\), but also on the values of the independent variabels \\(x_i\\) and the function \\(F'\\).\nSo, how do we get meaningful interpretations?"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#interpretation-method-1-marginal-effect-at-the-mean",
    "href": "lectures/lecture6/lecture6.html#interpretation-method-1-marginal-effect-at-the-mean",
    "title": "Empirical Economics",
    "section": "Interpretation Method 1: Marginal Effect at the Mean",
    "text": "Interpretation Method 1: Marginal Effect at the Mean\n\nWe simply calculate the change in predicted probability for a change in an x-variable at a particular value.\n\n\n\n\n\n\n\n\nMarginal Effect at the Mean\n\n\nThe marginal effect at the mean equals:\n\\[\n    \\frac{\\partial P(y=1|X)}{\\partial x_k} = f(\\beta \\bar{x}) \\cdot \\beta_k\n  \\]\nwhere \\(f(\\cdot)\\) is the PDF, the derivative of the CDF \\(F\\).\nIn other words, we simply calculate the MEs with all X variables set to their sample means.\nProblem: No single observation in the data might actually have all mean values. “The average person” doesn’t exist."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#interpretation-method-2-average-marginal-effect",
    "href": "lectures/lecture6/lecture6.html#interpretation-method-2-average-marginal-effect",
    "title": "Empirical Economics",
    "section": "Interpretation Method 2: Average Marginal Effect",
    "text": "Interpretation Method 2: Average Marginal Effect\n\nThis is the modern, preferred standard. It gives the best summary of the effect for the population in the sample.\n\n\n\n\n\n\n\n\nAverage Marginal Effect\n\n\nThe Average Marginal Effect (AME) equals:\n\\[\n    \\frac{\\partial P(y=1|X)}{\\partial x_k} = \\frac{1}{N} \\sum_{i=1}^N f(\\beta x_i) \\cdot \\beta_k\n  \\]\nIn other words, we compute the marginal effect using the values of each observations, and then take the average."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#interpretation-method-3-odds-ratios-logit-only",
    "href": "lectures/lecture6/lecture6.html#interpretation-method-3-odds-ratios-logit-only",
    "title": "Empirical Economics",
    "section": "Interpretation Method 3: Odds Ratios (Logit Only)",
    "text": "Interpretation Method 3: Odds Ratios (Logit Only)\n\nFor the Logit model, we can interpret results using Odds Ratios.\n\n\n\n\n\n\n\n\nDefinition: Odds Ratio (OR)\n\n\nThe odds are defined as: \\(\\text{Odds}=\\frac{P(y=1)}{P(y=0)} = \\frac{P}{1-P}\\).\nFor example, if P=0.8, Odds = 0.8 / 0.2 = 4 (or “4 to 1”). If P=0.5, Odds = 0.5 / 0.5 = 1.\n\n\n\n\n\nThe Logit model can be written as: \\[\n  \\ln(\\frac{P}{1-P}) = \\ln(\\text{Odds}) = \\beta_0 + \\beta_1 x_1 + ...\n\\qquad(1)\\]\nThe model is linear in the log-odds."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#interpretation-method-odds-ratio-cont.",
    "href": "lectures/lecture6/lecture6.html#interpretation-method-odds-ratio-cont.",
    "title": "Empirical Economics",
    "section": "Interpretation Method: Odds Ratio (Cont.)",
    "text": "Interpretation Method: Odds Ratio (Cont.)\n\nExponentiating both sides of Equation 1 gives: \\(\\text{Odds}=e^{\\beta_0 + \\beta_1 X_1 + \\dots}\\)\nThe Odds Ratio compares the odds for two different values of a predictor \\(x_j\\). Suppose we increase \\(x_j\\) by 1 unit while holding other predictors constant: \\[\n  \\text{OR}_j = \\frac{\\text{Odds} (X_j + 1)}{\\text{Odds} (X_j)} = e^{\\beta_j}\n\\]"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#odds-ratios-interpretation",
    "href": "lectures/lecture6/lecture6.html#odds-ratios-interpretation",
    "title": "Empirical Economics",
    "section": "Odds Ratios Interpretation",
    "text": "Odds Ratios Interpretation\n\nInterpretation: A one-unit increase in \\(x_k\\) multiplies the odds of success by a factor of \\(e^{\\beta_k}\\).\n\n\n\n\n\n\n\n\nExample: Odds Ratios\n\n\nIf \\(\\hat{\\beta}_k = 0.2\\), then \\(e^{0.2} \\approx 1.22\\). A one-unit increase in \\(x_k\\) increases the odds of the outcome by 22%.\nIf \\(\\hat{\\beta}_k = -0.5\\), then \\(e^{-0.5} \\approx 0.61\\). A one-unit increase in \\(x_k\\) decreases the odds by about 39% (multiplies them by 0.61)."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#interaction-terms",
    "href": "lectures/lecture6/lecture6.html#interaction-terms",
    "title": "Empirical Economics",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\nInteraction effects in probit/logit models are also not straightforward:\n\n\n\n\n\n\n\n\nInteraction Effects in Logit/Probit\n\n\nSuppose our model has two independent variables and an interaction effect, i.e. \\(P(y | x_1, x_2) = F(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2)\\).\nWe already know the marginal effect (ME) of \\(x_1\\) on the conditional probability is the partial derivative of P(y=1 | X) with respect to \\(x_1\\):\n\\(ME_{x_1} = f(\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2) \\cdot (\\beta_1 + \\beta_3 x_2)\\)\nNote that the marginal effect of x₁ is not constant; it depends on the value of x₂ (due to the \\(\\beta_3 x_2\\) term) and on the values of all variables through the \\(f(\\cdot)\\) term."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#interaction-terms-cont.",
    "href": "lectures/lecture6/lecture6.html#interaction-terms-cont.",
    "title": "Empirical Economics",
    "section": "Interaction Terms (Cont.)",
    "text": "Interaction Terms (Cont.)\n\n\n\n\n\n\n\nInteraction Effects in Logit/Probit (Cont.)\n\n\nThe interaction effect is defined as the change in the marginal effect of one variable, x₁, as the other variable, x₂, changes. This is the cross-partial derivative:\nInteraction Effect (IE) = ∂(ME_{x₁})/∂x₂ = ∂²P/∂x₂∂x₁\nWe differentiate ME_{x₁} with respect to x₂ using the product rule, where u = g(Z) and v = (β₁ + β₃x₂):\nIE = [∂g(Z)/∂x₂] ⋅ (β₁ + β₃x₂) + g(Z) ⋅ [∂(β₁ + β₃x₂)/∂x₂]\nLet’s compute each component: 1. ∂(β₁ + β₃x₂)/∂x₂ = β₃ 2. ∂g(Z)/∂x₂ requires the chain rule again: [dg(Z)/dZ] ⋅ [∂Z/∂x₂] = g'(Z) ⋅ (β₂ + β₃x₁)\nSubstituting these back into the product rule equation:\nIE = [ g'(Z) ⋅ (β₂ + β₃x₁) ] ⋅ (β₁ + β₃x₂) + g(Z) ⋅ β₃\nRearranging to clarify the components:\nIE = g(Z)β₃ + g'(Z)(β₁ + β₃x₂)(β₂ + β₃x₁)\n\n\n\n\n4. Analysis of the Result\nThis formal result is profoundly different from the coefficient β₃. It consists of two parts:\n\ng(Z)β₃: The “direct” effect, which is the interaction coefficient β₃ scaled by the PDF g(Z). This part’s sign is determined by the sign of β₃, but its magnitude depends on all covariates X through Z.\ng'(Z)(β₁ + β₃x₂)(β₂ + β₃x₁): The “non-linearity” or “functional form” effect. This term arises purely because g(Z) is not a constant. Critically, this term exists even if β₃ = 0. In that case, the interaction effect would be g'(Z)β₁β₂, which is generally non-zero. This means that an interaction effect on the probabilities is an inherent feature of non-linear models, regardless of whether an interaction term is included in the linear index Z.\n\nConclusion: The sign, magnitude, and even statistical significance of the true interaction effect on the probability can differ from that of the coefficient β₃. One cannot simply inspect β₃ to understand the interaction."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#contrast-with-the-linear-probability-model-lpm",
    "href": "lectures/lecture6/lecture6.html#contrast-with-the-linear-probability-model-lpm",
    "title": "Empirical Economics",
    "section": "Contrast with the Linear Probability Model (LPM)",
    "text": "Contrast with the Linear Probability Model (LPM)\n\nFor the LPM, the model is \\(P(y=1 | X) = \\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2 + \\alpha_3 x_1 x_2\\).\nIn the LPM, the interaction effect on the probability is precisely and unambiguously equal to the coefficient on the interaction term, \\(\\alpha_3\\).\n\nThis starkly contrasts with the result from the probit/logit model, highlighting the interpretational complexity introduced by non-linearity."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#panel-data-and-fixed-effects",
    "href": "lectures/lecture6/lecture6.html#panel-data-and-fixed-effects",
    "title": "Empirical Economics",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#goodness-of-fit-and-testing",
    "href": "lectures/lecture6/lecture6.html#goodness-of-fit-and-testing",
    "title": "Empirical Economics",
    "section": "Goodness-of-Fit and Testing",
    "text": "Goodness-of-Fit and Testing\nHow well does our model fit the data?\n\nPercent Correctly Predicted:\n\nIf \\(\\hat{p}_i &gt; 0.5\\), predict \\(y=1\\). If \\(\\hat{p}_i \\le 0.5\\), predict \\(y=0\\).\nCompare predictions to actual outcomes. What percentage did we get right?\nIntuitive, but sensitive to the 0.5 cutoff.\n\nPseudo R-Squared:\n\nSeveral versions exist, like McFadden’s R-squared.\n\\(R^2_{McF} = 1 - \\frac{\\ln L_{full}}{\\ln L_{null}}\\) (where \\(L_{null}\\) is from a model with only an intercept).\nRanges from 0 to 1, but values are much lower than OLS R-squared. A value of 0.2-0.4 can indicate a very good fit. Do not compare to OLS R-squared!\n\nLikelihood Ratio (LR) Test:\n\nTests the joint significance of a set of variables (like the F-test in OLS).\nCompares the log-likelihood of the restricted model to the unrestricted (full) model.\n\\(LR = 2(\\ln L_{full} - \\ln L_{restricted})\\) which follows a \\(\\chi^2\\) distribution."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#choosing-between-probit-and-logit",
    "href": "lectures/lecture6/lecture6.html#choosing-between-probit-and-logit",
    "title": "Empirical Economics",
    "section": "Choosing Between Probit and Logit",
    "text": "Choosing Between Probit and Logit\n\nIn practice, the choice rarely matters much.\n\nThe Normal and Logistic distributions are very similar, except the Logistic has slightly “fatter tails” (it’s less sensitive to outliers).\nPredicted probabilities from both models are usually very close.\n\nRule of Thumb:\n\nLogit coefficients are larger than Probit coefficients by a factor of ~1.6 - 1.8.\nLogit Marginal Effects \\(\\approx\\) Probit Marginal Effects.\nLogit is often preferred due to the simpler interpretation of coefficients as log-odds and the direct calculation of odds ratios."
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#logit-and-probit-in-software",
    "href": "lectures/lecture6/lecture6.html#logit-and-probit-in-software",
    "title": "Empirical Economics",
    "section": "Logit and Probit in Software",
    "text": "Logit and Probit in Software"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#the-big-picture-generalized-linear-models-glm",
    "href": "lectures/lecture6/lecture6.html#the-big-picture-generalized-linear-models-glm",
    "title": "Empirical Economics",
    "section": "The Big Picture: Generalized Linear Models (GLM)",
    "text": "The Big Picture: Generalized Linear Models (GLM)\nProbit and Logit aren’t just one-off tricks. They are part of a larger family of models called Generalized Linear Models (GLM).\nA GLM has three components: 1. Random Component: The probability distribution of the dependent variable. * Example: Bernoulli for Binary, Normal for OLS, Poisson for count data. 2. Systematic Component: A linear predictor. * Example: \\(\\eta_i = \\beta_0 + \\beta_1 x_{1i} + ...\\) (this is the same for all models). 3. Link Function, g(·): Connects the expected value of y to the linear predictor. * \\(g(E[y_i]) = \\eta_i\\)\n\n\n\nModel\nDistribution (Random)\nLink Function\n\n\n\n\nOLS\nNormal\nIdentity (\\(E[y] = \\eta\\))\n\n\nLogit\nBernoulli\nLogit (\\(\\ln(\\frac{p}{1-p}) = \\eta\\))\n\n\nProbit\nBernoulli\nProbit (\\(\\Phi^{-1}(p) = \\eta\\))\n\n\n\nThis framework unifies many different regression models!"
  },
  {
    "objectID": "lectures/lecture6/lecture6.html#what-did-we-do",
    "href": "lectures/lecture6/lecture6.html#what-did-we-do",
    "title": "Empirical Economics",
    "section": "What did we do?",
    "text": "What did we do?\n\nBinary outcomes are everywhere. Standard OLS (the LPM) is a simple starting point but is fundamentally flawed (out-of-range predictions, bad errors).\nProbit and Logit are the standard solutions. They are derived from a latent variable model and use a non-linear CDF to constrain predictions between 0 and 1.\nThey are estimated using Maximum Likelihood Estimation (MLE), which finds the parameters that make the observed data most probable.\nInterpretation is key!\n\nRaw coefficients are not marginal effects.\nUse Average Marginal Effects (AMEs) to talk about changes in probability.\nUse Odds Ratios for Logit models for a multiplicative interpretation.\n\nThese models are special cases of the powerful Generalized Linear Model (GLM) framework."
  },
  {
    "objectID": "mockexams.html",
    "href": "mockexams.html",
    "title": "Mock Exam",
    "section": "",
    "text": "Questions\nAnswers"
  },
  {
    "objectID": "mockexams.html#mid-term",
    "href": "mockexams.html#mid-term",
    "title": "Mock Exam",
    "section": "",
    "text": "Questions\nAnswers"
  },
  {
    "objectID": "mockexams.html#end-term",
    "href": "mockexams.html#end-term",
    "title": "Mock Exam",
    "section": "End-term",
    "text": "End-term\n\nQuestions\nAnswers"
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#the-nature-of-statistics-and-parameters",
    "href": "tutorials/tutorial2/tutorial2.html#the-nature-of-statistics-and-parameters",
    "title": "Empirical Economics",
    "section": "The Nature of Statistics and Parameters",
    "text": "The Nature of Statistics and Parameters\nThe lecture states that a parameter is a fixed value, while a statistic is a random variable.\nEplain why this is the case. Use the example of the average height of all citizens in a country versus the average height of a 1,000-person sample to illustrate your explanation."
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#constructing-an-exact-sampling-distribution",
    "href": "tutorials/tutorial2/tutorial2.html#constructing-an-exact-sampling-distribution",
    "title": "Empirical Economics",
    "section": "Constructing an Exact Sampling Distribution",
    "text": "Constructing an Exact Sampling Distribution\nConsider a tiny population consisting of only five numbers: [10, 20, 30, 40, 50].\n\nCalculate the true population mean, \\(\\mu\\).\nNow, consider taking random samples of size n=2 without replacement. List all possible unique samples you can draw. (Hint: The number of combinations is “5 choose 2”).*\nCalculate the sample mean (\\(\\bar{x}\\)) for each of these possible samples.\nThe list of sample means you created in part (c) is the exact sampling distribution of the sample mean. Present this distribution as a frequency table.\nIs the mean of this sampling distribution equal to the true population mean you calculated in part (a)?\n\n*: You can use comb() in R or itertools.combinations in Python."
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#sec-simulation",
    "href": "tutorials/tutorial2/tutorial2.html#sec-simulation",
    "title": "Empirical Economics",
    "section": "Simulating a Sampling Distribution",
    "text": "Simulating a Sampling Distribution\nThe lecture demonstrated simulating the sampling distribution from a Normal population. Your task is to do the same for a different type of population.\nAssume the population data follows a highly skewed Exponential distribution. The exponential distribution is often used to model time until an event.\n\nSimulate a population, i.e. a large number of draws.*\nDraw 5,000 different random samples, each of size n=50, from this population.**\nFor each of the 5,000 samples, calculate its sample mean.\nPlot a histogram of the 5,000 sample means you calculated.\nDescribe the shape of the histogram of sample means. Does it look like the original Exponential distribution, or does it look like something else?\n\n\n*: In Python, you can simulate a large number of draws from an exponential distribution using numpy.random.exponential(scale=10, size=100000). Let’s say the scale parameter (which is the true mean, \\(\\mu\\)) is 10.\n**: You can use np.random.choice() in Python or sample() in R"
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#applying-the-clt-conceptually",
    "href": "tutorials/tutorial2/tutorial2.html#applying-the-clt-conceptually",
    "title": "Empirical Economics",
    "section": "Applying the CLT Conceptually",
    "text": "Applying the CLT Conceptually\nA logistics company knows that the weight of packages it processes is not normally distributed; it is right-skewed with a long tail (a few very heavy packages). The true mean weight of all packages is \\(\\mu = 8\\) kg, and the true standard deviation is \\(\\sigma = 5\\) kg.\nThe company takes a random sample of 100 packages.\n\nAccording to the Central Limit Theorem, what can you say about the shape of the sampling distribution of the sample mean weight (\\(\\bar{x}\\))?\nWhat will be the theoretical mean of this sampling distribution?\nWhat will be the theoretical standard deviation of this sampling distribution (i.e., the standard error)?"
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#verifying-the-clt-in-pythonrstata",
    "href": "tutorials/tutorial2/tutorial2.html#verifying-the-clt-in-pythonrstata",
    "title": "Empirical Economics",
    "section": "Verifying the CLT in Python/R/Stata",
    "text": "Verifying the CLT in Python/R/Stata\nLet’s use the simulation from Slide 1.3 to verify the predictions of the Central Limit Theorem. Your population is from an Exponential distribution with a true mean (\\(\\mu\\)) of 10 and a true standard deviation (\\(\\sigma\\)) also of 10. Your sample size was n=50.\n\nFrom the list of 5,000 sample means you generated in Exercise 4, calculate the empirical mean (the average of all your sample means) and the empirical standard deviation.\nAccording to the CLT, what should the theoretical mean of the sampling distribution be?\nAccording to the CLT, what should the theoretical standard deviation of the sampling distribution (the standard error) be? The formula is \\(\\sigma / \\sqrt{n}\\).\nCompare the empirical results from part (a) with the theoretical results from parts (b) and (c). Are they close?"
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#formulating-hypotheses",
    "href": "tutorials/tutorial2/tutorial2.html#formulating-hypotheses",
    "title": "Empirical Economics",
    "section": "Formulating Hypotheses",
    "text": "Formulating Hypotheses\nFor each research question below, formulate the appropriate null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_A\\)). State whether a one-sided or a two-sided test would be more appropriate and justify your choice.\n\nScenario A: A city’s water department wants to know if the average daily water consumption per household has changed from last year’s average of 350 gallons.\nScenario B: A pharmaceutical company has developed a new drug to lower blood pressure. They want to test if the drug is effective, meaning it reduces blood pressure compared to a placebo.\nScenario C: An online retailer is testing a new website design. They want to know if the new design has a different conversion rate (proportion of visitors who make a purchase) than the old design’s rate of 15%."
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#interpretation-and-calculation",
    "href": "tutorials/tutorial2/tutorial2.html#interpretation-and-calculation",
    "title": "Empirical Economics",
    "section": "Interpretation and Calculation",
    "text": "Interpretation and Calculation\nA car manufacturer claims its new hybrid model has a mean fuel efficiency of 50 miles per gallon (mpg). A consumer watchdog group tests a random sample of n=36 cars and finds a sample mean efficiency of \\(\\bar{x}\\) = 48.5 mpg. Assume the population standard deviation for fuel efficiency is known to be \\(\\sigma = 6\\) MPG.\nThe consumer watchdog group wants to test if the manufacturer’s claim is overstated (i.e., if the true mean is less than 50 MPG). They set \\(\\alpha = 0.05\\).\n\nState the null (\\(H_0\\)) and alternative (\\(H_A\\)) hypotheses for this test.\nCalculate the standard error of the sample mean.\nCalculate the Z-test statistic using the formula: \\(Z = (\\bar{x} - \\mu_0) / SE\\).\nUsing your Z-statistic, find the corresponding p-value. (You may need a Z-table or a statistical function for this).\nBased on your p-value and the significance level of \\(\\alpha=0.05\\), what is your conclusion? Do you reject or fail to reject the null hypothesis?"
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#performing-a-hypothesis-test",
    "href": "tutorials/tutorial2/tutorial2.html#performing-a-hypothesis-test",
    "title": "Empirical Economics",
    "section": "Performing a Hypothesis Test",
    "text": "Performing a Hypothesis Test\nUse the WAGE2.DTA dataset. Import it in R/Python/Stata. Conduct a hypothesis test on the IQ variable. The null hypothesis is that mu_0 = 100.\n\nCalculate the standard error and the Z-test statistic in Python.\nThe p-value for a two-tailed test is the area under the standard normal curve that is more extreme (on both sides) than your calculated Z-statistic. Use scipy.stats.norm.cdf() or pnorm() to find this p-value."
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#correct-interpretation",
    "href": "tutorials/tutorial2/tutorial2.html#correct-interpretation",
    "title": "Empirical Economics",
    "section": "Correct Interpretation",
    "text": "Correct Interpretation\nA researcher calculates a 95% confidence interval for the average number of hours students at a university study per week and gets [12.5, 15.0] hours.\nWhich of the following statements is the correct interpretation of this result? Explain why the other statement is incorrect.\n\nStatement 1: “There is a 95% probability that the true average study time for all students at the university is between 12.5 and 15.0 hours.”\nStatement 2: “We are 95% confident that the method we used to generate this interval captures the true average study time for all students at the university.”"
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#factors-affecting-interval-width",
    "href": "tutorials/tutorial2/tutorial2.html#factors-affecting-interval-width",
    "title": "Empirical Economics",
    "section": "Factors Affecting Interval Width",
    "text": "Factors Affecting Interval Width\nWithout doing any calculations, explain how the width of a confidence interval would be affected by the following changes, assuming all other factors remain constant.\n\nIncreasing the confidence level from 90% to 99%.\nIncreasing the sample size from 100 to 400.\nThe sample having a larger standard deviation."
  },
  {
    "objectID": "tutorials/tutorial2/tutorial2.html#constructing-a-confidence-interval",
    "href": "tutorials/tutorial2/tutorial2.html#constructing-a-confidence-interval",
    "title": "Empirical Economics",
    "section": "Constructing a Confidence Interval",
    "text": "Constructing a Confidence Interval\nYou are given the same data from the fuel efficiency test: a sample of n=36 cars yielded a sample mean of \\(\\bar{x}\\) = 48.5 MPG, and the population standard deviation is \\(\\sigma = 6\\) MPG.\n\nWrite a Python script to calculate a 95% confidence interval for the true mean fuel efficiency.\n\nFind the point estimate.\nCalculate the standard error.\nFind the critical Z-value for a 95% CI. (Hint: for a 95% CI, you need the value that leaves 2.5% in each tail. You can use scipy.stats.norm.ppf(0.975)).\nCalculate the margin of error (Critical Value * Standard Error).\nConstruct the interval: Point Estimate ± Margin of Error.\n\nNow, calculate a 99% confidence interval using the same data. Is it wider or narrower than the 95% interval?\n\n\n\n\n\nEmpirical Economics: Tutorial 2 - Statistics and Probability"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#set-up",
    "href": "tutorials/tutorial1/tutorial1.html#set-up",
    "title": "Empirical Economics",
    "section": "Set-Up",
    "text": "Set-Up\nEmpirical Economics has lectures and tutorials\nBoth are important for the exam:\n\nLectures feature theoretical material\nTutorials recapitulate theoretical material\n\nThese tutorials also feature exercises, sometimes with code (R/Python/Stata). Make sure to bring your laptop.\nExercises help you understand the sometimes abstract theoretical materials. Exercise answers will be available on Brightspace after the last tutorials in a week.\n\nCode and coding exercises will allow you to practice with the material and obtain practical programming experience\nIn total, we have 8 tutorials, one each week"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#contact-points",
    "href": "tutorials/tutorial1/tutorial1.html#contact-points",
    "title": "Empirical Economics",
    "section": "Contact Points",
    "text": "Contact Points\nCourse coordinator:\n\nBas Machielsen (a.h.machielsen@uu.nl)\n\nTutorial teachers:\n\nTeacher 1: x\nTeacher 2: x\nTeacher 3: z\n\nFurther information about course organization: See course manual (Brightspace)"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#this-week",
    "href": "tutorials/tutorial1/tutorial1.html#this-week",
    "title": "Empirical Economics",
    "section": "This Week",
    "text": "This Week\nWe set up Python/Stata/R.\nWe focus on downloading and importing datasets.\nWe focus on statistics and probability. Both “on pen and paper” and “with the help of your laptop”.\n\nThese are important building blocks for the linear model (week 3)\nWhich in turn is an important building block for more complicated models later on in the course"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#setting-up-pythonstatar",
    "href": "tutorials/tutorial1/tutorial1.html#setting-up-pythonstatar",
    "title": "Empirical Economics",
    "section": "Setting Up Python/Stata/R",
    "text": "Setting Up Python/Stata/R\n\nPythonRStata\n\n\n\nThis guide will walk you through installing a powerful and popular setup for Python programming. We’ll use Miniconda to manage our Python environment and Visual Studio Code (VS Code) as our code editor.\n\nMiniconda makes managing Python packages (like pandas for data or matplotlib for plots) incredibly easy and avoids conflicts. We’ll use Miniconda, a lightweight version of Anaconda.\nVSCode is a free, modern, and highly customizable code editor that works beautifully with Python.\n\nFollow the steps for your operating system to set up your environment.\nOn Windows\nStep 1: Install Miniconda (Python)\n\nGo to the Miniconda download page: https://docs.conda.io/en/latest/miniconda.html\nDownload the latest Python 3.x Windows 64-bit installer (it will be a .exe file).\nRun the installer. When prompted, we recommend these settings:\n\nInstall for: “Just Me” (the default).\nDestination Folder: Leave as the default.\nAdvanced Options: Leave “Add Miniconda3 to my PATH” unchecked. The recommended option is to use Miniconda from the “Anaconda Prompt.”\n\n\nStep 2: Install Visual Studio Code (VS Code)\n\nGo to the VS Code download page: https://code.visualstudio.com/\nClick the big blue “Download for Windows” button.\nRun the installer. Accept the license agreement and click Next on all screens. We recommend keeping the default settings, especially “Add to PATH”, which should be checked by default.\n\nOn macOS\nStep 1: Install Miniconda (Python)\n\nGo to the Miniconda download page: https://docs.conda.io/en/latest/miniconda.html\nDownload the installer for your Mac’s chip:\n\nApple M1/M2/M3: Download the “macOS Apple M1 64-bit pkg” installer.\nIntel Chip: Download the “macOS Intel x86 64-bit pkg” installer.\n\nRun the installer (.pkg file). Click Continue and Agree through the prompts, using the default settings. It will install for the current user.\n\nStep 2: Install Visual Studio Code (VS Code)\n\nGo to the VS Code download page: https://code.visualstudio.com/\nClick the “Download Mac Universal” button.\nThis will download a .zip file. Unzip it to get the Visual Studio Code.app.\nDrag Visual Studio Code.app into your Applications folder. This makes it easy to find and launch.\n\nStep 3: Setting up VS Code for Python\nNow that everything is installed, let’s connect them.\n\nOpen VS Code.\nInstall the Python Extension:\n\nClick on the Extensions icon on the left-hand sidebar (it looks like four squares).\nIn the search bar, type Python.\nFind the one by Microsoft and click the Install button.\n\nSelect your Conda Python Interpreter:\n\nOpen the Command Palette by pressing Ctrl+Shift+P (on Windows) or Cmd+Shift+P (on Mac).\nType Python: Select Interpreter and press Enter.\nA list of available Python versions will appear. Choose the one that says ‘base’: conda. It should point to your Miniconda installation.\n\n\nYou are now set up! VS Code knows where to find your Conda Python installation.\nStep 4: Your First Project - Reading a Dataset\nLet’s perform our first real-world task: opening a project folder and reading data from a file.\nFirst, create a place for your project on your computer (e.g., in Documents or on your Desktop).\n\nCreate a new folder named MyFirstProject.\nInside MyFirstProject, create a new text file and name it main.py.\nInside MyFirstProject, create another new folder named data.\nInside the data folder, create a new text file named students.csv. Open it with a basic text editor (like Notepad or TextEdit) and paste the following content:\n\nStudentID,Name,Score\n101,Alice,88\n102,Bob,92\n103,Charlie,75\n104,Diana,95\nYour folder structure should look like this:\nMyFirstProject/\n├── main.py\n└── data/\n    └── students.csv\nStep 5: Open the Project in VS Code\nThis is a key step! Don’t just open the .py file. Open the entire folder.\n\nIn VS Code, go to File &gt; Open Folder….\nNavigate to and select your MyFirstProject folder. Click “Open” (or “Select Folder”).\nThe VS Code sidebar will now show your project files (main.py and the data folder).\n\nStep 6: Install a Package and Write the Code\nWe need the pandas library to read the .csv file. Let’s install it.\n\nIn VS Code, open a new terminal by going to Terminal &gt; New Terminal.\nA terminal will appear at the bottom of the screen. You should see (base) at the start of the prompt, indicating your Conda environment is active.\nType the following command and press Enter: conda install pandas Type y and press Enter if it asks you to proceed.\n\nNow, click on main.py in the sidebar to open it and paste this code:\n# 1. Import the pandas library, which is a powerful tool for data analysis.\n# We give it the nickname 'pd' to make it easier to use.\nimport pandas as pd\n\n# 2. Define the path to our dataset.\n# Because we opened the 'MyFirstProject' folder, we can use a relative path.\n# This means \"look inside the 'data' folder for the 'students.csv' file\".\nfile_path = 'data/students.csv'\n\n# 3. Use pandas to read the CSV file into a data structure called a DataFrame.\nprint(\"Reading the dataset...\")\nstudent_data = pd.read_csv(file_path)\n\n# 4. Print the first 5 rows of the data to see if it loaded correctly.\nprint(\"Here is the top of the dataset:\")\nprint(student_data.head())\n\n# 5. Print a success message!\nprint(\"\\nSuccessfully imported and displayed the dataset!\")\nStep 8: Run Your Python Script!\nLook for the Play button (a white triangle) in the top-right corner of the VS Code window. Click it.\nThe Terminal at the bottom will spring to life and you will see the output:\nReading the dataset...\nHere is the top of the dataset:\n   StudentID     Name  Score\n0        101    Alice     88\n1        102      Bob     92\n2        103  Charlie     75\n3        104    Diana     95\n\nSuccessfully imported and displayed the dataset!\nCongratulations! You are ready to start coding.\n\n\n\n\nThis guide will help you install the standard toolkit for statistical computing and data science: R and RStudio. You must install R first, then install RStudio. RStudio needs R to function.\nFollow the steps for your operating system.\nOn Windows\nStep 1: Install R (The Language)\n\nGo to the official R download site, CRAN (the Comprehensive R Archive Network): https://cran.r-project.org/bin/windows/base/\nClick the big link at the top that says “Download R-x.x.x for Windows”.\nRun the installer (.exe file).\nUse the default settings for all steps. Simply click Next through the installation wizard.\n\nStep 2: Install RStudio (The Dashboard)\n\nGo to the RStudio Desktop download page: https://posit.co/download/rstudio-desktop/\nThe website should automatically detect you’re on Windows. Click the “DOWNLOAD RSTUDIO DESKTOP FOR WINDOWS” button.\nRun the installer. Again, use the default settings by clicking Next until it’s finished.\n\nOn macOS\nStep 1: Install R (The Language)\n\nGo to the CRAN download page for macOS: https://cran.r-project.org/bin/macosx/\nChoose the correct package for your Mac’s chip:\n\nApple M1/M2/M3 (Apple Silicon): Download the package labeled R-x.x.x.pkg (arm64).\nIntel Chip: Download the package labeled R-x.x.x.pkg (x86_64).\n\nRun the installer (.pkg file). Click Continue and Agree through the prompts, using the default settings.\n\nStep 2: Install RStudio (The Dashboard)\n\nGo to the RStudio Desktop download page: https://posit.co/download/rstudio-desktop/\nThe website should detect you’re on a Mac. Click the “DOWNLOAD RSTUDIO DESKTOP FOR MACOS” button.\nThis will download a .dmg file. Open it.\nA window will appear. Drag the RStudio icon into your Applications folder.\n\nOpen RStudio. You’ll see four main panes (some may be combined at first):\n\nSource Editor (Top-Left): This is where you write and save your R scripts (.R files).\nConsole (Bottom-Left): This is where you can type R commands directly and where the output from your scripts will appear.\nEnvironment/History (Top-Right): Environment shows all the objects (like datasets) you’ve created. History shows your past commands.\nFiles/Plots/Packages (Bottom-Right): Files shows the files in your project folder. Plots is where your graphs will appear. Packages lets you manage your installed R packages.\n\nUsing RStudio Projects is the best way to keep your work organized. A project keeps all your files, scripts, and data together.\nStep 3: Create an RStudio Project\n\nIn RStudio, go to File &gt; New Project….\nChoose New Directory.\nChoose New Project.\nFor “Directory name”, type MyFirstRProject.\nFor “Create project as subdirectory of”, click Browse… and choose a location like your Desktop or Documents folder.\nClick Create Project.\n\nRStudio will now restart and open your new project. Notice the Files pane (bottom-right) now shows the contents of your MyFirstRProject folder.\nStep 4: Create Your Folder and Data File\nLet’s create the same structure as our Python example.\n\nIn the Files pane (bottom-right), click the New Folder button and name it data.\nNow, create the data file. On your computer (outside of RStudio), navigate into your new MyFirstRProject folder, then into the data folder.\nCreate a new text file named students.csv. Open it with a basic text editor (like Notepad or TextEdit) and paste the following content:\n\nStudentID,Name,Score\n101,Alice,88\n102,Bob,92\n103,Charlie,75\n104,Diana,95\nYour folder structure is now:\nMyFirstRProject/\n├── MyFirstRProject.Rproj  (RStudio creates this for you)\n└── data/\n    └── students.csv\nStep 5: Create an R Script and Write the Code\n\nIn RStudio, go to File &gt; New File &gt; R Script. A blank script will open in the Source editor.\nSave the file by clicking the floppy disk icon or pressing Ctrl+S (Windows) / Cmd+S (Mac). Name it main.R.\n\nNow, paste this code into your main.R script:\n# 1. Define the path to our dataset.\n# Because we are using an RStudio Project, we can use a relative path.\n# This means \"look inside the 'data' folder for the 'students.csv' file\".\nfile_path &lt;- \"data/students.csv\"\n\n# 2. Use R's built-in function to read the CSV file.\n# The data is stored in an object called a \"data frame\".\nprint(\"Reading the dataset...\")\nstudent_data &lt;- read.csv(file_path)\n\n# 3. Print the first 6 rows of the data to see if it loaded correctly.\n# The head() function is great for peeking at your data.\nprint(\"Here is the top of the dataset:\")\nprint(head(student_data))\n\n# 4. Print a success message!\nprint(\"Successfully imported and displayed the dataset!\")\nStep 6: Run Your R Script!\nThere are two easy ways to run your code:\n\nRun the whole script (recommended): Click the Source button at the top of your script editor. This will execute the entire file from top to bottom.\nRun line-by-line: Place your cursor on a line and press Ctrl+Enter (on Windows) or Cmd+Enter (on Mac). This is great for debugging.\n\nThe output will appear in the Console (bottom-left):\n[1] \"Reading the dataset...\"\n[1] \"Here is the top of the dataset:\"\n  StudentID    Name Score\n1       101   Alice    88\n2       102     Bob    92\n3       103 Charlie    75\n4       104   Diana    95\n[1] \"Successfully imported and displayed the dataset!\"\nCongratulations! You have installed R and RStudio, and are ready for data analysis.\n\n\n\n\nThis guide covers getting started with Stata for data analysis. Stata is commercial software, so you must have a license from UU.\nStep 1: Installation (Windows & macOS):\nObtain Software: Get the Stata installer and your license file (stata.lic) from your university’s software portal or IT department.\nInstall Stata: Run the installer, accepting the terms and choosing the Stata version you are licensed for (e.g., Stata/SE). Use all default settings.\nStep 2: Setup & First Script:\nCreate Project Folder: On your computer, create a folder named MyStataProject. Inside it, create a sub-folder named data and place your students.csv file inside.\nSet Working Directory: This is a crucial step. In Stata, go to File &gt; Change Working Directory… and select your MyStataProject folder.\nCreate and Run a Do-file:\nOpen the Do-file Editor (click the notepad-with-pencil icon).\nSave the empty file as main.do in your project folder.\nAdd these commands:\n\n* Import data from the data sub-folder\nimport delimited \"data/students.csv\", clear\n\n* Get summary statistics and view first 5 rows\nsummarize\nlist in 1/5\n\nClick the Execute (do) button in the Do-file Editor’s toolbar. The output will appear in the main Results window.\nCongratulations! You are ready to perform statistical analysis in Stata."
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#events-intersections-and-unions",
    "href": "tutorials/tutorial1/tutorial1.html#events-intersections-and-unions",
    "title": "Empirical Economics",
    "section": "Events, Intersections, and Unions",
    "text": "Events, Intersections, and Unions\nImagine you are drawing a single card from a standard 52-card deck. Let’s define two events:\n\nEvent A: The card drawn is a ‘King’.\nEvent B: The card drawn is a ‘Heart’.\n\nCalculate the following probabilities:\n\n\\(P(A)\\): The probability of drawing a King.\n\\(P(B)\\): The probability of drawing a Heart.\n\\(P(A \\cap B)\\): The probability of drawing a King AND a Heart (i.e., the King of Hearts).\n\\(P(A \\cup B)\\): The probability of drawing a King OR a Heart. Use the formula for the union of two events."
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#conditional-probability",
    "href": "tutorials/tutorial1/tutorial1.html#conditional-probability",
    "title": "Empirical Economics",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nUsing the same card-drawing experiment:\n\nCalculate \\(P(A|B)\\): The probability that the card is a King, given that you know it is a Heart.\nCalculate \\(P(B|A)\\): The probability that the card is a Heart, given that you know it is a King.\nAre events A and B independent? Justify your answer using the definition of independence (\\(P(A \\cap B) = P(A)P(B)\\))."
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#simulating-an-experiment",
    "href": "tutorials/tutorial1/tutorial1.html#simulating-an-experiment",
    "title": "Empirical Economics",
    "section": "Simulating an Experiment",
    "text": "Simulating an Experiment\nLet’s verify the theoretical results from the card-drawing experiment empirically. We will simulate drawing a card a large number of times and see if the frequencies match the probabilities.\n\nCreate a list or array representing the 52 cards. A simple way is a list of strings, e.g., ['2H', '3H', ..., 'KH', 'AH', '2D', ...].\nWrite a Python/R/Stata script to “draw” a card from the deck 100,000 times (with replacement).\nCount how many times:\n\n\nA ‘King’ was drawn (Event A).\nA ‘Heart’ was drawn (Event B).\nThe ‘King of Hearts’ was drawn (Event \\(A \\cap B\\)).\nA ‘King’ or a ‘Heart’ was drawn (Event \\(A \\cup B\\)).\n\n\nCalculate Frequencies: Divide the counts by the total number of simulations (100,000) to get the empirical probabilities. Compare these to your theoretical answers in Exercise 1.1."
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#pmf-expected-value-and-variance",
    "href": "tutorials/tutorial1/tutorial1.html#pmf-expected-value-and-variance",
    "title": "Empirical Economics",
    "section": "PMF, Expected Value, and Variance",
    "text": "PMF, Expected Value, and Variance\nAn online store has determined that the number of items a customer buys in a single visit, \\(X\\), is a discrete random variable with the following Probability Mass Function (PMF):\n\n\n\n\\(x\\) (items)\n0\n1\n2\n3\n\n\n\n\nP(X=x)\n0.1\n0.5\n0.3\n0.1\n\n\n\n\nVerify that this is a valid PMF.\nCalculate the expected number of items a customer will buy, \\(E[X]\\).\nCalculate the variance of the number of items, \\(Var(X)\\).\nWhat is the probability that a customer buys more than one item?"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#the-bernoulli-distribution-in-theory-and-programming",
    "href": "tutorials/tutorial1/tutorial1.html#the-bernoulli-distribution-in-theory-and-programming",
    "title": "Empirical Economics",
    "section": "The Bernoulli Distribution in Theory and Programming",
    "text": "The Bernoulli Distribution in Theory and Programming\nA loan has a probability of default of \\(p=0.05\\). Let \\(X\\) be a Bernoulli random variable where \\(X=1\\) if the loan defaults and \\(X=0\\) if it does not.\n\nUsing the formulas from the lecture for a Bernoulli distribution, calculate the expected value \\(E[X]\\) and the variance \\(Var(X)\\).\nUse numpy, scipy.stats or rbinom() to simulate 1,000,000 such loans.\n\n\nCalculate the sample mean and sample variance of your simulation.\nCompare your simulated results to the theoretical values. They should be very close."
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#standardization-and-z-scores",
    "href": "tutorials/tutorial1/tutorial1.html#standardization-and-z-scores",
    "title": "Empirical Economics",
    "section": "Standardization and Z-scores",
    "text": "Standardization and Z-scores\nSuppose the annual returns of a stock portfolio are normally distributed with a mean of 12% and a standard deviation of 20%. Let \\(R\\) be the random variable for the portfolio’s return. So, \\(R \\sim N(0.12, 0.20^2)\\).\n\nWhat is the Z-score for a return of 32%? What does this Z-score signify?\nWhat is the Z-score for a return of -8%?\nWhat portfolio return corresponds to a Z-score of 1.5?"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#calculating-probabilities-and-quantiles",
    "href": "tutorials/tutorial1/tutorial1.html#calculating-probabilities-and-quantiles",
    "title": "Empirical Economics",
    "section": "Calculating Probabilities and Quantiles",
    "text": "Calculating Probabilities and Quantiles\nContinuing with the portfolio from the previous exercise (\\(R \\sim N(0.12, 0.20^2)\\)):\nUse scipy.stats.norm in Python or pnorm in R to answer the following questions:\n\nWhat is the probability that the portfolio’s return is negative (\\(P(R &lt; 0)\\))?\nWhat is the probability that the return is greater than 25% (\\(P(R &gt; 0.25)\\))?\nWhat is the probability that the return is between 0% and 15% (\\(P(0 &lt; R &lt; 0.15)\\))?\nFind the 5th percentile of the return distribution. This is the value below which 5% of returns fall (often used in Value-at-Risk calculations).\n\n\n(Hint: you may need norm.cdf and norm.ppf from scipy.stats)"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#linear-combinations-of-normal-variables-in-python",
    "href": "tutorials/tutorial1/tutorial1.html#linear-combinations-of-normal-variables-in-python",
    "title": "Empirical Economics",
    "section": "Linear Combinations of Normal Variables in Python",
    "text": "Linear Combinations of Normal Variables in Python\nYou manage a portfolio consisting of two assets, Asset A and Asset B. Their annual returns are independent and normally distributed:\n\nReturn of Asset A: \\(R_A \\sim N(0.10, 0.15^2)\\)\nReturn of Asset B: \\(R_B \\sim N(0.06, 0.10^2)\\)\n\nYou create a portfolio where you invest 60% in Asset A and 40% in Asset B. The portfolio return is \\(R_P = 0.6 R_A + 0.4 R_B\\).\n\nBased on the rules for linear combinations of normal random variables, what are the expected value \\(E[R_P]\\) and variance \\(Var(R_P)\\) of the portfolio? What is the full distribution of \\(R_P\\)?\nSimulate 100,000 returns for \\(R_A\\) and \\(R_B\\).\n\n\nCalculate the portfolio return \\(R_P\\) for each of the 100,000 simulations.\nCalculate the sample mean and sample variance of your simulated \\(R_P\\).\nCompare your simulated results to your theoretical calculations from part 1.\nBonus: Plot a histogram of your simulated \\(R_P\\) to visually confirm it looks normally distributed."
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#rules-of-expectation-and-variance",
    "href": "tutorials/tutorial1/tutorial1.html#rules-of-expectation-and-variance",
    "title": "Empirical Economics",
    "section": "Rules of Expectation and Variance",
    "text": "Rules of Expectation and Variance\nLet \\(X\\) and \\(Y\\) be two random variables. You are given the following information:\n\n\\(E[X] = 10\\), \\(Var(X) = 4\\)\n\\(E[Y] = 5\\), \\(Var(Y) = 9\\)\n\\(Cov(X, Y) = -2\\)\n\nCalculate the following:\n\n\\(E[X + Y]\\)\n\\(Var(X + Y)\\)\n\\(E[3X - 2Y]\\)\n\\(Var(3X - 2Y)\\)\nRecalculate \\(Var(X+Y)\\) assuming \\(X\\) and \\(Y\\) were independent. How does it differ from your answer in part 2?"
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#interpreting-conditional-expectation",
    "href": "tutorials/tutorial1/tutorial1.html#interpreting-conditional-expectation",
    "title": "Empirical Economics",
    "section": "Interpreting Conditional Expectation",
    "text": "Interpreting Conditional Expectation\nA researcher is studying the relationship between years of education (\\(E\\)) and annual income (\\(I\\)). They model income as a random variable conditional on education.\nThe researcher finds that the conditional expectation of income given education is: \\(E[I | E=e] = 15000 + 4000e\\)\n\nWhat is the expected income for a person with 12 years of education?\nWhat is the expected income for a person with 16 years of education (a college degree)?\nThe researcher writes down the term \\(E[I|E]\\). Is this a single number or a random variable? Explain your reasoning."
  },
  {
    "objectID": "tutorials/tutorial1/tutorial1.html#univariate-statistics",
    "href": "tutorials/tutorial1/tutorial1.html#univariate-statistics",
    "title": "Empirical Economics",
    "section": "Univariate Statistics",
    "text": "Univariate Statistics\nImport a dataset with (hourly) wages, WAGE1.DTA (see Brightspace), and calculate the expected value, variance and standard deviation of the wage variable.\nSolutions:\n\nRPythonStata\n\n\n\n\nCode\nlibrary(haven)\ndataset &lt;- read_dta(\"../datafiles/WAGE1.DTA\")\nmean(dataset$wage)\n## [1] 5.896103\nvar(dataset$wage)\n## [1] 13.63888\nsd(dataset$wage)\n## [1] 3.693086\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\ndataset = pd.read_stata(\"../datafiles/WAGE1.DTA\")\nnp.mean(dataset['wage'])\n## 5.896103\nnp.var(dataset['wage'])\n## 13.612950325012207\nnp.std(dataset['wage'])\n## 3.689573287963867\n\n\n\n\nHey\n\n\n\n\n\n\n\nEmpirical Economics: Tutorial - Statistics and Probability"
  },
  {
    "objectID": "tutorials/tutorial8/tutorial8.html#regression-in-rpython",
    "href": "tutorials/tutorial8/tutorial8.html#regression-in-rpython",
    "title": "Empirical Economics",
    "section": "Regression in R/Python",
    "text": "Regression in R/Python\n\nRPython\n\n\n\nThe standard way of doing this in R:\n\n\n\n\nCall:\nlm(formula = cyl ~ mpg, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8569 -0.6484  0.1205  0.5965  1.5876 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.26068    0.59304   18.99  &lt; 2e-16 ***\nmpg         -0.25251    0.02831   -8.92 6.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.95 on 30 degrees of freedom\nMultiple R-squared:  0.7262,    Adjusted R-squared:  0.7171 \nF-statistic: 79.56 on 1 and 30 DF,  p-value: 6.113e-10\n\n\n\n\n\nThe standard way of doing this in Python:\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    cyl   R-squared:                       0.726\nModel:                            OLS   Adj. R-squared:                  0.717\nMethod:                 Least Squares   F-statistic:                     79.56\nDate:                Sun, 03 Aug 2025   Prob (F-statistic):           6.11e-10\nTime:                        13:17:15   Log-Likelihood:                -42.731\nNo. Observations:                  32   AIC:                             89.46\nDf Residuals:                      30   BIC:                             92.39\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         11.2607      0.593     18.988      0.000      10.050      12.472\nmpg           -0.2525      0.028     -8.920      0.000      -0.310      -0.195\n==============================================================================\nOmnibus:                        1.364   Durbin-Watson:                   2.000\nProb(Omnibus):                  0.506   Jarque-Bera (JB):                1.259\nSkew:                          -0.354   Prob(JB):                        0.533\nKurtosis:                       2.335   Cond. No.                         74.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "tutorials/tutorial8/tutorial8.html#regression-tables-in-rpython",
    "href": "tutorials/tutorial8/tutorial8.html#regression-tables-in-rpython",
    "title": "Empirical Economics",
    "section": "Regression Tables in R/Python",
    "text": "Regression Tables in R/Python\n\nRPythonStata\n\n\n\nThe best way to do this in R:\n\n\n\n\n\n\n\nTables in R\n\n\n\n(1)\n(2)\n\n\n\n\n(Intercept)\n11.261\n7.629\n\n\n\n(0.593)\n(1.226)\n\n\nmpg\n-0.253\n-0.154\n\n\n\n(0.028)\n(0.039)\n\n\nhp\n\n0.011\n\n\n\n\n(0.003)\n\n\nNum.Obs.\n32\n32\n\n\nR2\n0.726\n0.800\n\n\n\n\n\n\n\n\n\n\nThe best way to do this in Python:\n\n\n\n\n\n\n\nTables in Python\n\n\n\n\n\n\n\n\ncyl\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nmpg\n-0.253***\n(0.028)\n-0.154***\n(0.039)\n\n\nhp\n\n0.011**\n(0.003)\n\n\nIntercept\n11.261***\n(0.593)\n7.629***\n(1.226)\n\n\nstats\n\n\nObservations\n32\n32\n\n\nS.E. type\niid\niid\n\n\nR2\n0.726\n0.800\n\n\nAdj. R2\n0.717\n0.786\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\nEmpirical Economics: Tutorial 8 - Hands-On and Exam Prep."
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#recapitulation-of-the-lecture",
    "href": "tutorials/tutorial3/tutorial3.html#recapitulation-of-the-lecture",
    "title": "Empirical Economics",
    "section": "Recapitulation of the Lecture",
    "text": "Recapitulation of the Lecture"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#slide-1",
    "href": "tutorials/tutorial3/tutorial3.html#slide-1",
    "title": "Empirical Economics",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#slide-2",
    "href": "tutorials/tutorial3/tutorial3.html#slide-2",
    "title": "Empirical Economics",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#housing-prices",
    "href": "tutorials/tutorial3/tutorial3.html#housing-prices",
    "title": "Empirical Economics",
    "section": "Housing prices",
    "text": "Housing prices\nConsider the following regression output from a model trying to explain the price of a house: price = 300,000 + 1500 * sqmtr\nwhere price is the sale price of a house in euro, and sqmtr is the interior surface in square meters. The R-squared is 0.64.\n\nInterpret the intercept (300,000) and the slope coefficient (1,500) in plain English.\nWhat does the R-squared value of 0.64 tell us about this model?\nIf you were to re-estimate the model with price measured in thousands of euros (e.g., a 250,000 euro house becomes 250), what would the new equation be?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#log-log-model",
    "href": "tutorials/tutorial3/tutorial3.html#log-log-model",
    "title": "Empirical Economics",
    "section": "Log-Log Model",
    "text": "Log-Log Model\nSuppose you run a log-log regression to analyze the relationship between online ad spending and product sales, and you get the following result:\nlog(Sales) = 2.1 - 0.85 * log(Ad_Price)\nHow would you interpret the coefficient -0.85? What is the economic term for this value?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#error-term-and-residual",
    "href": "tutorials/tutorial3/tutorial3.html#error-term-and-residual",
    "title": "Empirical Economics",
    "section": "Error Term and Residual",
    "text": "Error Term and Residual\nWhat is the fundamental difference between the population error term (\\(u_i\\)) and the OLS residual (\\(e_i\\))?\nWhy can we observe one but not the other?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#proving-a-fundamental-ols-property",
    "href": "tutorials/tutorial3/tutorial3.html#proving-a-fundamental-ols-property",
    "title": "Empirical Economics",
    "section": "Proving a Fundamental OLS Property",
    "text": "Proving a Fundamental OLS Property\nThe lecture states that the OLS regression line always passes through the point of sample means, \\((\\bar{x}, \\bar{y})\\).\nUsing the formula for the OLS intercept estimator, \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\), prove that this is true.\nThat is, show that if you plug \\(\\bar{x}\\) into the estimated regression equation, the predicted value \\(\\hat{y}\\) is exactly \\(\\bar{y}\\)."
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#unbiasedness",
    "href": "tutorials/tutorial3/tutorial3.html#unbiasedness",
    "title": "Empirical Economics",
    "section": "Unbiasedness",
    "text": "Unbiasedness\nThe unbiasedness of the OLS estimator, \\(E(\\hat{\\beta}_1) = \\beta_1\\), is a cornerstone result that relies on the first four Gauss-Markov assumptions. Let’s prove it. Start with the formula for the slope estimator: \\[\n      \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n    \\]\n\nFirst, substitute the true population model \\(y_i = \\beta_0 + \\beta_1 x_i + u_i\\) into the numerator. Show that the estimator can be rewritten as:\n\\[ \\hat{\\beta}_1 = \\beta_1 + \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})u_i}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\]\n\n\n(Hint: You will need to use the fact that \\(\\sum(x_i - \\bar{x})(c) = 0\\) for any constant c, and that \\(\\sum(x_i - \\bar{x})(\\bar{y}) = 0\\).)\n(Hint: A useful property is \\(\\sum (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum (x_i - \\bar{x})y_i\\).)"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#unbiasedness-cont.",
    "href": "tutorials/tutorial3/tutorial3.html#unbiasedness-cont.",
    "title": "Empirical Economics",
    "section": "Unbiasedness (Cont.)",
    "text": "Unbiasedness (Cont.)\n\nNow, take the conditional expectation of your result from part (a) with respect to X (the set of all \\(x_i\\) values). Use the Zero Conditional Mean assumption, \\(E(u_i|X) = 0\\), to prove that \\(E(\\hat{\\beta}_1|X) = \\beta_1\\)."
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#omitted-variable-bias",
    "href": "tutorials/tutorial3/tutorial3.html#omitted-variable-bias",
    "title": "Empirical Economics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\n\nThis is a challenging but crucial derivation. Suppose the true population model is a multiple regression: \\[\n    y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\n  \\] However, you mistakenly estimate a simple regression, omitting \\(x_2\\): \\[\n      y = \\gamma_0 + \\gamma_1 x_1 + v\n    \\] Let \\(\\hat{\\gamma}_1\\) be the OLS estimate from your incorrect (short) regression. Show that the expected value of this estimator is:\n\\[\n    E(\\hat{\\gamma}_1) = \\beta_1 + \\beta_2 \\cdot \\delta_1\n  \\]\nwhere \\(\\delta_1\\) is the slope coefficient from an auxiliary regression of the omitted variable (\\(x_2\\)) on the included variable (\\(x_1\\)): \\(x_2 = \\delta_0 + \\delta_1 x_1 + \\text{error}\\).\n\n(Hint: Start with the formula for \\(\\hat{\\gamma}_1\\), substitute the true model for y, and then take the expectation. The term \\(\\beta_2 \\cdot \\delta_1\\) represents the omitted variable bias.)\n(Hint: \\(\\frac{\\sum(x_{1i}-\\bar{x}_1)x_{2i}}{\\sum(x_{1i}-\\bar{x}_1)^2} = \\frac{\\sum(x_{1i}-\\bar{x}_1)(x_{2i}-\\bar{x}_2)}{\\sum(x_{1i}-\\bar{x}_1)^2}\\))"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#marginal-effects",
    "href": "tutorials/tutorial3/tutorial3.html#marginal-effects",
    "title": "Empirical Economics",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nThe lecture explains how to interpret coefficients in models with different functional forms.\nUse calculus to derive the marginal effect of a change in \\(x\\) on \\(y\\) for the following two models:\n\nThe Quadratic Model: For the model \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + u\\), find the derivative \\(\\frac{dy}{dx}\\). How does your result show that the effect of a one-unit change in \\(x\\) on \\(y\\) depends on the current level of \\(x\\)?\nThe Level-Log Model: For the model \\(y = \\beta_0 + \\beta_1 \\log(x) + u\\), show that a 1% change in \\(x\\) leads to an approximate change of \\((\\beta_1/100)\\) units in \\(y\\).\n\n(Hint: For (b), recall that a change in \\(\\log(x)\\), i.e., \\(d(\\log(x))\\), is equal to \\(\\frac{dx}{x}\\), which is the proportional change in x.)"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#perfect-multicollinearity",
    "href": "tutorials/tutorial3/tutorial3.html#perfect-multicollinearity",
    "title": "Empirical Economics",
    "section": "Perfect Multicollinearity",
    "text": "Perfect Multicollinearity\nThe variance of a coefficient estimator in a multiple regression model with two variables (\\(x_1, x_2\\)) is given by:\n\\[\n    Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{SST_1 (1 - R_1^2)}\n  \\]\nwhere \\(R_1^2\\) is the R-squared from a regression of \\(x_1\\) on \\(x_2\\).\n\nWhat does it mean for \\(x_1\\) and \\(x_2\\) to have perfect multicollinearity in terms of their relationship?\nAnalytically, what happens to the value of \\(R_1^2\\) under perfect multicollinearity?\nUsing the variance formula, explain mathematically why it is impossible to calculate the OLS estimate \\(\\hat{\\beta}_1\\) in this scenario. What happens to the variance of the estimator?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#zero-conditional-mean",
    "href": "tutorials/tutorial3/tutorial3.html#zero-conditional-mean",
    "title": "Empirical Economics",
    "section": "Zero Conditional Mean",
    "text": "Zero Conditional Mean\nThe lecture states that the Zero Conditional Mean assumption (\\(E(u|x) = 0\\)) is the most crucial assumption for causality.\n\nExplain in your own words what this assumption means.\nUsing the lecture’s example of wage on education, explain why “innate ability” is a potential unobserved factor (\\(u\\)) that likely violates this assumption.\nIf higher ability is positively correlated with both education and wages, in which direction will the OLS estimate for the effect of education on wages (\\(\\hat{\\beta}_1\\)) be biased? Explain your reasoning."
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#variance-of-the-ols-estimator",
    "href": "tutorials/tutorial3/tutorial3.html#variance-of-the-ols-estimator",
    "title": "Empirical Economics",
    "section": "Variance of the OLS Estimator",
    "text": "Variance of the OLS Estimator\nThe variance of the OLS estimator in a simple linear regression is given by: \\(Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{SST_x}\\).\nImagine you are a researcher designing an experiment to find the causal effect of fertilizer (\\(x\\)) on crop yield (\\(y\\)).\nUsing this formula as your guide, what two things could you do in your experimental design to increase the precision of your estimate, \\(\\hat{\\beta}_1\\)?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#r-squared",
    "href": "tutorials/tutorial3/tutorial3.html#r-squared",
    "title": "Empirical Economics",
    "section": "R-squared",
    "text": "R-squared\nA junior analyst tells you, “My model is great, it has an R-squared of 0.92!”\nWhy is a high R-squared not necessarily the ultimate goal of an econometric analysis, especially if we are interested in making policy decisions based on one specific variable?\nWhat is often more important than a high R-squared?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#omitted-variable-bias-1",
    "href": "tutorials/tutorial3/tutorial3.html#omitted-variable-bias-1",
    "title": "Empirical Economics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nThe lecture introduces multiple regression as a way to control for other factors and mitigate omitted variable bias. Let’s return to the wage on education model.\nBesides experience (which was added in the lecture), what are two or three other variables you would want to include in the model to get a more credible estimate of the true return to education?\nWhat practical challenges might you face in obtaining data for these variables?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#ols-minimization",
    "href": "tutorials/tutorial3/tutorial3.html#ols-minimization",
    "title": "Empirical Economics",
    "section": "OLS Minimization",
    "text": "OLS Minimization\nWhy do we use the sum of squared residuals as the criterion to minimize in OLS?\nWhy not minimize the sum of the absolute values of the residuals, or just the sum of the residuals?\nWhat are the statistical and practical advantages of squaring them?"
  },
  {
    "objectID": "tutorials/tutorial3/tutorial3.html#polynomials",
    "href": "tutorials/tutorial3/tutorial3.html#polynomials",
    "title": "Empirical Economics",
    "section": "Polynomials",
    "text": "Polynomials\nThe lecture introduced polynomial terms (e.g., adding \\(x^2\\)) to model non-linear relationships.\nWhen might you suspect that a simple linear model is not sufficient and that a quadratic model (like wage on experience and experience²) would be more appropriate?\nWhat would a negative coefficient on the experience² term imply about the relationship between experience and wages?\n\n\n\n\nEmpirical Economics: Tutorial - The Linear Model"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical Economics",
    "section": "",
    "text": "Welcome to the Empirical Economics (Utrecht University) website. On this website, you can find all lecture slides, tutorial slides, tutorial answers and mock exams."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Empirical Economics",
    "section": "Syllabus",
    "text": "Syllabus\n\nThe syllabus is available here."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Empirical Economics",
    "section": "Schedule",
    "text": "Schedule\n\nThe schedule is available on your TimeTable"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Empirical Economics",
    "section": "Contact",
    "text": "Contact\n\nThe course coordinator is Bas Machielsen (a dot h dot machielsen at uu dot .nl)\nTutorial teachers:\n\nJelena\nKattia\nTina\nVincent\nMads"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#course-overview",
    "href": "lectures/lecture5/lecture5.html#course-overview",
    "title": "Empirical Economics",
    "section": "Course Overview",
    "text": "Course Overview\n\nStatistics and Probability - Basic Concepts\nStatistics and Probability - Hypothesis Testing\nThe Linear Regression Model\nTime Series Data\nPanel Data (FE) and Control Variables\nBinary Outcome Data\nPotential Outcomes and Difference-in-differences\nHands-on Econometrics in Practice"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#what-do-we-do-today",
    "href": "lectures/lecture5/lecture5.html#what-do-we-do-today",
    "title": "Empirical Economics",
    "section": "What do we do today?",
    "text": "What do we do today?\n\nWe have seen cross sectional and time series data\nThis lecture, we will talk about methods used when we can combine features of cross-sectional and time-series data\nWe will introduce two workhorse econometrics models, the Fixed Effects (FE) model, and the Random Effects (RE) model\nWe discuss a special case of panel data in the form of event studies."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#what-is-panel-data",
    "href": "lectures/lecture5/lecture5.html#what-is-panel-data",
    "title": "Empirical Economics",
    "section": "What is Panel Data?",
    "text": "What is Panel Data?\n\nA panel dataset (or longitudinal dataset) follows the same set of individuals over multiple time periods.\nStructure:\n\n\\(N\\): The number of individuals or entities (e.g., people, firms, countries, schools).\n\\(T\\): The number of time periods (e.g., years, quarters, days).\n\nThe total number of observations is \\(N \\times T\\) (for a balanced panel).\n\n\n\n\n\n\n\n\nExamples: Panel Data\n\n\nPSID (Panel Study of Income Dynamics): Tracks thousands of families and their descendants over many years.\nCompustat: Financial data for thousands of public firms over many years."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#sec-structure",
    "href": "lectures/lecture5/lecture5.html#sec-structure",
    "title": "Empirical Economics",
    "section": "Structure of Panel Data",
    "text": "Structure of Panel Data\n\nImagine tracking the GDP and foreign investment for 3 countries over 4 years.\n\n\n\n\n\nCountry (i)\nYear (t)\nGDP (\\(y_{it}\\))\nInvestment (\\(X_{it}\\))\n\n\n\n\nUSA\n2019\n21.4\n0.25\n\n\nUSA\n2020\n20.9\n0.16\n\n\nUSA\n2021\n23.0\n0.36\n\n\nUSA\n2022\n25.4\n0.13\n\n\nGermany\n2019\n3.8\n0.14\n\n\nGermany\n2020\n3.8\n0.13\n\n\n…\n…\n…\n…\n\n\nJapan\n…\n…\n…\n\n\n\n\n\nHere, \\(N=3\\) and \\(T=4\\).\nThe data has both a cross-sectional dimension (comparing USA, Germany, Japan in one year) and a time-series dimension (tracking the USA from 2019-2022)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#advantages-of-panel-data",
    "href": "lectures/lecture5/lecture5.html#advantages-of-panel-data",
    "title": "Empirical Economics",
    "section": "Advantages of Panel Data",
    "text": "Advantages of Panel Data\n\nControlling for Unobserved Heterogeneity:\n\nThis is the biggest advantage! We can control for factors that are unobserved and constant over time for each individual (e.g., intrinsic ability, corporate culture, national institutions).\nThis helps us mitigate omitted variable bias.\n\nIncreased Degrees of Freedom:\n\nPanel datasets are often much larger than pure cross-sectional or time-series datasets.\nMore data leads to more precise estimates (lower standard errors).\n\nAnalyzing Dynamics:\n\nWe can study how variables change over time and the speed of adjustment.\nFor example, how long does it take for a change in policy to affect unemployment? You can’t answer this with a single cross-section."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#notation-for-panel-data-models",
    "href": "lectures/lecture5/lecture5.html#notation-for-panel-data-models",
    "title": "Empirical Economics",
    "section": "Notation for Panel Data Models",
    "text": "Notation for Panel Data Models\n\nA standard panel data regression model is written as:\n\n\n\n\n\n\n\n\nDefinition: Panel Regression Model\n\n\n\\[ y_{it} = \\beta_0 + \\beta_1 X_{1,it} + ... + \\beta_k X_{k,it} + u_{it} \\]\nWhere:\n\n\\(i = 1, ..., N\\) indexes the individual or entity.\n\\(t = 1, ..., T\\) indexes the time period.\n\\(y_{it}\\): The dependent variable for individual \\(i\\) at time \\(t\\).\n\\(X_{k,it}\\): The \\(k^{th}\\) independent variable for individual \\(i\\) at time \\(t\\).\n\\(\\beta_k\\): The coefficient for variable \\(X_k\\).\n\\(u_{it}\\): The error term for individual \\(i\\) at time \\(t\\).\n\n\n\n\n\n\nThe key is what we assume about the error term, \\(u_{it}\\)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#the-pooled-ols-model-1",
    "href": "lectures/lecture5/lecture5.html#the-pooled-ols-model-1",
    "title": "Empirical Economics",
    "section": "The Pooled OLS Model",
    "text": "The Pooled OLS Model\n\nThe simplest approach is to ignore the panel structure entirely.\n\n\n\n\n\n\n\n\nDefinition: Pooled OLS Model\n\n\n\\[ y_{it} = \\beta_0 + \\beta_1 X_{it} + u_{it} \\]\n\n\n\n\n\nMethod:\n\nStack all \\(N \\times T\\) observations together.\nRun a single OLS regression as if it were one large cross-section.\n\nKey Assumption:\n\nThe error term \\(u_{it}\\) is uncorrelated with the regressors \\(X_{it}\\).\nThis implicitly assumes that there are no unobserved individual-specific or time-specific effects that are correlated with our \\(X\\) variables.\nThis assumption is almost always violated in practice! It ignores the very heterogeneity that panel data is designed to address, leading to biased estimates."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#unobserved-heterogeneity",
    "href": "lectures/lecture5/lecture5.html#unobserved-heterogeneity",
    "title": "Empirical Economics",
    "section": "Unobserved Heterogeneity",
    "text": "Unobserved Heterogeneity\n\nThe error term \\(u_{it}\\) in a panel model is often thought to have multiple parts:\n\\[ u_{it} = \\alpha_i + \\lambda_t + \\epsilon_{it} \\]\n\\(\\alpha_i\\): Individual-specific effect. This is an unobserved factor that is constant over time for a given individual \\(i\\), but varies across individuals.\n\\(\\lambda_t\\): Time effect. This is an unobserved factor that is constant for all individuals at a given time \\(t\\), but varies over time.\n\\(\\epsilon_{it}\\): The idiosyncratic error term that varies across both \\(i\\) and \\(t\\).\nThe core challenge of panel data is how to deal with \\(\\alpha_i\\) and \\(\\lambda_t\\)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#unobserved-heterogeneity-examples",
    "href": "lectures/lecture5/lecture5.html#unobserved-heterogeneity-examples",
    "title": "Empirical Economics",
    "section": "Unobserved Heterogeneity Examples",
    "text": "Unobserved Heterogeneity Examples\n\n\n\n\n\n\n\nExample: Unobserved Heterogeneity\n\n\n\\(\\alpha_i\\): An individual’s innate ability, a firm’s management quality, a country’s cultural norms. All of these affect a particular individual, firm, or country \\(i\\) in a way that is constant over time.\n\\(\\lambda_t\\): A global financial crisis, a major policy change, a technological shock. All of these affect an entire cross-section at particular point in time \\(t\\)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#fixed-effects",
    "href": "lectures/lecture5/lecture5.html#fixed-effects",
    "title": "Empirical Economics",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nCore Idea: Treat the individual-specific effects, \\(\\alpha_i\\), as parameters to be estimated. Each individual gets their own intercept.\n\n\n\n\n\n\n\n\nDefinition: Fixed Effects Model\n\n\n\\[ y_{it} = (\\beta_0 + \\alpha_i) + \\beta_1 X_{it} + \\epsilon_{it} \\] or more simply: \\[ y_{it} = \\alpha_i + \\beta_1 X_{it} + \\epsilon_{it} \\] (Here, the \\(\\alpha_i\\) represent the individual-specific intercepts)\n\n\n\n\n\nThe term “fixed effects” implies that we are making no assumptions about the distribution of the \\(\\alpha_i\\) or their correlation with \\(X_{it}\\). We allow them to be correlated with the regressors."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#the-within-estimator-for-fe",
    "href": "lectures/lecture5/lecture5.html#the-within-estimator-for-fe",
    "title": "Empirical Economics",
    "section": "The “Within” Estimator for FE",
    "text": "The “Within” Estimator for FE\n\nIt is easy to estimate a fixed effects model: it is actually OLS estimation on transformed data\nThis fact allows us to estimate the FE model without explicitly estimating \\(N\\) different intercepts?\nThe goal is to eliminate the fixed effect \\(\\alpha_i\\) in \\(Y_{it} = \\alpha_i + \\beta_1 X_{it} + \\epsilon_{it}\\)"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#within-transformation",
    "href": "lectures/lecture5/lecture5.html#within-transformation",
    "title": "Empirical Economics",
    "section": "Within Transformation",
    "text": "Within Transformation\n\n\n\n\n\n\n\nDefinition: Within Transformation\n\n\nStep 1: For each individual \\(i\\), calculate the time-average of their variables: \\[ \\bar{y}_i = \\frac{1}{T} \\sum_{t=1}^{T} y_{it} \\quad \\text{and} \\quad \\bar{X}_i = \\frac{1}{T} \\sum_{t=1}^{T} X_{it} \\]\nStep 2: Subtract the individual-specific average from the original model: \\[ y_{it} - \\bar{y}_i = \\beta_1 (X_{it} - \\bar{X}_i) + (\\epsilon_{it} - \\bar{\\epsilon}_i) \\] The fixed effect \\(\\alpha_i\\) is time-constant, so \\(\\alpha_i - \\bar{\\alpha}_i = \\alpha_i - \\alpha_i = 0\\). It drops out!\nStep 3: Run OLS on the “de-meaned” data: \\[ (y_{it} - \\bar{y}_i) \\text{ on } (X_{it} - \\bar{X}_i) \\] This gives a consistent estimate of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#least-squares-dummy-variable-model",
    "href": "lectures/lecture5/lecture5.html#least-squares-dummy-variable-model",
    "title": "Empirical Economics",
    "section": "Least Squares Dummy Variable Model",
    "text": "Least Squares Dummy Variable Model\n\nAn alternative, but equivalent, way to get FE estimates is the LSDV model.\n\n\n\n\n\n\n\n\nLSDV Model\n\n\nCreate a dummy (0/1) variable for each individual \\(i\\) (except for one, to avoid the dummy variable trap).\nRun a single OLS regression including these \\(N-1\\) dummy variables.\n\\[ y_{it} = \\beta_0 + \\beta_1 X_{it} + d_1\\alpha_1 + d_2\\alpha_2 + ... + d_{N-1}\\alpha_{N-1} + \\epsilon_{it} \\]\nThe estimated coefficient \\(\\beta_1\\) from the LSDV model is identical to the one from the “Within” estimator.\n\n\n\n\n\nThe coefficients on the dummies (\\(\\alpha_i\\)) are the estimated fixed effects.\nLSDV is impractical for panels with very large \\(N\\) (e.g., thousands of individuals) due to computational burden. The “Within” estimator is more efficient."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#interpretation-of-fe-coefficients",
    "href": "lectures/lecture5/lecture5.html#interpretation-of-fe-coefficients",
    "title": "Empirical Economics",
    "section": "Interpretation of FE Coefficients",
    "text": "Interpretation of FE Coefficients\n\n\n\n\n\n\n\nInterpretation of Fixed Effects models\n\n\nIn a Fixed Effects model, the coefficient \\(\\beta_1\\) measures:\nThe average change in \\(y\\) for a one-unit increase in \\(X\\) within a given individual over time.\n\n\n\n\n\nThe FE estimator uses only the variation within each individual (firm/country/etc.) to estimate the coefficients.\nIt effectively ignores variation between individuals. You are comparing individual A at time 1 to individual A at time 2, not to individual B."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#visualization-fixed-effects",
    "href": "lectures/lecture5/lecture5.html#visualization-fixed-effects",
    "title": "Empirical Economics",
    "section": "Visualization Fixed Effects",
    "text": "Visualization Fixed Effects"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#pros-and-cons-of-the-fixed-effects-model",
    "href": "lectures/lecture5/lecture5.html#pros-and-cons-of-the-fixed-effects-model",
    "title": "Empirical Economics",
    "section": "Pros and Cons of the Fixed Effects Model",
    "text": "Pros and Cons of the Fixed Effects Model\n\nPros:\n\nControls for all time-invariant omitted variables, whether observed or unobserved. This is its most powerful feature. If you are worried that unobserved ability is correlated with both education (your X) and wage (your Y), FE solves this problem because ability is constant for an individual.\nIt is consistent even if the unobserved effect \\(\\alpha_i\\) is correlated with the regressors \\(X_{it}\\).\n\nCons:\n\nCannot estimate the effect of time-invariant variables. The “Within” transformation wipes them out.\nFor example, you cannot estimate the effect of gender or race on wages using a standard FE model, because these variables do not change over time for an individual.\nMay be less efficient than the Random Effects model if its stricter assumptions hold."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#fixed-effects-in-software",
    "href": "lectures/lecture5/lecture5.html#fixed-effects-in-software",
    "title": "Empirical Economics",
    "section": "Fixed Effects in Software",
    "text": "Fixed Effects in Software\n\nArguably, the fixed effects model is one of the most often-used models in modern econometrics.\nStandard statistical software such as lm() in R or sm.OLS() in Python can implement FE using the LSDV method, but this is often tedious.\nThe fixest (R) and pyfixest (Python) package provide a very easy way to estimate FE proceding from a dataset that looks like the one on Slide 2.2.\nThis is how that works in practice:\n\n\nRPythonStata\n\n\n\nlibrary(fixest)\nmodel &lt;- feols(y ~ x1 + x2 | fe1 + fe2, data = dataset) \n# fixed effects separated from ind. vars. by |\nsummary(model)\n\n\n\n\nimport pyfixest as pf\n\nfit = pf.feols(fml=\"y ~ x1 + x2 | fe1 + fe2\", data=data)\n# fixed effects separated from the ind. vars. by | \nfit.summary()\n\n\n\n\nxtset country year *\\In order: $i$ variable, $t$ variable\nxtreg y x1 x2, fe1 fe2 *\\fixed effects separated by a comma"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#the-random-effects-re-model",
    "href": "lectures/lecture5/lecture5.html#the-random-effects-re-model",
    "title": "Empirical Economics",
    "section": "The Random Effects (RE) Model",
    "text": "The Random Effects (RE) Model\n\nCore Idea: Instead of treating \\(\\alpha_i\\) as a fixed parameter for each individual, we treat it as a random variable that is part of the composite error term.\n\n\n\n\n\n\n\n\nDefinition: Random Effects\n\n\n\\[ y_{it} = \\beta_0 + \\beta_1 X_{it} + u_{it} \\]\nWhere the composite error term is: \\[ u_{it} = \\alpha_i + \\epsilon_{it} \\]\n\n\\(\\alpha_i\\) is the random individual-specific error component.\n\\(\\epsilon_{it}\\) is the idiosyncratic error.\n\n\n\n\n\n\nThe RE model is a compromise between Pooled OLS and Fixed Effects."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#random-effects-specification",
    "href": "lectures/lecture5/lecture5.html#random-effects-specification",
    "title": "Empirical Economics",
    "section": "Random Effects Specification",
    "text": "Random Effects Specification\n\nThe Random Effects Estimator can also be written as follows:\n\\[\n  y_{it} - \\theta \\bar{y_i}  = \\beta_1( x_{1it} -\\theta\\overline{x_{1i}}) +\\dots + \\beta_k(x_{kit} - \\theta\\overline{x_{ki}}) + (u_{it} -\\theta \\bar{u}_i)\n\\]\n\\(\\bar{y_i} = (1/T) \\sum_t y_{it}\\) (the mean of \\(y\\) for individual \\(i\\)). Similar for \\(\\overline{x_{ji}}\\).\n\\(\\theta = 1 - \\sqrt{\\frac{\\sigma^2_\\epsilon}{(T_i \\sigma^2_\\alpha + \\sigma^2_\\epsilon)}}\\), with \\(\\sigma^2_{.}\\) denoting the variance of the \\(\\epsilon\\) and \\(\\alpha\\) terms respectively.\n\\(T_i\\) is the no. of observations for individual \\(i.\\) If balanced, \\(T_i = T\\)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#the-key-assumption-for-random-effects",
    "href": "lectures/lecture5/lecture5.html#the-key-assumption-for-random-effects",
    "title": "Empirical Economics",
    "section": "The Key Assumption for Random Effects",
    "text": "The Key Assumption for Random Effects\n\nFor the RE model to be valid (i.e., provide consistent estimates), we must assume:\n\n\\[ E(\\alpha_i | X_{it}) = 0 \\]\n\nIn English: The unobserved individual-specific effects (\\(\\alpha_i\\)) are uncorrelated with the explanatory variables (\\(X_{it}\\)) for all time periods.\nThis is a much stronger assumption than in the FE model.\n\nExample where it might be violated: In a wage regression, if unobserved ability (\\(\\alpha_i\\)) is correlated with education (\\(X_{it}\\)), the RE assumption is violated, and the RE estimator will be biased.\nExample where it might hold: In an experiment where treatment (\\(X_{it}\\)) is randomly assigned, the assumption would hold by design."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#estimation-of-re-models-gls",
    "href": "lectures/lecture5/lecture5.html#estimation-of-re-models-gls",
    "title": "Empirical Economics",
    "section": "Estimation of RE Models (GLS)",
    "text": "Estimation of RE Models (GLS)\n\nEstimating the RE model can “almost” be done using Pooled OLS.\n\nIf we run Pooled OLS on the data, assuming the RE model, the estimates of \\(\\beta\\) will be unbiased (if the key assumption holds), but they will be inefficient.\n\nThe composite error term \\(u_{it} = \\alpha_i + \\epsilon_{it}\\) creates serial correlation within each individual.\n\nThe error for individual \\(i\\) at time \\(t\\) is correlated with their error at time \\(t+1\\) because they share the same \\(\\alpha_i\\):\n\n\\[\\rho(u_{it}, u_{is}) \\neq 0 \\quad \\text{for } t \\neq s\\]\nSolution: Generalized Least Squares (GLS).\n\nGLS is a method that transforms the data to account for this specific error structure, producing efficient estimates.\nIn practice, we use Feasible GLS (FGLS) because we have to estimate the components of the error correlation first. This is what statistical software does automatically."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#interpretation-of-re-coefficients",
    "href": "lectures/lecture5/lecture5.html#interpretation-of-re-coefficients",
    "title": "Empirical Economics",
    "section": "Interpretation of RE Coefficients",
    "text": "Interpretation of RE Coefficients\n\nThe RE estimator uses a weighted average of the “within” and “between” variation in the data.\n\n\n\n\n\n\n\n\nInterpretation of RE Estimates\n\n\nThe coefficient \\(\\beta_1\\) from an RE model is interpreted as:\nThe estimated change in \\(y\\) for a one-unit increase in \\(X\\), assuming the unobserved individual effects \\(\\alpha_i\\) are uncorrelated with \\(X\\).\n\n\n\n\n\nIt’s a more general interpretation than the FE coefficient.\nThe reliability of this interpretation hinges entirely on the key RE assumption holding true."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#pros-and-cons-of-the-random-effects-model",
    "href": "lectures/lecture5/lecture5.html#pros-and-cons-of-the-random-effects-model",
    "title": "Empirical Economics",
    "section": "Pros and Cons of the Random Effects Model",
    "text": "Pros and Cons of the Random Effects Model\n\nPros:\n\nCan estimate the effects of time-invariant variables (e.g., gender, race, industry), because it does not wipe them out.\nMore efficient (i.e., has smaller standard errors) than the FE model, if the key assumption (\\(E(\\alpha_i | X_{it}) = 0\\)) is met. It uses both “within” and “between” variation.\n\nCons:\n\nEstimates are biased and inconsistent if the key assumption is violated. This is the critical weakness. If the unobserved effects are correlated with your regressors, the RE model suffers from omitted variable bias."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#comparing-fe-and-re-the-hausman-test",
    "href": "lectures/lecture5/lecture5.html#comparing-fe-and-re-the-hausman-test",
    "title": "Empirical Economics",
    "section": "Comparing FE and RE: The Hausman Test",
    "text": "Comparing FE and RE: The Hausman Test\n\nSo, which model should we use? FE or RE? The Hausman Test helps us decide.\nIntuition:\n\nThe FE estimator is always consistent, whether the unobserved effects \\(\\alpha_i\\) are correlated with \\(X\\) or not.\nThe RE estimator is consistent AND efficient if \\(\\alpha_i\\) and \\(X\\) are uncorrelated, but inconsistent if they are correlated.\n\nThe Test: We compare the coefficient estimates from FE and RE.\n\nIf the coefficients are “close” to each other, it suggests the RE assumption holds, and we should use the more efficient RE model.\nIf the coefficients are “far apart” and statistically different, it suggests the RE assumption is violated. We must use the consistent FE model."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#hausman-test-statistic",
    "href": "lectures/lecture5/lecture5.html#hausman-test-statistic",
    "title": "Empirical Economics",
    "section": "Hausman Test Statistic",
    "text": "Hausman Test Statistic\n\nThe test is based on the difference between the coefficient vectors from the two models: \\((\\hat{\\beta}_{FE} - \\hat{\\beta}_{RE})\\).\nThe Hausman statistic, \\(H\\), is a measure of the squared distance between the two vectors of coefficients, weighted by the precision (1/variance) of this difference.\n\nIt is constructed so that large, systematic differences between the coefficients lead to a large test statistic.\n\nUnder the null hypothesis (\\(H_0\\)), the test statistic follows a Chi-squared distribution with \\(k\\) degrees of freedom, where \\(k\\) is the number of time-varying regressors in the model.\n\n\\[ H \\sim \\chi^2(k) \\]"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#chi2-test-visualization",
    "href": "lectures/lecture5/lecture5.html#chi2-test-visualization",
    "title": "Empirical Economics",
    "section": "\\(\\chi^2\\) Test Visualization",
    "text": "\\(\\chi^2\\) Test Visualization\n\nThe \\(\\chi^2\\) distribution is one-sided.\n\nA significance level \\(\\alpha\\) gives you a critical value on the basis of which a test can be rejected.\nAlternatively, the \\(p\\)-value can be calculated according to the cdf."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#the-hausman-test-application",
    "href": "lectures/lecture5/lecture5.html#the-hausman-test-application",
    "title": "Empirical Economics",
    "section": "The Hausman Test: Application",
    "text": "The Hausman Test: Application\n\n\n\n\n\n\n\nHausman Test: Procedure\n\n\n\n\\(H_0\\): The Random Effects model is the appropriate model. (The difference in coefficients between FE and RE is not systematic, i.e., \\(E(\\alpha_i | X_{it}) = 0\\)).\n\\(H_A\\): The Fixed Effects model is the appropriate model. (The difference in coefficients is systematic, i.e., \\(E(\\alpha_i | X_{it}) \\neq 0\\)).\nStatistical software calculates a test statistic (Chi-squared) and a p-value.\n\nIf p-value &lt; 0.05 (or your chosen significance level): Reject the null hypothesis. The models are significantly different. Conclude that the RE assumption is likely violated. Use the Fixed Effects model.\nIf p-value &gt;= 0.05: Fail to reject the null hypothesis. You do not have evidence that the RE assumption is violated. Use the more efficient Random Effects model."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#the-first-difference-model",
    "href": "lectures/lecture5/lecture5.html#the-first-difference-model",
    "title": "Empirical Economics",
    "section": "The First Difference Model",
    "text": "The First Difference Model\n\nThere is another way to eliminate the fixed effect \\(\\alpha_i\\). Instead of de-meaning, we difference the data across time periods.\n\n\n\n\n\n\n\n\nFirst Differences: Procedure\n\n\nOriginal Model at time t: \\(y_{it} = \\alpha_i + \\beta_1 X_{it} + \\epsilon_{it}\\)\nModel at time t-1: \\(y_{i,t-1} = \\alpha_i + \\beta_1 X_{i,t-1} + \\epsilon_{i,t-1}\\)\nSubtracting the two: \\[\n    (y_{it} - y_{i,t-1}) = \\beta_1 (X_{it} - X_{i,t-1}) + (\\epsilon_{it} - \\epsilon_{i,t-1})\n  \\] \\[\\Delta y_{it} = \\beta_1 \\Delta X_{it} + \\Delta \\epsilon_{it}\\]\nThe fixed effect \\(\\alpha_i\\) is eliminated. We can estimate \\(\\beta_1\\) by running OLS on the differenced data."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#fd-vs.-fe-the-case-of-random-walk-errors",
    "href": "lectures/lecture5/lecture5.html#fd-vs.-fe-the-case-of-random-walk-errors",
    "title": "Empirical Economics",
    "section": "FD vs. FE: The Case of Random Walk Errors",
    "text": "FD vs. FE: The Case of Random Walk Errors\n\nThe standard Fixed Effects (FE) estimator is the most efficient linear unbiased estimator when the idiosyncratic errors, \\(\\epsilon_{it}\\), are serially uncorrelated.\n\nBut what happens if they are not?\n\n\n\n\n\n\n\n\n\nPanel Data Model with a Random Walk Error\n\n\n\nConsider the case where the error term follows a random walk:\n\n\\[ y_{it} = \\beta_1 X_{it} + \\alpha_i + \\epsilon_{it} \\]\n\nLet’s assume the error term \\(\\epsilon_{it}\\) is not i.i.d., but instead follows a random walk process. This means today’s error is equal to yesterday’s error plus a new, well-behaved shock, \\(\\nu_{it}\\).\n\n\\[ \\epsilon_{it} = \\epsilon_{i,t-1} + \\nu_{it} \\]\n\nHere, \\(\\nu_{it}\\) is a “white noise” error, meaning it is not serially correlated. The random walk structure means that \\(\\epsilon_{it}\\) is highly serially correlated."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#impact-on-the-fe-estimator",
    "href": "lectures/lecture5/lecture5.html#impact-on-the-fe-estimator",
    "title": "Empirical Economics",
    "section": "Impact on the FE Estimator",
    "text": "Impact on the FE Estimator\n\nThe FE estimator transforms the model by de-meaning the data.\nThe resulting error term is:\n\\[\n  \\tilde{\\epsilon}_{it} = \\epsilon_{it} - \\bar{\\epsilon}_i\n\\]\nIf the original error \\(\\epsilon_{it}\\) has a random walk structure, this transformed error \\(\\tilde{\\epsilon}_{it}\\) will still be serially correlated.\nWhile the FE estimator remains consistent, it is no longer efficient, and standard errors will be biased unless we use robust (clustered) standard errors."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#impact-on-the-fd-estimator",
    "href": "lectures/lecture5/lecture5.html#impact-on-the-fd-estimator",
    "title": "Empirical Economics",
    "section": "Impact on the FD Estimator",
    "text": "Impact on the FD Estimator\n\nThe First Differences (FD) estimator transforms the model by differencing the data.\nThe new error term is the difference of the original errors:\n\\[\n  \\Delta \\epsilon_{it} = \\epsilon_{it} - \\epsilon_{i,t-1}\n\\]\nNow, let’s substitute our random walk assumption into this equation:\n\\[\n  \\Delta \\epsilon_{it} = (\\epsilon_{i,t-1} + \\nu_{it}) - \\epsilon_{i,t-1}\n\\]\nThis simplifies perfectly to \\(\\Delta \\epsilon_{it} = \\nu_{it}\\)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#conclusion",
    "href": "lectures/lecture5/lecture5.html#conclusion",
    "title": "Empirical Economics",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe FD transformation has converted the highly serially correlated random walk error (\\(\\epsilon_{it}\\)) into a non-serially correlated error (\\(\\nu_{it}\\)).\nBecause OLS on the transformed data is most efficient when the errors are not serially correlated, the FD estimator will be more efficient than the FE estimator under the specific condition of random walk errors.\n\nThis makes FD a powerful alternative, especially for panels with a long time dimension (large T) where such error dynamics are more plausible.\n\nComparison with FE:\n\nIf \\(T=2\\), FD and FE give identical results.\nFor \\(T&gt;2\\), they are different. FE is more efficient if the original errors (\\(\\epsilon_{it}\\)) are not serially correlated.\nFD can be better if the errors follow a random walk. FD is a popular choice for robustness checks."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#practical-considerations-summary",
    "href": "lectures/lecture5/lecture5.html#practical-considerations-summary",
    "title": "Empirical Economics",
    "section": "Practical Considerations & Summary",
    "text": "Practical Considerations & Summary\n\nIn practice, researchers opt almost always for the FE model.\n\nHowever, if you want to be robust, you can follow this workflow:\n\n\n\n\n\n\n\n\n\nWorkflow for Panel Models\n\n\n\nStart by considering your research question. Are you interested in time-invariant variables? If yes, FE is not an option for those variables.\nRun both FE and RE models.\nPerform the Hausman test.\nIf Hausman test rejects H0 (p &lt; 0.05): Use FE. The correlation between unobserved effects and your regressors is a significant problem that FE solves.\nIf Hausman test fails to reject H0 (p &gt;= 0.05): You can justify using the more efficient Random Effects model.\nConsider Pooled OLS only if you have a strong theoretical reason to believe there is no unobserved heterogeneity (very rare).\nConsider the First Differences model as a robustness check."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#what-is-an-event-study",
    "href": "lectures/lecture5/lecture5.html#what-is-an-event-study",
    "title": "Empirical Economics",
    "section": "What is an Event Study?",
    "text": "What is an Event Study?\n\nA special case of the panel data model is an event study\nTo measure the economic impact of a specific, identifiable event on an outcome of interest.\n\n\n\n\n\n\n\n\nExample: Event Studies\n\n\n\nThe “Event” could be:\n\nA company-specific event: Merger announcement, CEO change, earnings surprise.\nA policy change: A new tax law, a change in minimum wage.\nA natural event: A major hurricane, a pandemic.\n\nThe “Outcome” is often:\n\nA firm’s stock price (most common in finance).\nA firm’s accounting performance (sales, profits).\nA macroeconomic variable (unemployment, inflation).\n\n\n\n\n\n\n\nThe central challenge is to determine what the outcome would have been if the event had not occurred."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#panel-event-study-model-with-controls",
    "href": "lectures/lecture5/lecture5.html#panel-event-study-model-with-controls",
    "title": "Empirical Economics",
    "section": "Panel Event Study Model With Controls",
    "text": "Panel Event Study Model With Controls\n\nThe canonical event study model uses firm and time fixed effects and replaces the single interaction term with a series of dummies for time relative to the event.\n\n\n\n\n\n\n\n\nDefinition: Panel Event Study Model (with Control Subjects)\n\n\n\\[ y_{it} = \\alpha_i + \\lambda_t + \\sum_{k=-K}^{L} \\delta_k D_{it}^k + \\epsilon_{it} \\]\n\n\\(\\alpha_i\\): Firm Fixed Effects. These absorb all time-invariant differences between firms.\n\\(\\lambda_t\\): Time Fixed Effects. These absorb all shocks or trends common to all firms in a given year t.\n\\(D_{it}^k\\): A dummy variable equal to 1 if firm i in year t is k periods away from its event date. k is the relative time or event time.\n\\(\\delta_k\\) are the key coefficients. They measure the average change in the outcome for treated firms k periods away from the event, relative to the control group."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#example-dataset",
    "href": "lectures/lecture5/lecture5.html#example-dataset",
    "title": "Empirical Economics",
    "section": "Example Dataset",
    "text": "Example Dataset\n\nA dataset for an event study would look as follows:\n\nNote: For simplicity, only dummy columns for \\(k = -2, -1, 0, 1\\) are included - a full model would include dummies for all relevant pre/post periods (e.g., \\(k \\leq 3\\) and \\(k \\geq 2\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirm ID (i)\nYear (t)\nEvent Year (E_i)\nRelative Time (k = t - E_i)\nOutcome (y_it)\nDummy k=-2\nDummy k=-1\nDummy k=0\nDummy k=1\n\n\n\n\n1 (Control)\n2021\nNA\nNA\n2.0\n0\n0\n0\n0\n\n\n1 (Control)\n2022\nNA\nNA\n2.1\n0\n0\n0\n0\n\n\n1 (Control)\n2023\nNA\nNA\n2.2\n0\n0\n0\n0\n\n\n1 (Control)\n2024\nNA\nNA\n2.3\n0\n0\n0\n0\n\n\n1 (Control)\n2025\nNA\nNA\n2.4\n0\n0\n0\n0\n\n\n2 (Treated)\n2021\n2023\n-2\n2.5\n1\n0\n0\n0\n\n\n2 (Treated)\n2022\n2023\n-1\n2.6\n0\n1\n0\n0\n\n\n2 (Treated)\n2023\n2023\n0\n5.5\n0\n0\n1\n0\n\n\n2 (Treated)\n2024\n2023\n1\n5.8\n0\n0\n0\n1\n\n\n2 (Treated)\n2025\n2023\n2\n6.0\n0\n0\n0\n0\n\n\n3 (Treated)\n2021\n2024\n-3\n3.1\n0\n0\n0\n0\n\n\n3 (Treated)\n2022\n2024\n-2\n3.3\n1\n0\n0\n0\n\n\n3 (Treated)\n2023\n2024\n-1\n3.4\n0\n1\n0\n0\n\n\n3 (Treated)\n2024\n2024\n0\n7.1\n0\n0\n1\n0\n\n\n3 (Treated)\n2025\n2024\n1\n7.5\n0\n0\n0\n1\n\n\n4 (Treated)\n2021\n2024\n-3\n2.9\n0\n0\n0\n0\n\n\n4 (Treated)\n2022\n2024\n-2\n3.0\n1\n0\n0\n0\n\n\n4 (Treated)\n2023\n2024\n-1\n3.2\n0\n1\n0\n0\n\n\n4 (Treated)\n2024\n2024\n0\n6.8\n0\n0\n1\n0\n\n\n4 (Treated)\n2025\n2024\n1\n7.2\n0\n0\n0\n1"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#interpretation-of-the-results",
    "href": "lectures/lecture5/lecture5.html#interpretation-of-the-results",
    "title": "Empirical Economics",
    "section": "Interpretation of the Results",
    "text": "Interpretation of the Results\n\nAfter running the regression, you will get a set of coefficients δ_k which are typically plotted on a graph:\nTesting Pre-Trends (k &lt; 0): The coefficients δ_{-K}, ..., δ_{-2} test the crucial parallel trends assumption. If the model is valid, these coefficients should be close to zero and not statistically significant.\nThe Effect at Impact (k = 0): δ_0 shows the immediate effect of the treatment in the event period itself.\nDynamic Post-Treatment Effects (k &gt; 0): δ_1, δ_2, ..., δ_L show how the treatment effect evolves over time after the event. It might grow, shrink, or stay constant."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#event-study-visualization",
    "href": "lectures/lecture5/lecture5.html#event-study-visualization",
    "title": "Empirical Economics",
    "section": "Event Study Visualization",
    "text": "Event Study Visualization"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#what-happens-without-a-control-group",
    "href": "lectures/lecture5/lecture5.html#what-happens-without-a-control-group",
    "title": "Empirical Economics",
    "section": "What Happens Without a Control Group?",
    "text": "What Happens Without a Control Group?\n\nImagine you removed the control firm (Firm 1) from the dataset.\n\nNow, in the year 2023, the only firms you have are Firm 2 (which just got treated, k=0) and Firms 3 & 4 (which are pre-treatment, k=-1).\n\nThe regression would see that Firm 2’s outcome went up in 2023. But it has no way to know:\n\nWas it because of the treatment (\\(\\delta_0\\))?\nOr was 2023 just a great year for everyone (\\(\\lambda_{2023}\\))?\n\nWithout the control group, the effect of being in the year 2023 (\\(\\lambda_{2023}\\)) is perfectly collinear with the effect of being treated in 2023 (\\(\\delta_0\\)).\nThe model cannot distinguish between them, and the regression will fail or produce meaningless results."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#event-study-without-control-subjects",
    "href": "lectures/lecture5/lecture5.html#event-study-without-control-subjects",
    "title": "Empirical Economics",
    "section": "Event Study Without Control Subjects",
    "text": "Event Study Without Control Subjects\n\nThere are, however, also event studies that do not need control firms.\nThese tend to make use of an estimation window: a “clean” period before the event.\nThis is used to establish a baseline or “normal” behavior for the outcome variable. Typically, ~120 to ~250 days before the event window.\nThe period immediately surrounding (or after) the event date where we expect to see an impact. For example, from 5 days before to 5 days after the announcement ([-5, +5])."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#specification",
    "href": "lectures/lecture5/lecture5.html#specification",
    "title": "Empirical Economics",
    "section": "Specification",
    "text": "Specification\n\n\n\n\n\n\n\nDefinition: Event Study (Without Control Group)\n\n\n\nThe model you can estimate with only treated units is:\n\\[\n  y_{it} = \\alpha_i + \\sum_{k=-K}^{L} \\delta_k D_{it}^k + \\epsilon_{it}\n\\]\n\\(y_{it}\\): The outcome for firm i at time t.\n\\(\\alpha_i\\): Firm Fixed Effects. These absorb all stable, time-invariant differences between the treated firms.\n\\(D_{it}^k\\): A dummy variable = 1 if firm i in year t is k periods away from its event date.\n\\(\\delta_k\\) are the key coefficients. They measure the average outcome for a firm k periods from its event, relative to the outcome in the omitted baseline period (usually k=-1).\nThere are no \\(\\lambda_t\\) (time fixed effects). You cannot include them.\n\nIf you did, the model could not be estimated because your event-time dummies (\\(D^k\\)) would be perfectly predicted by the combination of firm and time fixed effects."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#example-required-dataset-no-control-group",
    "href": "lectures/lecture5/lecture5.html#example-required-dataset-no-control-group",
    "title": "Empirical Economics",
    "section": "Example Required Dataset (No Control Group)",
    "text": "Example Required Dataset (No Control Group)\n\nLet’s use the same staggered adoption scenario as before, but we’ll remove the control firm (Firm 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirm ID (i)\nYear (t)\nEvent Year (E_i)\nRelative Time (k = t - E_i)\nOutcome (y_it)\nDummy k=-2\nDummy k=-1 (Baseline)\nDummy k=0\nDummy k=1\n\n\n\n\n2 (Treated)\n2021\n2023\n-2\n2.5\n1\n0\n0\n0\n\n\n2 (Treated)\n2022\n2023\n-1\n2.6\n0\n1\n0\n0\n\n\n2 (Treated)\n2023\n2023\n0\n5.5\n0\n0\n1\n0\n\n\n2 (Treated)\n2024\n2023\n1\n5.8\n0\n0\n0\n1\n\n\n2 (Treated)\n2025\n2023\n2\n6.0\n0\n0\n0\n0\n\n\n3 (Treated)\n2021\n2024\n-3\n3.1\n0\n0\n0\n0\n\n\n3 (Treated)\n2022\n2024\n-2\n3.3\n1\n0\n0\n0\n\n\n3 (Treated)\n2023\n2024\n-1\n3.4\n0\n1\n0\n0\n\n\n3 (Treated)\n2024\n2024\n0\n7.1\n0\n0\n1\n0\n\n\n3 (Treated)\n2025\n2024\n1\n7.5\n0\n0\n0\n1\n\n\n4 (Treated)\n2021\n2024\n-3\n2.9\n0\n0\n0\n0\n\n\n4 (Treated)\n2022\n2024\n-2\n3.0\n1\n0\n0\n0\n\n\n4 (Treated)\n2023\n2024\n-1\n3.2\n0\n1\n0\n0\n\n\n4 (Treated)\n2024\n2024\n0\n6.8\n0\n0\n1\n0\n\n\n4 (Treated)\n2025\n2024\n1\n7.2\n0\n0\n0\n1"
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#crucial-caveat-the-problem-with-this-approach",
    "href": "lectures/lecture5/lecture5.html#crucial-caveat-the-problem-with-this-approach",
    "title": "Empirical Economics",
    "section": "Crucial Caveat: The Problem with This Approach",
    "text": "Crucial Caveat: The Problem with This Approach\n\nEstimating this model means your results for \\(\\delta_k\\) are highly susceptible to bias from confounding factors.\nThe coefficient \\(\\delta_k\\) now measures the sum of two things:\n\nThe true causal effect of the treatment at relative time k.\nAny and all other unobserved shocks or trends that happened to occur at relative time k.\n\n\n\n\n\n\n\n\n\nExample: Confounding Factor\n\n\nLet’s say a major economic boom started in 2023.\n\nFor Firm 2, its outcome y jumps in 2023. The model will attribute this entire jump to the treatment (\\(\\delta_0\\)) because it has no control group to learn that all firms (even untreated ones) would have seen a jump in 2023.\nYour estimate of the treatment effect will be severely biased upwards."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#the-core-idea-abnormal-returns",
    "href": "lectures/lecture5/lecture5.html#the-core-idea-abnormal-returns",
    "title": "Empirical Economics",
    "section": "The Core Idea: Abnormal Returns",
    "text": "The Core Idea: Abnormal Returns\n\nAn event study with control firms works by isolating the “abnormal” part of an outcome’s movement.\n\n\n\n\n\n\n\n\nDefinition: Abnormal Return\n\n\n\\[ \\text{Abnormal Return}_{it} = \\text{Actual Return}_{it} - \\text{Normal Return}_{it} \\]\n\nActual Return (\\(R_{it}\\)): The observed outcome for firm \\(i\\) on day \\(t\\). This is the raw data.\nNormal Return (\\(E[R_{it}]\\)): The expected return for firm \\(i\\) on day \\(t\\), conditional on the event not happening. This is our counterfactual.\n\n\n\n\n\n\nWe need a model to estimate the Normal Return."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#estimating-normal-returns",
    "href": "lectures/lecture5/lecture5.html#estimating-normal-returns",
    "title": "Empirical Economics",
    "section": "Estimating “Normal” Returns",
    "text": "Estimating “Normal” Returns\n\nThe Normal Return is estimated using data from the estimation window.\n\n\n\n\n\n\n\n\nExamples of Normal Return Models\n\n\nConstant Mean Return Model: Assumes the normal return is just the firm’s average return during the estimation period: \\(E[R_{it}] = \\bar{R}_i\\)\nMarket Model: Assumes the firm’s return is related to the overall market return (e.g., S&P 500).\n\nWe run an OLS regression (one for each firm) using data only from the estimation window: \\(R_{it} = \\alpha_i + \\beta_i R_{mt} + e_{it}\\)\nHere, \\(R_{mt}\\) is the return on the market index. We get estimates for \\(\\hat{\\alpha}_i\\) and \\(\\hat{\\beta}_i\\).\nThe Normal Return for any day \\(t\\) in the event window is then predicted as: \\(\\widehat{E[R_{it}]} = \\hat{\\alpha}_i + \\hat{\\beta}_i R_{mt}\\)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#calculating-abnormal-returns-ar",
    "href": "lectures/lecture5/lecture5.html#calculating-abnormal-returns-ar",
    "title": "Empirical Economics",
    "section": "Calculating Abnormal Returns (AR)",
    "text": "Calculating Abnormal Returns (AR)\n\nOnce we have our estimate of the Normal Return, we can calculate the Abnormal Return (AR) for each firm \\(i\\) on each day \\(t\\) in the event window.\n\n\\[ AR_{it} = R_{it} - (\\hat{\\alpha}_i + \\hat{\\beta}_i R_{mt}) \\]\n\n\\(AR_{it} &gt; 0\\) suggests positive news or impact.\n\\(AR_{it} &lt; 0\\) suggests negative news or impact.\n\\(AR_{it} \\approx 0\\) suggests no impact."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#aggregating-abnormal-returns-car",
    "href": "lectures/lecture5/lecture5.html#aggregating-abnormal-returns-car",
    "title": "Empirical Economics",
    "section": "Aggregating Abnormal Returns (CAR)",
    "text": "Aggregating Abnormal Returns (CAR)\n\nWe are usually interested in the overall effect, not just one firm on one day. We are interested in two things:\n\n\n\n\n\n\n\n\nAggregation of Abnormal Returns\n\n\n1. Average Abnormal Return (AAR): Average the abnormal returns across all \\(N\\) firms for a single day \\(t\\) in the event window. \\[\n      AAR_t = \\frac{1}{N} \\sum_{i=1}^{N} AR_{it}\n    \\]\n\nThis gives us the average effect on a specific day relative to the event (e.g., the effect on day t=+1)."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#aggregating-abnormal-returns-car-cont.",
    "href": "lectures/lecture5/lecture5.html#aggregating-abnormal-returns-car-cont.",
    "title": "Empirical Economics",
    "section": "Aggregating Abnormal Returns (CAR) (Cont.)",
    "text": "Aggregating Abnormal Returns (CAR) (Cont.)\n\n\n\n\n\n\n\nAggregation of Abnormal Returns\n\n\n2. Cumulative Average Abnormal Return (CAAR or CAR): Sum the AARs over a period of time within the event window (from \\(t_1\\) to \\(t_2\\)). \\[\n    CAR(t_1, t_2) = \\sum_{t=t_1}^{t_2} AAR_t\n  \\]\n\nThis tells us the total cumulative impact of the event over a specific window. For example, \\(CAR(-1, +1)\\) measures the total effect from the day before to the day after the event."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#visualizing-event-study-results",
    "href": "lectures/lecture5/lecture5.html#visualizing-event-study-results",
    "title": "Empirical Economics",
    "section": "Visualizing Event Study Results",
    "text": "Visualizing Event Study Results\n\nThe standard way to present results is a plot of the Average Abnormal Return (AAR) over the event window."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#hypothesis-testing",
    "href": "lectures/lecture5/lecture5.html#hypothesis-testing",
    "title": "Empirical Economics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nStandard \\(t\\)-statistics from regression estimates can be used to test hypotheses on AAR’s on particular days.\nWe might also be interested in hypothesis testing of CAR’s.\n\nIs the observed CAR just random noise, or is there a real effect?\nThe null hypothesis is typically \\(H_0: CAR(t_1, t_2) = 0\\).\nStandard errors are calculated based on the variance of the returns in the estimation window.\nIf the t-statistic is large enough (and p-value is small), we conclude the event had a statistically significant impact."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#assumptions-and-caveats-in-event-studies",
    "href": "lectures/lecture5/lecture5.html#assumptions-and-caveats-in-event-studies",
    "title": "Empirical Economics",
    "section": "Assumptions and Caveats in Event Studies",
    "text": "Assumptions and Caveats in Event Studies\n\nEvent studies without a control group have various caveats and assumptions:\n\nEfficient Markets (for stock studies): The model assumes prices react quickly and rationally to new information.\nNo Confounding Events: The event window must be “clean” of other major, contemporaneous events that could also affect the outcome.\nCorrect Event Date: The analysis is sensitive to using the right date of the information release.\nModel Specification: The results can depend on the model chosen for normal returns (e.g., Market Model vs. Fama-French 3-Factor Model).\nStable Estimation Window: The relationship between the firm and the market (the \\(\\beta\\)) must be stable between the estimation and event windows."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#what-did-we-do",
    "href": "lectures/lecture5/lecture5.html#what-did-we-do",
    "title": "Empirical Economics",
    "section": "What did we do?",
    "text": "What did we do?\n\nIntroduced panel data:\n\nPanel data tracks the same individuals (e.g., firms, people) over time. It highlighted its main advantage: the ability to control for unobserved, time-invariant factors (like firm culture or individual ability) that could otherwise cause omitted variable bias.\n\nExplained the two main panel data models:\n\nFixed Effects (FE) and Random Effects (RE). The lecture described how the FE model controls for unobserved factors by using only “within-individual” variation, while the RE model assumes these factors are random and uncorrelated with the explanatory variables.\n\nIntroduced a method for choosing:\n\nThe Hausman test helps determine whether the unobserved individual effects are correlated with the regressors. A significant result suggests using the FE model to avoid bias, while a non-significant result allows for the more efficient RE model."
  },
  {
    "objectID": "lectures/lecture5/lecture5.html#what-did-we-do-cont.",
    "href": "lectures/lecture5/lecture5.html#what-did-we-do-cont.",
    "title": "Empirical Economics",
    "section": "What did we do? (Cont.)",
    "text": "What did we do? (Cont.)\n\nIntroduced an alternative method:\n\nThe First Difference (FD) estimator. The lecture explained that by differencing data over time, the FD model also removes fixed effects and is particularly useful and more efficient than FE when errors are highly serially correlated (e.g., follow a random walk).\n\nIntroduced event studies:\n\nWe detailed the “event study” methodology as a powerful application of panel data. It explained how event studies measure the impact of a specific event (like a policy change or merger) on an outcome by analyzing the periods before and after the event.\n\nContrasted two types of event studies:\n\nWe first showed the modern panel event study which requires a control group and uses firm and time fixed effects to isolate the event’s causal impact.\nWe then explained the classic finance approach used without a control group, which relies on calculating “abnormal returns” against a predicted “normal” outcome."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#course-overview",
    "href": "lectures/lecture3/lecture3.html#course-overview",
    "title": "Empirical Economics",
    "section": "Course Overview",
    "text": "Course Overview\n\nStatistics and Probability - Basic Concepts\nStatistics and Probability - Hypothesis Testing\nThe Linear Regression Model\nTime Series Data\nPanel Data (FE) and Control Variables\nBinary Outcome Data\nPotential Outcomes and Difference-in-differences\nHands-on Econometrics in Practice"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#what-do-we-do-today",
    "href": "lectures/lecture3/lecture3.html#what-do-we-do-today",
    "title": "Empirical Economics",
    "section": "What do we do today?",
    "text": "What do we do today?\n\nFirst two/three lectures devoted to Probability & Statistics\nLecture 1:\n\nHow do we model the processes that might have generated our data?\nProbability\n\nLecture 2:\n\nHow do we summarize and describe data, and try to uncover what process may have generated it?\nStatistics\n\nThis lecture and rest of the course:\n\nHow do we uncover patterns between variables?\nEconometrics"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#what-is-econometrics-1",
    "href": "lectures/lecture3/lecture3.html#what-is-econometrics-1",
    "title": "Empirical Economics",
    "section": "What is Econometrics?",
    "text": "What is Econometrics?\n\nEconometrics is the use of statistical methods to:\n\nEstimate economic relationships.\nTest economic theories.\nEvaluate and implement government and business policy.\nForecast economic variables.\n\nIt’s where economic theory meets real-world data.\nTheory proposes relationships (e.g., Law of Demand), but econometrics tells us the magnitude and statistical significance of these relationships."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#why-study-econometrics",
    "href": "lectures/lecture3/lecture3.html#why-study-econometrics",
    "title": "Empirical Economics",
    "section": "Why study econometrics?",
    "text": "Why study econometrics?\n\nIt allows you to quantify the relationships that you learn about in your other economics courses.\n\nBy how much does demand fall if we raise the price by 10%?\nWhat is the effect of an additional year of education on future wages?\nIt helps distinguish between correlation and causation.\nIt is an essential tool for empirical research in economics and finance, and a highly valued skill in the job market."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-nature-of-economic-data",
    "href": "lectures/lecture3/lecture3.html#the-nature-of-economic-data",
    "title": "Empirical Economics",
    "section": "The Nature of Economic Data",
    "text": "The Nature of Economic Data\n\nThe type of data we have determines the econometric methods we should use.\n\nCross-Sectional Data: A snapshot of many different individuals, households, firms, countries, etc., at a single point in time.\nTime Series Data: Observations on a single entity (e.g., a country, a company) collected over multiple time periods.\nPooled Cross-Sections: A combination of two or more cross-sectional datasets from different time periods. The individuals are different in each period.\nPanel (or Longitudinal) Data: The same cross-sectional units are followed over time."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#examples-of-economic-data",
    "href": "lectures/lecture3/lecture3.html#examples-of-economic-data",
    "title": "Empirical Economics",
    "section": "Examples of Economic Data",
    "text": "Examples of Economic Data\n\n\n\n\n\n\n\nExamples of Economic Data\n\n\nCross-sectional data: A survey of 500 individuals in 2023, with data on their wage, education, gender, and age.\nTime-series data: Data on Dutch GDP, inflation, and unemployment from 1950 to 2023.\nPooled cross-sections: A random survey of households in 1990, and another different random survey of households in 2020.\nPanel data: Tracking the wage, education, and city of residence for the same 500 individuals every year from 2010 to 2020."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-population-regression-function-prf",
    "href": "lectures/lecture3/lecture3.html#the-population-regression-function-prf",
    "title": "Empirical Economics",
    "section": "The Population Regression Function (PRF)",
    "text": "The Population Regression Function (PRF)\n\nIn econometrics, we are interested in relationships between variables.\n\nLet’s say we are interested in the relationship between wages (\\(y\\)) and years of education (\\(x\\)). Economic theory suggests a positive relationship.\n\nWe can model the average wage for a given level of education. This is the Population Regression Function (PRF):\n\n\\[\nE(y | x) = \\beta_0 + \\beta_1 x.\n\\]\n\nWhere:\n\n\\(E(y | x)\\) is the expected value (average) of y, given a value of x.\n\\(\\beta_0\\) is the population intercept.\n\\(\\beta_1\\) is the population slope. These are unknown constants (parameters) that we want to estimate.\n\nThe PRF represents the true, but unknown, relationship in the population."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-visualization-of-prf",
    "href": "lectures/lecture3/lecture3.html#example-visualization-of-prf",
    "title": "Empirical Economics",
    "section": "Example: Visualization of PRF",
    "text": "Example: Visualization of PRF"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-stochastic-error-term",
    "href": "lectures/lecture3/lecture3.html#the-stochastic-error-term",
    "title": "Empirical Economics",
    "section": "The Stochastic Error Term",
    "text": "The Stochastic Error Term\n\nOf course, not everyone with the same level of education has the same wage. Other factors matter (experience, innate ability, location, luck, etc.).\nWe capture all these other unobserved factors in a stochastic error term, \\(u\\).\nOur individual-level population model is:\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n\\]\n\nWhere:\n\n\\(y_i\\) is the wage of individual \\(i\\).\n\\(x_i\\) is the education of individual \\(i\\).\n\\(u_i\\) is the error term for individual \\(i\\). It represents the deviation of individual \\(i\\)’s actual wage from the population average, \\(E(y|x_i)\\).\n\nBy definition of the conditional expectation, \\(E(u|x) = 0\\). The average of the unobserved factors does not depend on the level of education."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#from-population-to-sample",
    "href": "lectures/lecture3/lecture3.html#from-population-to-sample",
    "title": "Empirical Economics",
    "section": "From Population to Sample",
    "text": "From Population to Sample\n\nWe can’t observe the entire population. We only have a sample of data.\nOur goal is to use the sample data to estimate the unknown population parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nThe Sample Regression Function (SRF) is our estimate of the PRF:\n\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\\]\n\nWhere:\n\n\\(\\hat{y}\\) (y-hat) is the predicted or fitted value of y.\n\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the estimators of \\(\\beta_0\\) and \\(\\beta_1\\). They are statistics calculated from our sample data."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-regression-in-a-sample",
    "href": "lectures/lecture3/lecture3.html#example-regression-in-a-sample",
    "title": "Empirical Economics",
    "section": "Example Regression in a Sample",
    "text": "Example Regression in a Sample\n\n\n\n\n\n\n\nExample: Sample Data and Regression"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#residuals",
    "href": "lectures/lecture3/lecture3.html#residuals",
    "title": "Empirical Economics",
    "section": "Residuals",
    "text": "Residuals\n\nHow do we choose the “best” values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)? We want a line that fits the data as closely as possible.\n\n\n\n\n\n\n\n\nDefinition: Residual\n\n\nWe define the residual, \\(e_i\\), as the difference between the actual value \\(y_i\\) and the fitted value \\(\\hat{y}_i\\): \\[\ne_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)\n\\]"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#ols-method",
    "href": "lectures/lecture3/lecture3.html#ols-method",
    "title": "Empirical Economics",
    "section": "OLS Method",
    "text": "OLS Method\n\nThe Ordinary Least Squares (OLS) method chooses \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the Sum of Squared Residuals (SSR):\n\n\n\n\n\n\n\n\nDefinition: OLS Optimzation Problem\n\n\n\\[\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} SSR = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n\\]\n\n\n\n\n\nWe square the residuals so that positive and negative errors don’t cancel out, and because it penalizes larger errors more heavily."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-residuals",
    "href": "lectures/lecture3/lecture3.html#example-residuals",
    "title": "Empirical Economics",
    "section": "Example: Residuals",
    "text": "Example: Residuals"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#derivation-of-ols",
    "href": "lectures/lecture3/lecture3.html#derivation-of-ols",
    "title": "Empirical Economics",
    "section": "Derivation of OLS",
    "text": "Derivation of OLS\n\nTo minimize the SSR, we use calculus: take the partial derivatives with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) and set them to zero.\nThese are the First Order Conditions (FOCs).\n\n\n\n\n\n\n\n\nOLS First Order Conditions\n\n\n\n\\(\\frac{\\partial SSR}{\\partial \\hat{\\beta}_0} = -2 \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\implies \\sum (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\\)\n\\(\\frac{\\partial SSR}{\\partial \\hat{\\beta}_1} = -2 \\sum_{i=1}^{n} x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\implies \\sum x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\\)"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#ols-solution",
    "href": "lectures/lecture3/lecture3.html#ols-solution",
    "title": "Empirical Economics",
    "section": "OLS Solution",
    "text": "OLS Solution\n\nSolving this system of two equations for the two unknowns (\\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\)) gives the OLS estimator formulas:\n\n\n\n\n\n\n\n\nTheorem: OLS Estimates for \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} = \\frac{\\text{Sample Covariance}(x,y)}{\\text{Sample Variance}(x)}\n\\]\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\]\nwhere \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means of \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#algebraic-properties-of-ols",
    "href": "lectures/lecture3/lecture3.html#algebraic-properties-of-ols",
    "title": "Empirical Economics",
    "section": "Algebraic Properties of OLS",
    "text": "Algebraic Properties of OLS\n\nThe OLS estimators have some important algebraic properties that come directly from the FOCs:\nThe sum of the OLS residuals is zero:\n\nThis implies that the sample average of the residuals, \\(\\bar{e}\\), is also zero.\n\n\n\\[\\sum_{i=1}^{n} e_i = 0\\]"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#algebraic-properties-of-ols-cont.",
    "href": "lectures/lecture3/lecture3.html#algebraic-properties-of-ols-cont.",
    "title": "Empirical Economics",
    "section": "Algebraic Properties of OLS (Cont.)",
    "text": "Algebraic Properties of OLS (Cont.)\n\nThe sample covariance between the regressor (\\(x\\)) and the OLS residuals (\\(e\\)) is zero:\n\nThis means the part of \\(y\\) that we can’t explain with \\(x\\) (the residual) is uncorrelated with \\(x\\) in our sample.\n\n\n\\[\\sum_{i=1}^{n} x_i e_i = 0\\]\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the OLS regression line.\n\nFrom the formula \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\), we can write \\(\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#interpreting-ols-coefficients",
    "href": "lectures/lecture3/lecture3.html#interpreting-ols-coefficients",
    "title": "Empirical Economics",
    "section": "Interpreting OLS Coefficients",
    "text": "Interpreting OLS Coefficients\n\nLet’s run our wage-education regression: \\(\\text{Wage}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{Educ}_i\\)\n\n\nRPythonStata\n\n\n\n\nCode\nslr_model &lt;- lm(wage ~ educ, data = dat)\n# The coefficients are:\nsummary(slr_model)\n## \n## Call:\n## lm(formula = wage ~ educ, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.7201 -2.3878 -0.3926  1.9554 11.6092 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.7863     2.5164   0.710    0.479    \n## educ          1.1498     0.1887   6.092 2.19e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.4 on 98 degrees of freedom\n## Multiple R-squared:  0.2747, Adjusted R-squared:  0.2673 \n## F-statistic: 37.11 on 1 and 98 DF,  p-value: 2.192e-08\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\nX = r.dat.educ\ny = r.dat.wage\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n##                             OLS Regression Results                            \n## ==============================================================================\n## Dep. Variable:                   wage   R-squared:                       0.275\n## Model:                            OLS   Adj. R-squared:                  0.267\n## Method:                 Least Squares   F-statistic:                     37.11\n## Date:                Sun, 03 Aug 2025   Prob (F-statistic):           2.19e-08\n## Time:                        13:18:05   Log-Likelihood:                -263.27\n## No. Observations:                 100   AIC:                             530.5\n## Df Residuals:                      98   BIC:                             535.8\n## Df Model:                           1                                         \n## Covariance Type:            nonrobust                                         \n## ==============================================================================\n##                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n## ------------------------------------------------------------------------------\n## const          1.7863      2.516      0.710      0.479      -3.207       6.780\n## educ           1.1498      0.189      6.092      0.000       0.775       1.524\n## ==============================================================================\n## Omnibus:                        8.366   Durbin-Watson:                   2.235\n## Prob(Omnibus):                  0.015   Jarque-Bera (JB):                8.006\n## Skew:                           0.626   Prob(JB):                       0.0183\n## Kurtosis:                       3.595   Cond. No.                         99.2\n## ==============================================================================\n## \n## Notes:\n## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\nSo our estimated SRF is: \\(\\widehat{wage} = 1.79 + 1.15 \\times educ\\)"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#interpretation",
    "href": "lectures/lecture3/lecture3.html#interpretation",
    "title": "Empirical Economics",
    "section": "Interpretation",
    "text": "Interpretation\n\nSlope (\\(\\hat{\\beta}_1 \\approx 1.15\\)): “For each additional year of education, we estimate the hourly wage to increase by $1.15, on average.” This is the key policy parameter.\nIntercept (\\(\\hat{\\beta}_0 \\approx 1.79\\)): “For an individual with zero years of education, we predict an hourly wage of $1.79.”"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#units-and-functional-form",
    "href": "lectures/lecture3/lecture3.html#units-and-functional-form",
    "title": "Empirical Economics",
    "section": "Units and Functional Form",
    "text": "Units and Functional Form\n\nThe values of the coefficients depend on the units of measurement of \\(y\\) and \\(x\\). We’ve used a level-level model (\\(y\\) and \\(x\\) are in their natural units).\nSuppose we measured wage in cents instead of dollars.\n\nThe new dependent variable is \\(wage_{cents} = 100 \\times wage\\).\nThe new regression would be: \\(\\widehat{wage_{cents}} = (100 \\times \\hat{\\beta}_0) + (100 \\times \\hat{\\beta}_1) \\times educ\\)\nBoth the intercept and slope would be 100 times larger. The interpretation is the same, just the units change (“an extra year of education increases wage by 125 cents”)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#units-and-functional-form-cont.",
    "href": "lectures/lecture3/lecture3.html#units-and-functional-form-cont.",
    "title": "Empirical Economics",
    "section": "Units and Functional Form (Cont.)",
    "text": "Units and Functional Form (Cont.)\n\nWhat if we measured education in months instead of years?\n\nThe interpretation of \\(\\hat{\\beta}_1\\) would become “the estimated change in wage for an additional month of education.” The coefficient value would be \\(\\frac{1}{12}\\) of its original value:\n\n\n\n\n\n\n\n\n\nExample: Education in Months\n\n\nFrom our definition, \\(educ_{years} = \\frac{1}{12} educ_{months}\\). Let’s substitute this into the original estimated equation:\n\\[\\begin{align*}\n\\widehat{wage} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 educ_{years} \\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\left( \\frac{1}{12} educ_{months} \\right) \\\\\n&= \\hat{\\beta}_0 + \\left( \\frac{\\hat{\\beta}_1}{12} \\right) educ_{months}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#why-use-different-functional-forms",
    "href": "lectures/lecture3/lecture3.html#why-use-different-functional-forms",
    "title": "Empirical Economics",
    "section": "Why use different functional forms?",
    "text": "Why use different functional forms?\n\nSo far, we’ve assumed a linear relationship: a one-unit change in \\(x\\) leads to the same change in \\(y\\), regardless of the starting value of \\(x\\).\n\nBut often, relationships are not linear. We use transformations (like logarithms) to:\nModel Non-Linear Relationships: Capture effects that are proportional or diminishing.\nChange the Interpretation: Analyze percentage changes (elasticities) instead of unit changes.\nImprove Statistical Properties: Stabilize the variance of the error term or make the distribution of a variable more symmetric.\n\nThe most common transformations involve the natural logarithm, \\(\\log()\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-log-level-model-logy-on-x",
    "href": "lectures/lecture3/lecture3.html#the-log-level-model-logy-on-x",
    "title": "Empirical Economics",
    "section": "The Log-Level Model: \\(\\log(y)\\) on \\(x\\)",
    "text": "The Log-Level Model: \\(\\log(y)\\) on \\(x\\)\n\nHere, we transform the dependent variable \\(y\\): \\(\\log(y) = \\beta_0 + \\beta_1 x + u\\)\nInterpretation of \\(\\beta_1\\): A one-unit increase in \\(x\\) is associated with a \\((100 \\times \\beta_1)\\%\\) change in \\(y\\).\n\n\n\n\n\n\n\n\nInterpretation of \\(\\beta\\) in the Log-Level Model\n\n\nTo see this, take the derivative of the equation with respect to \\(x\\): \\[ \\frac{d(\\log(y))}{dx} = \\beta_1 \\]\nRecall the calculus rule/approximation: for small changes, \\(\\Delta \\log(y) \\approx \\frac{\\Delta y}{y}\\).\nFor a one-unit change in \\(x\\) (\\(\\Delta x = 1\\)): \\[ \\beta_1 = \\frac{\\Delta \\log(y)}{\\Delta x} \\approx \\frac{\\Delta y / y}{1} \\]"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-log-level",
    "href": "lectures/lecture3/lecture3.html#example-log-level",
    "title": "Empirical Economics",
    "section": "Example Log-Level",
    "text": "Example Log-Level\n\n\n\n\n\n\n\nExample: Log-Level Interpretation\n\n\nIn a log-level model, \\(\\beta_1\\) is the proportional change in \\(y\\).\nWe multiply by 100 to get a percentage.\nIf \\(\\widehat{\\log(wage)} = 1.5 + 0.08 \\times educ\\), an additional year of education is associated with an approximate \\(0.08 \\times 100 = 8\\%\\) increase in wage."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-level-log-model-y-on-logx",
    "href": "lectures/lecture3/lecture3.html#the-level-log-model-y-on-logx",
    "title": "Empirical Economics",
    "section": "The Level-Log Model: \\(y\\) on \\(\\log(x)\\)",
    "text": "The Level-Log Model: \\(y\\) on \\(\\log(x)\\)\n\nHere, we transform the independent variable \\(x\\): \\(y = \\beta_0 + \\beta_1 \\log(x) + u\\)\nInterpretation of \\(\\beta_1\\): A 1% increase in \\(x\\) is associated with a \\((\\beta_1 / 100)\\) unit change in \\(y\\).\n\n\n\n\n\n\n\n\nInterpretation of \\(\\beta\\) in Level-Log Model\n\n\nTo see this, take the derivative of the equation with respect to \\(\\log(x)\\): \\[ \\frac{dy}{d(\\log(x))} = \\beta_1 \\]\nA change in \\(\\log(x)\\) is approximately the proportional change in \\(x\\): \\(\\Delta \\log(x) \\approx \\frac{\\Delta x}{x}\\).\n\nSo, \\(\\Delta y \\approx \\beta_1 \\Delta(\\log(x)) \\approx \\beta_1 \\frac{\\Delta x}{x}\\).\nIf we consider a 1% change in \\(x\\), then \\(\\frac{\\Delta x}{x} = 0.01\\).\nThe resulting change in \\(y\\) is: \\(\\Delta y \\approx \\beta_1 \\times (0.01) = \\frac{\\beta_1}{100}\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-level-log-model",
    "href": "lectures/lecture3/lecture3.html#example-level-log-model",
    "title": "Empirical Economics",
    "section": "Example Level-Log Model",
    "text": "Example Level-Log Model\n\n\n\n\n\n\n\nExample: Level-Log Model\n\n\nSuppose that \\(\\text{price} = 200 + 75 \\times \\log(\\text{sqft})\\) is an estimated regression model for house prices.\nThen, A 1% increase in square footage is associated with a \\(75/100 = \\$0.75\\) increase in price."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-log-log-model-logy-on-logx",
    "href": "lectures/lecture3/lecture3.html#the-log-log-model-logy-on-logx",
    "title": "Empirical Economics",
    "section": "The Log-Log Model: \\(\\log(y)\\) on \\(\\log(x)\\)",
    "text": "The Log-Log Model: \\(\\log(y)\\) on \\(\\log(x)\\)\n\nThis model is very common in economics because \\(\\beta_1\\) is an elasticity: \\(\\log(y) = \\beta_0 + \\beta_1 \\log(x) + u\\)\n\n\n\n\n\n\n\n\nInterpretation of \\(\\beta\\) in the Log-Log Model\n\n\nA 1% increase in \\(x\\) is associated with a \\(\\beta_1\\%\\) change in \\(y\\). To see this, from the model, we can write: \\[ \\beta_1 = \\frac{d(\\log(y))}{d(\\log(x))} \\]\nUsing the same approximations as before: \\[ \\beta_1 \\approx \\frac{\\Delta y / y}{\\Delta x / x} = \\frac{\\%\\Delta y}{\\%\\Delta x} \\]\nThis is the definition of elasticity. If we set the percentage change in \\(x\\) to 1% (\\(\\%\\Delta x=1\\)), then the percentage change in \\(y\\) is just \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-elasticity",
    "href": "lectures/lecture3/lecture3.html#example-elasticity",
    "title": "Empirical Economics",
    "section": "Example: Elasticity",
    "text": "Example: Elasticity\n\n\n\n\n\n\n\nExample: Interpretation of \\(\\beta_1\\) in the Log-Log Model\n\n\nSuppose we have estimated \\(\\log(\\text{sales}) = 4.8 - 1.2 \\times \\log(\\text{price})\\) for a product. Then, a 1% increase in price is associated with a 1.2% decrease in sales. The price elasticity of demand is -1.2."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#other-forms-polynomials",
    "href": "lectures/lecture3/lecture3.html#other-forms-polynomials",
    "title": "Empirical Economics",
    "section": "Other Forms: Polynomials",
    "text": "Other Forms: Polynomials\n\nWe can also add polynomial terms (like \\(x^2\\), \\(x^3\\), etc.) to capture more complex non-linear patterns, such as diminishing returns.\n\nModel (Quadratic): \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + u\\)\n\n\n\n\n\n\n\n\n\nInterpretation of the Quadratic Model\n\n\nThe effect of a change in \\(x\\) on \\(y\\) now depends on the level of \\(x\\).\nThe marginal effect of \\(x\\) on \\(y\\) is the derivative with respect to \\(x\\): \\[ \\frac{\\Delta y}{\\Delta x} \\approx \\frac{dy}{dx} = \\beta_1 + 2 \\beta_2 x \\]\nA one-unit change in \\(x\\) is associated with a change in \\(y\\) of approximately \\(\\beta_1 + 2 \\beta_2 x\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-polynomial",
    "href": "lectures/lecture3/lecture3.html#example-polynomial",
    "title": "Empirical Economics",
    "section": "Example Polynomial",
    "text": "Example Polynomial\n\n\n\n\n\n\n\nExample: Polynomial Regression\n\n\nSuppose we have estimated \\(\\widehat{wage} = 3.50 + 0.60 \\times educ - 0.02 \\times educ^2\\).\nThe effect of the first year of education (\\(x=0 \\to x=1\\)) is about \\(\\$0.60\\).\nThe effect of the 13th year of education (\\(x=12 \\to x=13\\)) is: \\(0.60 + 2(-0.02)(12) = 0.60 - 0.48 = \\$0.12\\).\nThis captures the diminishing returns to education on wage."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#summary-of-interpretations",
    "href": "lectures/lecture3/lecture3.html#summary-of-interpretations",
    "title": "Empirical Economics",
    "section": "Summary of Interpretations",
    "text": "Summary of Interpretations\n\n\n\n\n\n\n\n\n\nModel Name\nEquation\nInterpretation of \\(\\hat{\\beta}_1\\)\n\n\n\n\nLevel-Level\n\\(y = \\beta_0 + \\beta_1 x\\)\nA 1-unit change in \\(x\\) leads to a \\(\\hat{\\beta}_1\\) unit change in \\(y\\).\n\n\nLog-Level\n\\(\\log(y) = \\beta_0 + \\beta_1 x\\)\nA 1-unit change in \\(x\\) leads to a \\((100 \\times \\hat{\\beta}_1)\\%\\) change in \\(y\\).\n\n\nLevel-Log\n\\(y = \\beta_0 + \\beta_1 \\log(x)\\)\nA 1% change in \\(x\\) leads to a \\((\\hat{\\beta}_1/100)\\) unit change in \\(y\\).\n\n\nLog-Log\n\\(\\log(y) = \\beta_0 + \\beta_1 \\log(x)\\)\nA 1% change in \\(x\\) leads to a \\(\\hat{\\beta}_1\\%\\) change in \\(y\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#goodness-of-fit",
    "href": "lectures/lecture3/lecture3.html#goodness-of-fit",
    "title": "Empirical Economics",
    "section": "Goodness-of-Fit",
    "text": "Goodness-of-Fit\n\nHow well does our estimated line explain the variation in our dependent variable, \\(y\\)?\nWe can partition the total variation in \\(y\\) into two parts: the part explained by the model, and the part that is not explained.\n\n\n\n\n\n\n\n\nPartition of Variation in \\(Y\\)\n\n\nSST (Total Sum of Squares): Total variation in \\(y\\). \\(SST = \\sum (y_i - \\bar{y})^2\\)\nSSE (Explained Sum of Squares): Variation explained by the regression. \\(SSE = \\sum (\\hat{y}_i - \\bar{y})^2\\)\nSSR (Sum of Squared Residuals): Unexplained variation. \\(SSR = \\sum e_i^2\\)\nIt is a mathematical property that SST = SSE + SSR."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#goodness-of-fit-visualization",
    "href": "lectures/lecture3/lecture3.html#goodness-of-fit-visualization",
    "title": "Empirical Economics",
    "section": "Goodness-of-Fit Visualization",
    "text": "Goodness-of-Fit Visualization"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#goodness-of-fit-r2",
    "href": "lectures/lecture3/lecture3.html#goodness-of-fit-r2",
    "title": "Empirical Economics",
    "section": "Goodness-of-Fit: \\(R^2\\)",
    "text": "Goodness-of-Fit: \\(R^2\\)\n\nWe want to encapsulate “goodness-of-fit” into one number.\n\n\n\n\n\n\n\n\nDefinition: \\(R^2\\)\n\n\nThe R-squared measures the proportion of the total sample variation in \\(y\\) that is “explained” by the regression model.\n\\[\nR^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\n\\]\n\n\n\n\n\n\\(R^2\\) is always between 0 and 1.\nA higher \\(R^2\\) means the model fits the data better in-sample.\nCaution: A high \\(R^2\\) is not the ultimate goal of econometrics! We care more about getting an unbiased estimate of the causal effect \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-classical-assumptions",
    "href": "lectures/lecture3/lecture3.html#the-classical-assumptions",
    "title": "Empirical Economics",
    "section": "The Classical Assumptions",
    "text": "The Classical Assumptions\n\nThe objective of a regression is to say something about the population parameters \\(\\beta\\). However, we only have a sample equivalent, \\(\\hat{\\beta}\\) at our disposal.\n\nThis estimate goes paired with some uncertainty.\nFor our OLS estimates to have desirable statistical properties, certain assumptions must hold. These are the Gauss-Markov Assumptions.\n\n\n\n\n\n\n\n\n\nGauss-Markov Assumptions\n\n\n\nAssumption 1: Linearity in Parameters. The population model is \\(y = \\beta_0 + \\beta_1 x + u\\).\nAssumption 2: Random Sampling. The data \\((x_i, y_i)\\) are a random sample from the population described by the model.\nAssumption 3: Sample Variation in \\(x\\). The values of \\(x_i\\) in the sample are not all the same. This is the no perfect collinearity assumption. If all \\(x_i\\) are the same, the denominator of \\(\\hat{\\beta}_1\\) is zero!\nAssumption 4: Zero Conditional Mean. \\(E(u|x) = 0\\). The average value of the unobserved factors is unrelated to the value of \\(x\\).\nAssumption 5: Homoskedasticity. \\(Var(u|x) = \\sigma^2\\). The variance of the error term is constant for all values of \\(x\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-crucial-assumption-zero-conditional-mean",
    "href": "lectures/lecture3/lecture3.html#the-crucial-assumption-zero-conditional-mean",
    "title": "Empirical Economics",
    "section": "The Crucial Assumption: Zero Conditional Mean",
    "text": "The Crucial Assumption: Zero Conditional Mean\n\nAssumption 4, \\(E(u|x) = 0\\), is the most important assumption for establishing causality.\n\nIt means that the explanatory variable (\\(x\\)) must not be correlated with any of the unobserved factors (\\(u\\)) that affect the dependent variable (\\(y\\)).\n\n\n\n\n\n\n\n\n\nExample: Zero Conditional Mean Assumption\n\n\nLet \\(y = wage\\), \\(x = educ\\), \\(u\\) = unobserved factors like innate ability, family background, motivation.\nIs it likely that \\(E(u|educ)=0\\)? Probably not. Innate ability (\\(u\\)) is likely correlated with education (\\(x\\)). People with higher ability may find it easier to get more education.\nIf \\(Cov(educ, ability) &gt; 0\\), then our OLS estimate \\(\\hat{\\beta}_1\\) will be biased upwards. It will capture the effect of education and the effect of ability. This is Omitted Variable Bias."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#unbiasedness-of-ols",
    "href": "lectures/lecture3/lecture3.html#unbiasedness-of-ols",
    "title": "Empirical Economics",
    "section": "Unbiasedness of OLS",
    "text": "Unbiasedness of OLS\n\n\n\n\n\n\n\nTheorem: Unbiasedness of OLS\n\n\nUnder assumptions SLR.1 through SLR.4, the OLS estimators are unbiased.\n\\[\nE(\\hat{\\beta}_0) = \\beta_0 \\quad \\text{and} \\quad E(\\hat{\\beta}_1) = \\beta_1\n\\]\n\n\n\n\n\nWhat does this mean?\n\nUnbiasedness is a property of the procedure of OLS estimation.\n\nIf we could draw many, many random samples from the population and calculate \\(\\hat{\\beta}_1\\) for each sample, the average of all these estimates would be equal to the true population parameter, \\(\\beta_1\\).\n\nOur estimate from any single sample might be higher or lower than the true value, but on average, we get it right.\n\nThis property relies critically on the Zero Conditional Mean assumption (SLR.4). If SLR.4 fails, OLS is biased."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#variance-of-ols-estimators",
    "href": "lectures/lecture3/lecture3.html#variance-of-ols-estimators",
    "title": "Empirical Economics",
    "section": "Variance of OLS Estimators",
    "text": "Variance of OLS Estimators\n\nWe also want our estimators to be precise, meaning they don’t vary too much from sample to sample. This is measured by their sampling variance.\n\n\n\n\n\n\n\n\nTheorem: Variance of the OLS Estimator\n\n\nUnder assumptions SLR.1 through SLR.5 (all five Gauss-Markov assumptions), the variance of the OLS slope estimator is:\n\\[\nVar(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sigma^2}{SST_x}\n\\]\nConsult the appendix for a derivation."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#variance-of-ols-estimators-cont.",
    "href": "lectures/lecture3/lecture3.html#variance-of-ols-estimators-cont.",
    "title": "Empirical Economics",
    "section": "Variance of OLS Estimators (Cont.)",
    "text": "Variance of OLS Estimators (Cont.)\n\nLet’s forget regression for a second.\n\nHow much does a simple sample mean (\\(\\bar{x}\\)) wobble?\nIts variance is famously simple: \\(Var(\\bar{x}) = \\frac{\\sigma^2}{n}\\)\n\nThis formula is driven by two intuitive ideas:\n\n\\(\\sigma^2\\) is the variance of the underlying population. It’s the inherent “noisiness” or “spread” of the data points themselves.\n\\(n\\) is your sample size. It’s how much data you have to “anchor” your estimate.\n\nLooking at \\(Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sigma^2}{SST_x}\\) tells us the exact same.."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#determinants-of-the-variance",
    "href": "lectures/lecture3/lecture3.html#determinants-of-the-variance",
    "title": "Empirical Economics",
    "section": "Determinants of the Variance",
    "text": "Determinants of the Variance\n\nWhat determines the precision of our estimate?\n\nThe error variance, \\(\\sigma^2\\): More “noise” in the relationship (larger \\(\\sigma^2\\)) leads to a larger variance for \\(\\hat{\\beta}_1\\).\nThe total sample variation in x, \\(SST_x\\): More variation in our explanatory variable (\\(x\\)) leads to a smaller variance for \\(\\hat{\\beta}_1\\). We learn more about the slope when our \\(x\\) values are more spread out.\nThe sample size, n: A larger sample size generally increases \\(SST_x\\), which decreases the variance of \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#determining-the-variance-in-practice",
    "href": "lectures/lecture3/lecture3.html#determining-the-variance-in-practice",
    "title": "Empirical Economics",
    "section": "Determining the Variance in Practice",
    "text": "Determining the Variance in Practice\n\nThere’s a catch. We need \\(\\sigma^2\\) (the true variance of the errors) for our formula.\n\nBut we can never know the true errors, because we don’t know the true line!\nAll we have are the residuals (\\(e_i\\)) from our estimated line: \\(e_i = y_i - \\hat{y}_i\\)\n\nSolution: We use the residuals to estimate the error variance. We call this estimate \\(s^2\\) or \\(\\hat{\\sigma^2}\\), and its square root is called the standard error of a regression coefficient."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-standard-error",
    "href": "lectures/lecture3/lecture3.html#the-standard-error",
    "title": "Empirical Economics",
    "section": "The Standard Error",
    "text": "The Standard Error\n\nThe SER is an estimator of the standard deviation of the population error term, \\(\\sigma\\). It measures the typical size of a residual (the model’s “average mistake”).\n\n\n\n\n\n\n\n\nDefinition: Standard Error of a Regression\n\n\n\\[\n\\hat{\\sigma} = SER = \\sqrt{\\frac{SSR}{n-2}} = \\sqrt{\\frac{\\sum e_i^2}{n-2}}\n\\]\n\n\n\n\n\nWe divide by \\(n-2\\) (degrees of freedom) because we had to estimate two parameters (\\(\\beta_0, \\beta_1\\)) to get the residuals.\nSER is measured in the same units as \\(y\\). A smaller SER is better."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#from-standard-error-to-inference",
    "href": "lectures/lecture3/lecture3.html#from-standard-error-to-inference",
    "title": "Empirical Economics",
    "section": "From Standard Error to Inference",
    "text": "From Standard Error to Inference\n\nNow we have all the pieces:\n\nStart with the true (but unusable) variance formula \\(Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\)\nPlug in our estimate \\(s^2\\) for the unknown \\(\\sigma^2\\)\nThis gives us the estimated variance: \\(s^2 / \\sum (X_i - \\bar{X})^2\\)\nTake the square root to get it back to the original units of β₁:\nThis is the Standard Error of the Coefficient.\n\n\n\n\n\n\n\n\n\nDefinition: Standard Error of a Coefficient\n\n\n\\[\nSE(\\hat{\\beta}) = \\hat{\\sigma} / \\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#hypotheses-testing",
    "href": "lectures/lecture3/lecture3.html#hypotheses-testing",
    "title": "Empirical Economics",
    "section": "Hypotheses Testing",
    "text": "Hypotheses Testing\n\nTo test a hypothesis about a single coefficient (e.g., \\(H_0: \\beta = 0\\)), we want to see how many standard deviations our estimate \\(\\hat{\\beta}\\) is from the hypothesized value. \\[\n  \\text{Test Stat} = (\\text{Our Estimate of } \\hat{\\beta} - \\text{Hypothesized Value}) / \\text{Standard Error}(\\hat{\\beta})\n\\]\nIf we knew the true \\(\\sigma\\), this statistic would follow a perfect Normal distribution.\nWe don’t know the population error variance, \\(\\sigma^2\\).\nSolution: We replace \\(\\sigma^2\\) with its sample estimate, \\(\\hat{\\sigma}^2 = \\frac{SSR}{n-k-1}\\).\n\nBecause we had to estimate \\(\\sigma^2\\), we introduce extra sampling variability into our statistic.\nThis means that our test statistic will be \\(t\\)-distributed instead of normally distributed."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#why-a-t-statistic",
    "href": "lectures/lecture3/lecture3.html#why-a-t-statistic",
    "title": "Empirical Economics",
    "section": "Why a t-statistic?",
    "text": "Why a t-statistic?\n\nThe ratio of our estimate to its standard error is no longer normally distributed. It follows a t-distribution.\n\n\n\n\n\n\n\n\nDefinition: Distribution of t-value under \\(H_0\\)\n\n\n\\[\nt = \\frac{\\hat{\\beta} - \\beta}{se(\\hat{\\beta}_j)} \\sim t_{n-2}\n\\]\n\n\n\n\n\nThe t-distribution looks very similar to the normal distribution but has “fatter tails,” reflecting the added uncertainty from estimating \\(\\sigma^2\\).\n\nIt is characterized by its degrees of freedom (df), which for Simple Linear Regression is \\(df = n - 2\\).\nAs the sample size (\\(n\\)) gets large, the t-distribution converges to the standard normal distribution."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#visualization",
    "href": "lectures/lecture3/lecture3.html#visualization",
    "title": "Empirical Economics",
    "section": "Visualization",
    "text": "Visualization\n\n\n\n\n\n\n\nExample: \\(t\\)-distribution vs. Normal Distribution"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#example-a-t-test-in-simple-linear-regression",
    "href": "lectures/lecture3/lecture3.html#example-a-t-test-in-simple-linear-regression",
    "title": "Empirical Economics",
    "section": "Example: A \\(t\\)-test in Simple Linear Regression",
    "text": "Example: A \\(t\\)-test in Simple Linear Regression\n\n\n\n\n\n\n\nExample: \\(t\\)-test in Linear Regression\n\n\nConsider the following regression output:\n\n\nCode\nsummary(slr_model)\n## \n## Call:\n## lm(formula = wage ~ educ, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.7201 -2.3878 -0.3926  1.9554 11.6092 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.7863     2.5164   0.710    0.479    \n## educ          1.1498     0.1887   6.092 2.19e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.4 on 98 degrees of freedom\n## Multiple R-squared:  0.2747, Adjusted R-squared:  0.2673 \n## F-statistic: 37.11 on 1 and 98 DF,  p-value: 2.192e-08\n\n\nFrom this, we can see that the regression standard error (SER), \\(\\hat{\\sigma} = 3.4\\). We can also see that the SE on the educ coefficient is 0.19. We can relate the two by dividing SER by \\(\\sqrt{\\sum (X_i - \\bar{X})^2}\\):\n\n\nCode\nser &lt;- sqrt(sum(slr_model$residuals^2/98))\ncat(\"Standard error regression:\", ser)\n## Standard error regression: 3.400451\n\nse_beta_educ &lt;- ser / sqrt(sum((dat$educ - mean(dat$educ))^2))\ncat(\"Standard error Beta_educ:\", se_beta_educ)\n## Standard error Beta_educ: 0.1887422\n\n\nWe can therefore manually calculate the \\(t\\)-statistic testing \\(H_0: \\beta=0\\) as:\n\n\nCode\nt_stat &lt;- (slr_model$coefficients['educ'] - 0) / se_beta_educ\n\n\nFinally we can even calculate the two-sided \\(p\\)-value of observing a test statistic this extreme under the null hypothesis:\n\n\nCode\np_val &lt;- (1-pt(t_stat, 98)) + pt(-t_stat, 98)\ncat(\"The p-value is:\", p_val)\n## The p-value is: 2.192191e-08"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#introduction-to-multiple-linear-regression",
    "href": "lectures/lecture3/lecture3.html#introduction-to-multiple-linear-regression",
    "title": "Empirical Economics",
    "section": "Introduction to Multiple Linear Regression",
    "text": "Introduction to Multiple Linear Regression\n\nSimple Linear Regression is often inadequate because we can’t control for other factors that might be important. This leads to omitted variable bias.\nThe solution is to include those other factors in the model.\n\n\n\n\n\n\n\n\nDefinition: Multiple Linear Regression Model\n\n\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + u\n\\]\nNow we have \\(k\\) explanatory variables.\n\n\\(\\beta_j\\) is the effect of a one-unit change in \\(x_j\\) on \\(y\\), holding all other explanatory variables (\\(x_1, ..., x_{j-1}, x_{j+1}, ... x_k\\)) constant.\nThis is the concept of ceteris paribus (all else equal). MLR allows us to isolate the effect of one variable while mathematically controlling for the others."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#ols-estimation-in-mlr",
    "href": "lectures/lecture3/lecture3.html#ols-estimation-in-mlr",
    "title": "Empirical Economics",
    "section": "OLS Estimation in MLR",
    "text": "OLS Estimation in MLR\n\nThe principle is the same: we choose \\(\\hat{\\beta}_0, \\hat{\\beta}_1, ..., \\hat{\\beta}_k\\) to minimize the Sum of Squared Residuals (SSR).\nThe formulas are complex (usually done with matrix algebra) but are easily handled by software:\n\nlm(y ~ x1 + x2, data=df) in R\nreg y x1 x2 in Stata\nPython:\n\n\nfrom sklearn.linear_model import LinearRegression\nLinearRegression().fit(X, y)"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#variance-of-ols-estimators-in-mlr",
    "href": "lectures/lecture3/lecture3.html#variance-of-ols-estimators-in-mlr",
    "title": "Empirical Economics",
    "section": "Variance of OLS Estimators in MLR",
    "text": "Variance of OLS Estimators in MLR\n\n\n\n\n\n\n\nDefinition: Variance of the OLS Estimator (Multivariate)\n\n\n\\[\nVar(\\hat{\\beta}_j) = \\frac{\\sigma^2}{SST_j (1 - R_j^2)}\n\\]\n\n\\(SST_j\\) is the total variation in \\(x_j\\): \\(\\sum_{i=1}^N (x_{ij} - \\bar{x}_j)^2\\)\n\\(R_j^2\\) is the R-squared from a regression of \\(x_j\\) on all other explanatory variables in the model.\n\n\n\n\n\n\nThe variance of a coefficient \\(\\hat{\\beta}_j\\) now depends on multicollinearity – how correlated \\(x_j\\) is with the other explanatory variables.\n\nIf \\(x_j\\) is highly correlated with other \\(x\\)’s, \\(R_j^2\\) will be close to 1, making the denominator small and \\(Var(\\hat{\\beta}_j)\\) very large. This is imperfect multicollinearity.\nThe no perfect collinearity assumption for MLR means that no \\(x_j\\) can be a perfect linear combination of the others (i.e., \\(R_j^2 \\neq 1\\))."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#hypothesis-testing-in-multiple-regression",
    "href": "lectures/lecture3/lecture3.html#hypothesis-testing-in-multiple-regression",
    "title": "Empirical Economics",
    "section": "Hypothesis Testing in Multiple Regression",
    "text": "Hypothesis Testing in Multiple Regression\n\nOnce we’ve estimated our model, \\(\\widehat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_k x_k\\), we need to ask: Is the relationship we found statistically significant?\n\nOur estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_j\\) are based on a sample of data. They are subject to sampling variability.\nIt’s possible that the true relationship in the population is zero (\\(\\beta_1 = 0\\)), and we just found a non-zero \\(\\hat{\\beta}_1\\) by random chance.\n\nIn addition to the \\(t\\)-test, introduced with the Simple Linear Regression model, we also have the \\(F\\)-test:\n\nThis tests the joint significance of multiple coefficients or the model as a whole."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-t-test-significance-of-a-single-coefficient",
    "href": "lectures/lecture3/lecture3.html#the-t-test-significance-of-a-single-coefficient",
    "title": "Empirical Economics",
    "section": "The \\(t\\)-Test: Significance of a Single Coefficient",
    "text": "The \\(t\\)-Test: Significance of a Single Coefficient\n\nThe \\(t\\)-test is our tool for testing a hypothesis about a single coefficient.\n\nThe most common hypothesis is that a variable has no effect on the dependent variable.\n\nHypotheses for a single coefficient \\(\\beta_j\\):\n\nNull Hypothesis (\\(H_0\\)): The variable has no effect. \\(H_0: \\beta_j = 0\\)\nAlternative Hypothesis (\\(H_A\\)): The variable does have an effect. \\(H_A: \\beta_j \\neq 0\\)\nWe calculate the t-statistic, which measures how many standard errors our estimated coefficient is away from the hypothesized value (zero).\n\n\n\n\n\n\n\n\n\nThe \\(t\\)-statistic (Multivariate)\n\n\n\\[\nt = \\frac{\\text{Estimate} - \\text{Hypothesized Value}}{\\text{Standard Error}} = \\frac{\\hat{\\beta}_j - 0}{se(\\hat{\\beta}_j)}\n\\]\nwhere the \\(se(\\hat{\\beta}_j)\\) is the square root of \\(Var(\\hat{\\beta}_j)\\) as presented earlier."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#the-f-test-testing-joint-significance",
    "href": "lectures/lecture3/lecture3.html#the-f-test-testing-joint-significance",
    "title": "Empirical Economics",
    "section": "The F-Test: Testing Joint Significance",
    "text": "The F-Test: Testing Joint Significance\n\nThe F-test is used to test hypotheses about multiple coefficients at the same time.\n\nIts most common use is to test the overall significance of the regression model.\n\n\n\n\n\n\n\n\n\nF Test for Overall Significance\n\n\nThis tests whether any of our independent variables have an effect on the dependent variable.\nModel: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u\\)\nNull Hypothesis (\\(H_0\\)): None of the independent variables have an effect on \\(y\\). The model has no explanatory power. \\(H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0\\)\nAlternative Hypothesis (\\(H_A\\)): At least one of the coefficients is not zero. The model has some explanatory power. \\(H_A: \\text{At least one } \\beta_j \\neq 0 \\text{ for } j=1, \\dots, k\\)"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#f-statistic",
    "href": "lectures/lecture3/lecture3.html#f-statistic",
    "title": "Empirical Economics",
    "section": "F-statistic",
    "text": "F-statistic\n\nIn general, the F-test serves to compare a “restricted model”, where some of the \\(\\beta\\) coefficients are zero under a null hypothesis, against an “unrestricted” model where coefficients are allowed to vary.\nA large F-statistic suggests that the unrestricted model explains significantly more variation in \\(y\\) than the restricted model.\nLike the t-test, we typically look at the p-value for the F-statistic. If \\(p &lt; 0.05\\), we reject the null and conclude our model is jointly significant."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#f-statistic-procedure",
    "href": "lectures/lecture3/lecture3.html#f-statistic-procedure",
    "title": "Empirical Economics",
    "section": "F-statistic Procedure",
    "text": "F-statistic Procedure\n\n\n\n\n\n\n\nF Statistic: Definition (General)\n\n\nThe F statistic is defined as:\n\\[\nF = \\frac{(SSR_{\\text{restricted}} - SSR_{\\text{unrestricted}}) / (k_{\\text{unrestricted}} - k_{\\text{restricted}})}{SSR_{\\text{unrestricted}} / (n - k_{\\text{unrestricted}} - 1)}\n\\]\n\nWhere:\n\n\\(SSR_{\\text{restricted}}\\) = Sum of Squared Residuals from the restricted model (fewer predictors)\n\\(SSR_{\\text{unrestricted}}\\) = Sum of Squared Residuals from the unrestricted model (more predictors)\n\\(k_{\\text{restricted}}\\) = Number of parameters in the restricted model (including intercept)\n\\(k_{\\text{unrestricted}}\\) = Number of parameters in the unrestricted model (including intercept)\n\\(n\\) = Number of observations\n\nIn addition, define:\n\n\\(df_1 = k_{\\text{unrestricted}} - k_{\\text{restricted}}\\)\n(difference in the number of parameters)\n\\(df_2 = n - k_{\\text{unrestricted}} - 1\\)\n(sample size minus number of unrestricted parameters minus 1)"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#f-distribution-visualization",
    "href": "lectures/lecture3/lecture3.html#f-distribution-visualization",
    "title": "Empirical Economics",
    "section": "F Distribution: Visualization",
    "text": "F Distribution: Visualization\n\nThe \\(F\\) distribution comes with two parameters, \\(df_1\\) and \\(df_2\\), as defined in the previous slide.\n\n\n\n\n\n\n\n\nExample: F Distribution"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#summary-t-test-vs.-f-test",
    "href": "lectures/lecture3/lecture3.html#summary-t-test-vs.-f-test",
    "title": "Empirical Economics",
    "section": "Summary: \\(t\\)-test vs. \\(F\\)-test",
    "text": "Summary: \\(t\\)-test vs. \\(F\\)-test\n\nIt’s crucial to understand when to use each test.\n\nA group of variables can be jointly significant (F-test) even if no single variable is individually significant (t-tests).\n\n\n\n\n\n\n\n\n\n\n\nFeature\nt-test\nF-test\n\n\n\n\nScope\nOne coefficient at a time\nTwo or more coefficients at a time\n\n\nTypical Use\nIs this specific variable significant?\nIs this group of variables jointly significant? OR Is the model as a whole useful?\n\n\nNull Hypothesis\n\\(H_0: \\beta_j = 0\\)\n\\(H_0: \\beta_1 = \\beta_2 = \\dots = 0\\)\n\n\nTest Statistic\n\\(t = \\frac{\\hat{\\beta}_j}{se(\\hat{\\beta}_j)}\\)\nCompares Restricted vs. Unrestricted sum of squares\n\n\nKey Question\n“Does education significantly affect wage, holding other factors constant?”\n“Does a person’s work experience, measured by exper and exper^2, jointly affect their wage?”"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#understanding-statistical-output-1",
    "href": "lectures/lecture3/lecture3.html#understanding-statistical-output-1",
    "title": "Empirical Economics",
    "section": "Understanding Statistical Output",
    "text": "Understanding Statistical Output\n\nBy now, we can understand virtually all of the standard statistical output from R/Python/Stata.\nLet’s look at the full output from R/Python/Stata for our simple regression.\n\n\nRPythonStata\n\n\n\nmlr_model &lt;- lm(wage ~ educ + exper, data = dat_mlr)\nsummary(mlr_model)\n\n\nCall:\nlm(formula = wage ~ educ + exper, data = dat_mlr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3544 -1.9212  0.1218  2.1522  7.5290 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.82941    2.64978   1.068   0.2883    \neduc         1.02959    0.17534   5.872 6.03e-08 ***\nexper        0.16733    0.06647   2.518   0.0135 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.142 on 97 degrees of freedom\nMultiple R-squared:  0.2825,    Adjusted R-squared:  0.2677 \nF-statistic:  19.1 on 2 and 97 DF,  p-value: 1.016e-07\n\n\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\n# Create a DataFrame for X (automatically keeps variable names)\nX = pd.DataFrame({\n    'educ': r.dat_mlr['educ'],\n    'exper': r.dat_mlr['exper']\n})\n\nX = sm.add_constant(X)  # Adds 'const' column\ny = r.dat_mlr['wage']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.283\nModel:                            OLS   Adj. R-squared:                  0.268\nMethod:                 Least Squares   F-statistic:                     19.10\nDate:                Sun, 03 Aug 2025   Prob (F-statistic):           1.02e-07\nTime:                        13:18:06   Log-Likelihood:                -254.87\nNo. Observations:                 100   AIC:                             515.7\nDf Residuals:                      97   BIC:                             523.6\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.8294      2.650      1.068      0.288      -2.430       8.088\neduc           1.0296      0.175      5.872      0.000       0.682       1.378\nexper          0.1673      0.066      2.518      0.013       0.035       0.299\n==============================================================================\nOmnibus:                        0.187   Durbin-Watson:                   2.158\nProb(Omnibus):                  0.911   Jarque-Bera (JB):                0.359\nSkew:                          -0.059   Prob(JB):                        0.836\nKurtosis:                       2.731   Cond. No.                         175.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nreg wage educ expr"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#understanding-statistical-output-cont.",
    "href": "lectures/lecture3/lecture3.html#understanding-statistical-output-cont.",
    "title": "Empirical Economics",
    "section": "Understanding Statistical Output (Cont.)",
    "text": "Understanding Statistical Output (Cont.)\n\nCoefficients:\n\nEstimate or coef: These are \\(\\hat{\\beta}_0\\) (Intercept), \\(\\hat{\\beta}_1\\) (educ) and \\(\\hat{\\beta}_2\\) (exper).\nStd. Error: The standard errors of the estimates, \\(se(\\hat{\\beta}_j)\\), which measure their sampling uncertainty.\nt value: The t-statistic used for hypothesis testing (Estimate / Std. Error).\nPr(&gt;|t|): The p-value for the t-test.\n\nGoodness-of-Fit:\n\nResidual standard error (R only): This is the SER (3.14).\nR-squared: This is our R-squared (0.27). The model explains about 28% of the variation in wages."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#interpretation-with-controls",
    "href": "lectures/lecture3/lecture3.html#interpretation-with-controls",
    "title": "Empirical Economics",
    "section": "Interpretation with Controls",
    "text": "Interpretation with Controls\n\neduc: \\(\\hat{\\beta}_1 \\approx 1.02\\): Holding experience constant, one more year of education is associated with a $1.02/hr increase in wages, on average.\nexper: \\(\\hat{\\beta}_2 \\approx 0.16\\): Holding education constant, one more year of experience is associated with a $0.16/hr increase in wages, on average.\nThe R-squared increased relative to the simple model, suggesting this model explains more variation in wages.\nThe t-statistics tell us whether these coefficients are statistically distinguishable from zero."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#what-did-we-do",
    "href": "lectures/lecture3/lecture3.html#what-did-we-do",
    "title": "Empirical Economics",
    "section": "What did we do?",
    "text": "What did we do?\n\nThe Linear Model’s Purpose:\n\nEconometrics uses the linear regression model to estimate the relationship between a dependent variable (e.g., wage) and one or more explanatory variables (e.g., education). The goal is to estimate an unknown “population” relationship using a “sample” of data.\n\nThe OLS Method:\n\nThe model’s coefficients (slope and intercept) are estimated using the Ordinary Least Squares (OLS) method. This technique finds the best-fitting line by minimizing the sum of the squared differences (residuals) between the actual data points and the predicted values on the line.\n\nInterpretation of Coefficients:\n\nThe meaning of a coefficient depends on the model’s structure. While a basic model shows unit changes, using logarithms (log-level, level-log, log-log) allows for interpreting relationships in terms of percentage changes or elasticities, which is common in economics."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#what-did-we-do-cont.",
    "href": "lectures/lecture3/lecture3.html#what-did-we-do-cont.",
    "title": "Empirical Economics",
    "section": "What did we do? (Cont.)",
    "text": "What did we do? (Cont.)\n\nEvaluated Assumptions:\n\nFor OLS estimates to be unbiased (correct on average), a set of classical assumptions must hold. The most critical is the Zero Conditional Mean assumption, which states that unobserved factors (like innate ability) must not be correlated with the explanatory variable (like education); otherwise, the estimate will suffer from omitted variable bias.\n\nHypothesis Testing:\n\nAfter estimating a model, we use hypothesis testing to determine if the results are statistically significant.\nThe t-test is used to assess the significance of a single variable, while the F-test is used to assess the joint significance of multiple variables or the overall explanatory power of the model."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#derivation-of-variance-of-ols-slope-estimator",
    "href": "lectures/lecture3/lecture3.html#derivation-of-variance-of-ols-slope-estimator",
    "title": "Empirical Economics",
    "section": "Derivation of Variance of OLS Slope Estimator",
    "text": "Derivation of Variance of OLS Slope Estimator\n\n\n\n\n\n\n\nDerivation of Variance of OLS Estimator\n\n\nConsider the univariate linear regression model:\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\sim \\text{iid } N(0, \\sigma^2)\n\\]\nThe OLS estimator for the slope coefficient \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}.\n\\]\nWhere \\(S_{xx} = \\sum (x_i - \\bar{x})^2\\) and \\(S_{xy} = \\sum (x_i - \\bar{x})(y_i - \\bar{y})\\)."
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#rewriting-the-estimator",
    "href": "lectures/lecture3/lecture3.html#rewriting-the-estimator",
    "title": "Empirical Economics",
    "section": "Rewriting the Estimator",
    "text": "Rewriting the Estimator\n\n\n\n\n\n\n\nDerivation of Variance of OLS Estimator\n\n\nSubstitute \\(y_i = \\alpha + \\beta x_i + \\epsilon_i\\) into the estimator:\n\\[\\begin{align*}\n\\hat{\\beta} &= \\frac{\\sum (x_i - \\bar{x})(\\alpha + \\beta x_i + \\epsilon_i - \\bar{y})}{S_{xx}} \\\\\n&= \\beta + \\frac{\\sum (x_i - \\bar{x})\\epsilon_i}{S_{xx}}\n\\end{align*}\\]\nSince \\(\\text{Var}(\\hat{\\beta}) = \\text{Var}\\left(\\beta + \\frac{\\sum (x_i - \\bar{x})\\epsilon_i}{S_{xx}}\\right) = \\text{Var}\\left(\\frac{\\sum (x_i - \\bar{x})\\epsilon_i}{S_{xx}}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#variance-derivation",
    "href": "lectures/lecture3/lecture3.html#variance-derivation",
    "title": "Empirical Economics",
    "section": "Variance Derivation",
    "text": "Variance Derivation\n\n\n\n\n\n\n\nDerivation of Variance of OLS Estimator\n\n\nGiven that \\(\\epsilon_i\\) are iid with \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\):\n\\[\\begin{align*}\n\\text{Var}(\\hat{\\beta}) &= \\frac{1}{S_{xx}^2} \\text{Var}\\left(\\sum (x_i - \\bar{x})\\epsilon_i\\right) \\\\\n&= \\frac{1}{S_{xx}^2} \\sum (x_i - \\bar{x})^2 \\text{Var}(\\epsilon_i) \\quad \\text{(by indep., variance of sum = sum of variances)} \\\\\n&= \\frac{\\sigma^2}{S_{xx}^2} \\sum (x_i - \\bar{x})^2 \\quad \\text{(Var = $\\sigma^2$, take constant out of sum)} \\\\\n&= \\frac{\\sigma^2}{S_{xx}}\n\\end{align*}\\]\nThe variance of the OLS slope estimator is \\(\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\)"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#linearity-and-normality-of-hatbeta",
    "href": "lectures/lecture3/lecture3.html#linearity-and-normality-of-hatbeta",
    "title": "Empirical Economics",
    "section": "Linearity and Normality of \\(\\hat{\\beta}\\)",
    "text": "Linearity and Normality of \\(\\hat{\\beta}\\)\n\n\n\n\n\n\n\nLinearity and Normality of the OLS Estimator\n\n\nSince \\(\\hat{\\beta}\\) is a linear combination of the $_i$ (which are normally distributed), \\(\\hat{\\beta}\\) is also normally distributed:\n\\[\n\\hat{\\beta} \\sim N\\left(\\beta, \\text{Var}(\\hat{\\beta})\\right)\n\\]\nFrom the previous derivation, we know that \\(\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2}{S_{xx}}\\). Hence, the sampling distribution of \\(\\hat{\\beta}\\) is:\n\\[\n\\hat{\\beta} \\sim N\\left(\\beta, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}\\right) \\text{or equivalently, } \\sim N\\left(\\beta, \\frac{\\sigma^2}{S_{xx}}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture3/lecture3.html#standardized-distribution",
    "href": "lectures/lecture3/lecture3.html#standardized-distribution",
    "title": "Empirical Economics",
    "section": "Standardized Distribution",
    "text": "Standardized Distribution\n\n\n\n\n\n\n\nLinearity and Normality of the OLS Estimator\n\n\nIf we standardize \\(\\hat{\\beta}\\), we obtain a standard normal distribution:\n\\[\n\\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\text{Var}(\\hat{\\beta})}} = \\frac{\\hat{\\beta} - \\beta}{\\sigma / \\sqrt{S_{xx}}} \\sim N(0,1)\n\\]\nFrom this follows:\nUnbiasedness: The expected value of \\(\\hat{\\beta}\\) is the true parameter \\(\\beta\\).\nNormality: The distribution is exactly normal when errors are normal.\nVariance decreases with:\n\nLarger sample size \\(n\\) (generally increases \\(S_{xx}\\)).\nGreater spread in \\(x_i\\) (increases \\(S_{xx}\\)).\nSmaller error variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\nEmpirical Economics: Lecture 3 - The Linear Model"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#course-overview",
    "href": "lectures/lecture7/lecture7.html#course-overview",
    "title": "Empirical Economics",
    "section": "Course Overview",
    "text": "Course Overview\n\nStatistics and Probability - Basic Concepts\nStatistics and Probability - Hypothesis Testing\nThe Linear Regression Model\nTime Series Data\nPanel Data (FE) and Control Variables\nBinary Outcome Data\nPotential Outcomes and Difference-in-differences\nHands-on Econometrics in Practice"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#what-do-we-do-today",
    "href": "lectures/lecture7/lecture7.html#what-do-we-do-today",
    "title": "Empirical Economics",
    "section": "What do we do today?",
    "text": "What do we do today?"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#correlation-vs.-causality",
    "href": "lectures/lecture7/lecture7.html#correlation-vs.-causality",
    "title": "Empirical Economics",
    "section": "Correlation vs. Causality",
    "text": "Correlation vs. Causality\n\nThe fundamental challenge in empirical work.\nCorrelation: Two variables move together.\n\nExample: Ice cream sales are positively correlated with crime rates.\n\nCausation: A change in one variable causes a change in another.\n\nDoes eating ice cream cause crime? Unlikely.\n\nConfounding Variable: A third variable affects both.\n\nHot weather increases both ice cream sales and the number of people outside (leading to more opportunities for crime).\n\nOur goal is to isolate the causal effect, not just the correlation."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#the-potential-outcomes-framework",
    "href": "lectures/lecture7/lecture7.html#the-potential-outcomes-framework",
    "title": "Empirical Economics",
    "section": "The Potential Outcomes Framework",
    "text": "The Potential Outcomes Framework\n\nAlso known as the Rubin Causal Model.\nLet’s think about the effect of a treatment (e.g., a job training program) on an individual i.\n\\(Y_i(1)\\): The potential outcome for unit i if they receive the treatment.\n\\(Y_i(0)\\): The potential outcome for unit i if they do NOT receive the treatment.\n\n\n\n\n\n\n\n\nExample: Potential Outcomes\n\n\nIn the context where the treatment is a job training program:\n\\(Y_i(1)\\): A person’s earnings if they attend the program.\n\\(Y_i(0)\\): A person’s earnings if they do not attend the program."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#the-individual-causal-effect",
    "href": "lectures/lecture7/lecture7.html#the-individual-causal-effect",
    "title": "Empirical Economics",
    "section": "The Individual Causal Effect",
    "text": "The Individual Causal Effect\n\nFor any single individual i, the true causal effect of the treatment is the difference between their two potential outcomes:\n\n\n\n\n\n\n\n\nDefinition: Individual Causal Effect\n\n\n\\[\\tau_i = Y_i(1) - Y_i(0)\\]\n\n\n\n\n\nThis is the pure, unadulterated effect of the treatment on that one person.\nExample: The increase in Person i’s earnings caused only by the training program."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#the-average-treatment-effect-ate",
    "href": "lectures/lecture7/lecture7.html#the-average-treatment-effect-ate",
    "title": "Empirical Economics",
    "section": "The Average Treatment Effect (ATE)",
    "text": "The Average Treatment Effect (ATE)\n\nSince we usually can’t measure the effect for every single individual, we focus on averages.\nThe Average Treatment Effect (ATE) is the average of the individual causal effects over the entire population.\n\nThis tells us, “On average, what is the effect of this treatment for a person randomly drawn from the population?”\nFor future reference, consider also the Average Treatment Effect on the treated population:\n\n\n\n\n\n\n\n\n\nDefinition: Average Treatment Effect\n\n\n\\[\\text{ATE} = E[\\tau_i] = E[Y(1) - Y(0)]\\]\n\\[\\text{ATT} = E[\\tau_i | T=1] = E[Y(1) - Y(0) | T=1]\\]"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#the-fundamental-problem-of-causal-inference",
    "href": "lectures/lecture7/lecture7.html#the-fundamental-problem-of-causal-inference",
    "title": "Empirical Economics",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\n\nThis is the core challenge that all causal methods try to solve.\nFor any given unit i, we can only ever observe one of their potential outcomes.\n\nIf person i takes the training program, we see \\(Y_i(1)\\). We will never know what their earnings would have been without it, \\(Y_i(0)\\).\nIf person i does not take the program, we see \\(Y_i(0)\\). We will never know \\(Y_i(1)\\).\n\nCausal inference is a missing data problem. The \\(Y_i(0)\\) for the treated and the \\(Y_i(1)\\) for the untreated are called counterfactuals."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#illustrating-the-fundamental-problem",
    "href": "lectures/lecture7/lecture7.html#illustrating-the-fundamental-problem",
    "title": "Empirical Economics",
    "section": "Illustrating the Fundamental Problem",
    "text": "Illustrating the Fundamental Problem\n\nThe following illustrates the data we have at our disposal.\n\n\n\n\n\n\n\n\n\n\n\n\nUnit (i)\nAttends Program?\nObserved Earnings\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\n\n\n\nAlice\nYes (T=1)\n$50,000\n$50,000\n???\n\n\nBob\nNo (T=0)\n$40,000\n???\n$40,000\n\n\nCarol\nYes (T=1)\n$45,000\n$45,000\n???\n\n\nDavid\nNo (T=0)\n$60,000\n???\n$60,000\n\n\n\n\n\nWe can’t calculate \\(Y_i(1) - Y_i(0)\\) for anyone!"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#why-simple-comparisons-fail",
    "href": "lectures/lecture7/lecture7.html#why-simple-comparisons-fail",
    "title": "Empirical Economics",
    "section": "Why Simple Comparisons Fail",
    "text": "Why Simple Comparisons Fail\n\nA naive approach might be to just compare the average earnings of those who attended the program to those who didn’t.\n\\[\n  \\text{Difference-in-means} = E[Y | T=1] - E[Y | T=0]\n\\]\nThis is almost always wrong. Why?\nBecause the people who choose to get treatment might be different from those who don’t in ways that also affect the outcome.\nThis is referred to as Selection Bias"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#selection-bias-the-hidden-difference",
    "href": "lectures/lecture7/lecture7.html#selection-bias-the-hidden-difference",
    "title": "Empirical Economics",
    "section": "Selection Bias: The Hidden Difference",
    "text": "Selection Bias: The Hidden Difference\n\nThe simple difference-in-means can be decomposed:\n\n\n\\[\\begin{align*}\n    \\text{Difference-in-means} &= E[Y|T=1] - E[Y|T=0] \\\\\n    &= E[Y(1)|T=1] - E[Y(0)|T=0] \\\\\n    &= E[Y(1)|T=1] - \\overbrace{E[Y(0)|T=1] + E[Y(0)|T=1]}^{\\text{Subtract and add the same term.}} - E[Y(0)|T=0] \\\\\n    &= \\underbrace{\\left( E[Y(1)|T=1] - E[Y(0)|T=1] \\right)}_{\\text{Average Treatment Effect on the Treated (ATT)}} \\\\\n     &\\quad \\quad \\hspace{3em} + \\underbrace{\\left( E[Y(0)|T=1] - E[Y(0)|T=0] \\right)}_{\\text{Selection Bias}}\n\\end{align*}\\]\n\n\nThe selection bias tells you the difference in the untreated potential outcomes between the treatment and control groups."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#selection-bias-decomposition",
    "href": "lectures/lecture7/lecture7.html#selection-bias-decomposition",
    "title": "Empirical Economics",
    "section": "Selection Bias: Decomposition",
    "text": "Selection Bias: Decomposition\n\nHence, we can make the following decomposition:\n\n\n\n\n\n\n\n\nTheorem: Decomposition of Sample Average\n\n\n\\[E[Y|T=1] - E[Y|T=0] = ATT + \\text{Selection Bias},\\]\nwhere \\(\\text{Selection Bias} = E[Y(0)|T=1] - E[Y(0)|T=0].\\)\n\n\n\n\n\nIn words, selection bias is the difference in the no-treatment outcome between the treated and untreated groups.\nJob Program Example: People who sign up for training (\\(T=1\\)) might be more motivated. Even without the program, their earnings \\(Y(0)\\) might have been higher than the less motivated group (\\(T=0\\))."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#introduction-to-differences-in-differences-did",
    "href": "lectures/lecture7/lecture7.html#introduction-to-differences-in-differences-did",
    "title": "Empirical Economics",
    "section": "Introduction to Differences-in-Differences (DiD)",
    "text": "Introduction to Differences-in-Differences (DiD)\n\nThe core idea in DiD is to use data from a pre-treatment period to account for selection bias.\n\nWe assume that the “selection bias” (the difference between the groups) is constant over time.\n\nWe compare the change in the outcome over time for the treatment group to the change over time for a control group.\n\nThe “difference in the differences” isolates the treatment effect."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#the-2-times-2-did-setup",
    "href": "lectures/lecture7/lecture7.html#the-2-times-2-did-setup",
    "title": "Empirical Economics",
    "section": "The \\(2 \\times 2\\) DiD Setup",
    "text": "The \\(2 \\times 2\\) DiD Setup\n\nThe classic setup involves two groups and two time periods.\n\n\n\n\n\n\nBefore Period (Pre)\nAfter Period (Post)\n\n\n\n\nTreatment Group\n\\(\\hat{Y}_{T, Pre}\\)\n\\(\\hat{Y}_{T, Post}\\)\n\n\nControl Group\n\\(\\hat{Y}_{C, Pre}\\)\n\\(\\hat{Y}_{C, Post}\\)\n\n\n\n\n\nTreatment Group: A group that is exposed to the policy/treatment in the “After” period.\nControl Group: A similar group that is not exposed to the treatment in either period."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#calculating-the-simple-did-estimator",
    "href": "lectures/lecture7/lecture7.html#calculating-the-simple-did-estimator",
    "title": "Empirical Economics",
    "section": "Calculating the Simple DiD Estimator",
    "text": "Calculating the Simple DiD Estimator\n\nWe calculate two differences, then take the difference between them.\n\n\n\n\n\n\n\n\nManual Calculation of DiD Estimator\n\n\n\nFirst Difference (Treatment Group): The change over time for the treated.\n\\(\\Delta_T = \\hat{Y}_{T,Post} - \\hat{Y}_{T, Pre}\\)\nFirst Difference (Control Group): The change over time for the controls. This represents the “secular trend” – what would have happened without the treatment.\n\\(\\Delta_C = \\hat{Y}_{C, Post} - \\hat{Y}_{C, Pre}\\)\nThe Difference-in-Differences:\n\\(\\tau_{DiD} = \\Delta_T - \\Delta_C\\)"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#example-did-in-a-2-times-2-set-up",
    "href": "lectures/lecture7/lecture7.html#example-did-in-a-2-times-2-set-up",
    "title": "Empirical Economics",
    "section": "Example: DiD in a \\(2 \\times 2\\) Set-Up",
    "text": "Example: DiD in a \\(2 \\times 2\\) Set-Up\n\n\n\n\n\n\n\nExample: Card and Krueger (1994)\n\n\nThe study by Card and Alan Krueger (AER, 1994), titled “Minimum Wages and Employment: A Case Study of the Fast Food Industry in New Jersey and Pennsylvania,” is a landmark paper in labor economics.\nIt challenged the conventional wisdom that raising the minimum wage necessarily reduces employment.\nThe authors analyzed the impact of New Jersey’s 1992 minimum wage increase (from $4.25 to $5.05 per hour) by comparing employment changes at fast-food restaurants in New Jersey (where the wage rose) to those in Pennsylvania (where it remained unchanged).\nSurprisingly, they found that employment in New Jersey increased by 13% relative to Pennsylvania, contradicting traditional economic predictions"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#example-did-in-a-2-times-2-set-up-1",
    "href": "lectures/lecture7/lecture7.html#example-did-in-a-2-times-2-set-up-1",
    "title": "Empirical Economics",
    "section": "Example: DiD in a \\(2 \\times 2\\) Set-Up",
    "text": "Example: DiD in a \\(2 \\times 2\\) Set-Up\n\nRPythonStata\n\n\n\nDownload the Card & Krueger (1994) data:\n\n\n\nCode\nlibrary(tidyverse); library(fixest)\nfile_url &lt;- \"https://github.com/mca91/EconometricsWithR/blob/master/data/fastfood.dta?raw=true\"\ncard_krueger_data &lt;- foreign::read.dta(file_url) |&gt; as_tibble()\nhead(card_krueger_data)\n## # A tibble: 6 × 46\n##   sheet chain co_owned state southj centralj northj   pa1   pa2 shore ncalls\n##   &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;int&gt;  &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n## 1    46     1        0     0      0        0      0     1     0     0      0\n## 2    49     2        0     0      0        0      0     1     0     0      0\n## 3   506     2        1     0      0        0      0     1     0     0      0\n## 4    56     4        1     0      0        0      0     1     0     0      0\n## 5    61     4        1     0      0        0      0     1     0     0      0\n## 6    62     4        1     0      0        0      0     1     0     0      2\n## # ℹ 35 more variables: empft &lt;dbl&gt;, emppt &lt;dbl&gt;, nmgrs &lt;dbl&gt;, wage_st &lt;dbl&gt;,\n## #   inctime &lt;dbl&gt;, firstinc &lt;dbl&gt;, bonus &lt;int&gt;, pctaff &lt;dbl&gt;, meals &lt;int&gt;,\n## #   open &lt;dbl&gt;, hrsopen &lt;dbl&gt;, psoda &lt;dbl&gt;, pfry &lt;dbl&gt;, pentree &lt;dbl&gt;,\n## #   nregs &lt;int&gt;, nregs11 &lt;int&gt;, type2 &lt;int&gt;, status2 &lt;int&gt;, date2 &lt;int&gt;,\n## #   ncalls2 &lt;int&gt;, empft2 &lt;dbl&gt;, emppt2 &lt;dbl&gt;, nmgrs2 &lt;dbl&gt;, wage_st2 &lt;dbl&gt;,\n## #   inctime2 &lt;int&gt;, firstin2 &lt;dbl&gt;, special2 &lt;int&gt;, meals2 &lt;int&gt;, open2r &lt;dbl&gt;,\n## #   hrsopen2 &lt;dbl&gt;, psoda2 &lt;dbl&gt;, pfry2 &lt;dbl&gt;, pentree2 &lt;dbl&gt;, nregs2 &lt;int&gt;, …\n\n\n\nCompute \\(\\tau_{DID}\\)\n\n\n\nCode\n# Treatment group before and after\ntreated &lt;- card_krueger_data |&gt;\n  filter(state==1)|&gt;\n  summarize(treated_emp_before = mean(employment[time=='before'], na.rm=T),\n            treated_emp_after = mean(employment[time=='after'], na.rm=T))\n\ntreated\n## # A tibble: 1 × 2\n##   treated_emp_before treated_emp_after\n##                &lt;dbl&gt;             &lt;dbl&gt;\n## 1               20.4              21.0\n\ncontrol &lt;- card_krueger_data |&gt;\n  filter(state==0)|&gt;\n  summarize(control_emp_before = mean(employment[time=='before'], na.rm=T),\n            control_emp_after = mean(employment[time=='after'], na.rm=T))\n\ncontrol\n## # A tibble: 1 × 2\n##   control_emp_before control_emp_after\n##                &lt;dbl&gt;             &lt;dbl&gt;\n## 1               23.3              21.2\n\ntau_did &lt;- treated$treated_emp_after - treated$treated_emp_before - (control$control_emp_after - control$control_emp_before)\n\ntau_did\n## [1] 2.753606\n\npre_treatment_mean &lt;- card_krueger_data |&gt; \n  filter(state==1, time=='before') |&gt; \n  summarize(m = mean(employment, na.rm=T)) |&gt;\n  pull(m)\n\ntau_did/pre_treatment_mean\n## [1] 0.1347204\n\n\n\n\n\nDownload the Card & Krueger (1994) data\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Load data\nfile_url = \"https://github.com/mca91/EconometricsWithR/blob/master/data/fastfood.dta?raw=true\"\ncard_krueger_data = pd.read_stata(file_url)\n\n# Preview data\nprint(card_krueger_data.head())\n##    sheet  chain  co_owned  state  ...  pfry2  pentree2  nregs2  nregs112\n## 0     46      1         0      0  ...    NaN      0.94     4.0       4.0\n## 1     49      2         0      0  ...   0.89      2.35     4.0       4.0\n## 2    506      2         1      0  ...   0.74      2.33     4.0       3.0\n## 3     56      4         1      0  ...   0.79      0.87     2.0       2.0\n## 4     61      4         1      0  ...   0.84      0.95     2.0       2.0\n## \n## [5 rows x 46 columns]\n\n\n\nCompute \\(\\tau_{DID}\\)\n\n\n\nCode\n# Treatment group before and after\ntreated = card_krueger_data[card_krueger_data['state'] == 1]\ntreated_before = treated[treated['time'] == 'before']['employment'].mean()\ntreated_after = treated[treated['time'] == 'after']['employment'].mean()\n\nprint(\"\\nTreated group:\")\n## \n## Treated group:\nprint(f\"Before: {treated_before}\")\n## Before: 20.439407348632812\nprint(f\"After: {treated_after}\")\n## After: 21.027429580688477\n\n# Control group before and after\ncontrol = card_krueger_data[card_krueger_data['state'] == 0]\ncontrol_before = control[control['time'] == 'before']['employment'].mean()\ncontrol_after = control[control['time'] == 'after']['employment'].mean()\n\nprint(\"\\nControl group:\")\n## \n## Control group:\nprint(f\"Before: {control_before}\")\n## Before: 23.33116912841797\nprint(f\"After: {control_after}\")\n## After: 21.165584564208984\n\n# Calculate difference-in-differences\ntau_did = (treated_after - treated_before) - (control_after - control_before)\n\nprint(\"\\nDifference-in-differences estimate:\")\n## \n## Difference-in-differences estimate:\nprint(tau_did)\n## 2.7536068\n\nprint(\"\\nRelative to pre-treatment mean:\")\n## \n## Relative to pre-treatment mean:\ntau_did / treated_before\n## 0.13472047\n\n\n\n\n\nDownload the Card & Krueger (1994) data\n\n\n\nCode\n* Load data from URL\ncopy \"https://github.com/mca91/EconometricsWithR/blob/master/data/fastfood.dta?raw=true\" fastfood.dta, replace\nuse fastfood.dta, clear\n\n* Preview data\nlist in 1/5\n\n\n\nCompute \\(\\tau_{DID}\\)\n\n\n\nCode\n* Calculate difference-in-differences\nsum fte if state == 1 & time == 1  // treated before\nlocal treated_before = r(mean)\nsum fte if state == 1 & time == 2  // treated after\nlocal treated_after = r(mean)\nsum fte if state == 0 & time == 1  // control before\nlocal control_before = r(mean)\nsum fte if state == 0 & time == 2  // control after\nlocal control_after = r(mean)\n\nlocal did = (`treated_after' - `treated_before') - (`control_after' - `control_before')\ndisplay \"Difference-in-differences estimate: \" `did'\n\ndisplay \"Relative to pre-treatment mean:\" `did / treated_before'"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#visualization",
    "href": "lectures/lecture7/lecture7.html#visualization",
    "title": "Empirical Economics",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#deconstructing-the-did-graph",
    "href": "lectures/lecture7/lecture7.html#deconstructing-the-did-graph",
    "title": "Empirical Economics",
    "section": "Deconstructing the DiD Graph",
    "text": "Deconstructing the DiD Graph\n\nThe solid blue line is the observed trend for the control group.\nThe solid green line is the observed outcome for the treatment group.\nThe dotted orange line is the counterfactual for the treatment group, constructed by assuming its trend would have been parallel to the control group’s trend.\nThe DiD effect is the vertical distance between the actual outcome for the treatment group and its counterfactual outcome in the post-period."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#did-under-potential-outcomes-1",
    "href": "lectures/lecture7/lecture7.html#did-under-potential-outcomes-1",
    "title": "Empirical Economics",
    "section": "DiD Under Potential Outcomes",
    "text": "DiD Under Potential Outcomes\n\nThe observed outcome, \\(Y_{it}\\), is determined by the unit’s group status and the time period.\nFor the treated group (\\(D_i=1\\)): They are untreated at \\(t=0\\) and treated at \\(t=1\\).\n\n\\(Y_{i0} = Y_{i0}(0)\\) for \\(t=0\\) (pre-treatment)\n\\(Y_{i1} = Y_{i1}(1)\\) for \\(t=1\\) (post-treatment)\n\nFor the control group (\\(D_i=0\\)): They are never treated.\n\n\\(Y_{i0} = Y_{i0}(0)\\) for \\(t=0\\)\n\\(Y_{i1} = Y_{i1}(0)\\) for \\(t=1\\)"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#example-interpretation",
    "href": "lectures/lecture7/lecture7.html#example-interpretation",
    "title": "Empirical Economics",
    "section": "Example Interpretation",
    "text": "Example Interpretation\n\n\n\n\n\n\n\nExample: DiD Under Potential Outcomes\n\n\nLet the treatment \\(D\\) be a job training program. Let the outcome \\(Y\\) be monthly income. Let the treated group \\(D_i=1\\) consist of Alice, and the control group \\(D_i=0\\) consist of Bob.\n\nFor the treated group:\n\nPre-treatment (\\(t=0\\)): \\(Y_{i0} = Y_{i0} (0)\\)\nAlice’s observed income before training is her potential income without training.\nPost-treatment (\\(t=1\\)): \\(Y_{i1} = Y_{i1} (1)\\)\nAlice’s observed income after training is her potential income with training.\n\nFor the control group:\n\nPre-treatment (\\(t=0\\)): \\(Y_{i0} = Y_{i0} (0)\\)\nBob’s observed income is his potential income without training.\nPost-treatment (\\(t=1\\)): \\(Y_{i1} = Y_{i1} (0)\\)\nBob’s observed income is his potential income without training, as he remains untreated."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#estimand-att",
    "href": "lectures/lecture7/lecture7.html#estimand-att",
    "title": "Empirical Economics",
    "section": "Estimand: ATT",
    "text": "Estimand: ATT\n\nFormally, the ATT is the difference between the treated group’s outcome at \\(t=1\\) and what their outcome would have been at \\(t=1\\) if they had not been treated.\n\\[\n  \\text{ATT} = E[Y_{i1}(1) | D_i=1] - E[Y_{i1}(0) | D_i=1]\n\\]\nThe first term, \\(E[Y_{i1}(1) | D_i=1]\\), is observed as the average outcome for the treated group in the post-period, \\(E[Y_{i1} | D_i=1]\\).\nThe second term, \\(E[Y_{i1}(0) | D_i=1]\\), is the counterfactual.\n\nIt is the unobservable average outcome for the treated group had they not received the treatment.\nThe entire goal of the DiD strategy is to find a way to estimate this term."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#parallel-trends",
    "href": "lectures/lecture7/lecture7.html#parallel-trends",
    "title": "Empirical Economics",
    "section": "Parallel Trends",
    "text": "Parallel Trends\n\nThe DiD estimator is valid under the parallel trends assumption.\n\nThis assumption states that, in the absence of treatment, the average outcome for the treated group would have changed over time by the same amount as the average outcome for the control group.\nMathematically, this is expressed in terms of the potential outcome under no-treatment, \\(Y_{it}(0)\\):\n\n\n\n\n\n\n\n\n\nDefinition: Parallel Trends\n\n\n\\[E[Y_{i1}(0) - Y_{i0}(0) | D_i=1] = E[Y_{i1}(0) - Y_{i0}(0) | D_i=0] \\qquad(1)\\]\n\n\n\n\n\nThe left side is the counterfactual change for the treated group.\nThe right side is the observed change for the control group, since for them \\(Y_{it} = Y_{it}(0)\\).\n\nThis assumption allows us to use the control group to identify the counterfactual trend for the treated group."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#derivation-of-the-did-estimator",
    "href": "lectures/lecture7/lecture7.html#derivation-of-the-did-estimator",
    "title": "Empirical Economics",
    "section": "Derivation of the DiD Estimator",
    "text": "Derivation of the DiD Estimator\n\nWe rearrange the parallel trends assumption from Equation 1 to solve for our unobserved counterfactual:\n\n\\[\\begin{align}\n\\label{eq:cf}\nE[Y_{i1}(0) | D_i=1] = &\\underbrace{E[Y_{i0}(0) | D_i=1]}_{\\text{Treated pre-treatment}} + \\\\ &\\underbrace{\\left( E[Y_{i1}(0) | D_i=0] - E[Y_{i0}(0) | D_i=0] \\right)}_{\\text{Change in control group}}\n\\end{align}\\]\n\nThis equation shows how we construct the counterfactual: we take the treated group’s initial level and add the change experienced by the control group."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#derivation-of-the-did-estimator-1",
    "href": "lectures/lecture7/lecture7.html#derivation-of-the-did-estimator-1",
    "title": "Empirical Economics",
    "section": "Derivation of the DiD Estimator",
    "text": "Derivation of the DiD Estimator\n\nSubstitute this expression for the counterfactual back into the definition of ATT:\n\n\\[\n\\begin{align}\n\\text{ATT} &= E[Y_{i1}(1) | D_i=1] - E[Y_{i1}(0) | D_i=1] \\\\\n    &= E[Y_{i1}(1) | D_i=1] - \\\\\n    &\\quad \\underbrace{\\left( E[Y_{i0}(0) | D_i=1] + E[Y_{i1}(0) | D_i=0] - E[Y_{i0}(0) | D_i=0] \\right)}_{\\text{Expression for counterfactual}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#replacing-potential-outcomes-with-observables",
    "href": "lectures/lecture7/lecture7.html#replacing-potential-outcomes-with-observables",
    "title": "Empirical Economics",
    "section": "Replacing Potential Outcomes with Observables",
    "text": "Replacing Potential Outcomes with Observables\n\nFinally, we replace the potential outcomes with their observable counterparts to obtain \\(\\widehat{\\tau_{\\text{DID}}}\\):\n\\[\n  \\begin{align}\n  \\text{ATT} &= \\color{red}{E[Y_{i1}(1) | D_i=1]} - \\\\\n  &\\quad \\biggl( \\color{blue}{E[Y_{i0}(0) | D_i=1]} + \\color{orange}{E[Y_{i1}(0) | D_i=0]} - \\color{green}{E[Y_{i0}(0) | D_i=0]} \\biggr) \\\\\n  &= \\color{red}{E[Y_{i1} | D_i = 1 ]} - \\\\\n  &\\quad \\biggl(  \\color{blue}{E[Y_{i0} | D_i = 1]} + \\color{orange}{E[Y_{i1} | D_i = 0]} - \\color{green}{E[Y_{i0} | D_i = 0]}\\biggr) \\\\\n  &= \\widehat{\\tau_{DID}}\n  \\end{align}\n\\]\nThis is the famous “difference-in-differences” formula. It identifies the ATT under the crucial assumption of parallel trends."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#did-using-a-regression-framework",
    "href": "lectures/lecture7/lecture7.html#did-using-a-regression-framework",
    "title": "Empirical Economics",
    "section": "DiD using a Regression Framework",
    "text": "DiD using a Regression Framework\n\nWe can estimate the exact same 2x2 DiD using a simple OLS regression. This is more powerful and flexible.\n\n\n\n\n\n\n\n\nDefinition: DiD in a Regression Framework\n\n\n\\(Y_{it} = \\beta_0 + \\beta_1 Treat_i + \\beta_2 Post_t + \\beta_3(Treat_i × Post_t) + \\epsilon_{it}\\)\n\n\\(Y_{it}\\): Outcome for unit i at time t.\n\\(Treat_i\\): A dummy variable = 1 if unit i is in the treatment group, 0 otherwise.\n\\(Post_t\\): A dummy variable = 1 if the period is “Post”, 0 otherwise.\n\\(Treat_i \\times Post_t\\): An interaction term."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#interpretation-of-coefficients-12",
    "href": "lectures/lecture7/lecture7.html#interpretation-of-coefficients-12",
    "title": "Empirical Economics",
    "section": "Interpretation of Coefficients (1/2)",
    "text": "Interpretation of Coefficients (1/2)\n\nLet’s break down what each \\(\\beta\\) represents:\n\n\\(Y_{it} = \\beta_0 + \\beta_1 Treat_i + \\beta_2 Post_t + \\beta_3(Treat_i \\times Post_t) + \\epsilon_{it}\\)\n\n\\(\\beta_0\\) (Intercept): The average outcome for the Control Group (Treat=0) in the Pre-Period (Post=0).\n\n\\(E[Y | Treat=0, Post=0] = \\beta_0\\)\n\n\\(\\beta_1\\): The average pre-existing difference between the treatment and control groups in the pre-period. This is the selection bias.\n\n\\(E[Y | Treat=1, Post=0] = \\beta_0 + \\beta_1\\)\nSo, \\(\\beta_1 = E[Y | Treat=1, Post=0] - E[Y | Treat=0, Post=0]\\)"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#interpretation-of-coefficients-22",
    "href": "lectures/lecture7/lecture7.html#interpretation-of-coefficients-22",
    "title": "Empirical Economics",
    "section": "Interpretation of Coefficients (2/2)",
    "text": "Interpretation of Coefficients (2/2)\n\\[\n    Y_{it} = \\beta_0 + \\beta_1Treat_i + \\beta_2Post_t + \\beta_3(Treat_i \\times Post_t) + \\epsilon_{it}\n  \\]\n\n\\(\\beta_2\\): The average change in the outcome for the Control Group from the pre- to the post-period. This is the secular trend.\n\n\\(E[Y | Treat=0, Post=1] = \\beta_0 + \\beta_2\\)\nSo, \\(\\beta_2 = E[Y | Treat=0, Post=1] - E[Y | Treat=0, Post=0]\\)\n\n\\(\\beta_3\\): This is the DiD estimator! It’s the additional change in the outcome for the Treatment Group, above and beyond the secular trend.\n\nIt is the causal effect of interest.\n\\(\\beta_3 = (E[Y|T=1,P=1] - E[Y|T=1,P=0]) - (E[Y|T=0,P=1] - E[Y|T=0,P=0])\\)\nHence \\(\\beta_3\\) is \\(\\tau_{DID}\\)"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#example-did-in-a-regression-framework",
    "href": "lectures/lecture7/lecture7.html#example-did-in-a-regression-framework",
    "title": "Empirical Economics",
    "section": "Example: DiD in a Regression Framework",
    "text": "Example: DiD in a Regression Framework\n\n\n\n\n\n\n\nExample: Card and Krueger (1994)\n\n\nThe Card and Krueger (1994) estimate can also be recovered using the specification we have seen above. Recall that \\(\\tau_{DID}=2.75\\).\n\nRPythonStata\n\n\n\n\nCode\nmodel &lt;- lm(employment ~ state*time, data = card_krueger_data)\nsummary(model)\n## \n## Call:\n## lm(formula = employment ~ state * time, data = card_krueger_data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -21.166  -6.439  -1.027   4.473  64.561 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       23.331      1.072  21.767   &lt;2e-16 ***\n## state             -2.892      1.194  -2.423   0.0156 *  \n## timeafter         -2.166      1.516  -1.429   0.1535    \n## state:timeafter    2.754      1.688   1.631   0.1033    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.406 on 790 degrees of freedom\n##   (26 observations deleted due to missingness)\n## Multiple R-squared:  0.007401,   Adjusted R-squared:  0.003632 \n## F-statistic: 1.964 on 3 and 790 DF,  p-value: 0.118\n\n\n\n\n\n\nCode\nimport pyfixest as pf\nmodel = pf.feols(\"employment ~ time + state + time*state\", data=r.card_krueger_data)\nmodel.summary()\n## ###\n## \n## Estimation:  OLS\n## Dep. var.: employment, Fixed effects: 0\n## Inference:  iid\n## Observations:  794\n## \n## | Coefficient         |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n## |:--------------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n## | Intercept           |     23.331 |        1.072 |    21.767 |      0.000 | 21.227 |  25.435 |\n## | time[T.after]       |     -2.166 |        1.516 |    -1.429 |      0.154 | -5.141 |   0.810 |\n## | state               |     -2.892 |        1.194 |    -2.423 |      0.016 | -5.235 |  -0.549 |\n## | time[T.after]:state |      2.754 |        1.688 |     1.631 |      0.103 | -0.561 |   6.068 |\n## ---\n## RMSE: 9.382 R2: 0.007\n\n\n\n\nHey"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#advantages-of-the-regression-framework",
    "href": "lectures/lecture7/lecture7.html#advantages-of-the-regression-framework",
    "title": "Empirical Economics",
    "section": "Advantages of the Regression Framework",
    "text": "Advantages of the Regression Framework\n\nWhy bother with regression instead of just calculating the four means?\n\n\nStandard Errors: Regression automatically provides standard errors, t-statistics, and p-values for your DiD estimate (\\(\\beta_3\\)), allowing for statistical inference.\nAdding Covariates: It is easy to add control variables to the model to increase precision and make the parallel trends assumption more plausible.\nFlexibility: The framework is easily extended to more complex scenarios (more groups, more time periods, etc.)."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#adding-covariates-to-the-did-model",
    "href": "lectures/lecture7/lecture7.html#adding-covariates-to-the-did-model",
    "title": "Empirical Economics",
    "section": "Adding Covariates to the DiD Model",
    "text": "Adding Covariates to the DiD Model\n\nWe can add a vector of control variables, \\(X\\), to the regression.\n\\[\n  Y_{it} = \\beta_0 + \\beta_1Treat_i + \\beta_2Post_t + \\beta_3(Treat_i \\times Post_t) + \\gamma'X_{it} + \\epsilon_{it}\n\\]\nThe purpose is to control for observable characteristics that might differ between the groups and affect trends in the outcome.\nThis helps strengthen the parallel trends assumption. It becomes “parallel trends conditional on X”.\nExample: When studying a state-level policy, you might control for state GDP, population size, etc."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#testing-the-parallel-trends-assumption",
    "href": "lectures/lecture7/lecture7.html#testing-the-parallel-trends-assumption",
    "title": "Empirical Economics",
    "section": "Testing the Parallel Trends Assumption",
    "text": "Testing the Parallel Trends Assumption\n\nWe can’t prove the assumption, but we can build evidence for it. This requires data from multiple pre-treatment periods.\nMethod 1: Visual Inspection (Most Common)\n\nPlot the average outcomes for the treatment and control groups over time.\nVisually check if their trends were parallel in the periods before the treatment was introduced."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#statistical-tests-for-parallel-trends",
    "href": "lectures/lecture7/lecture7.html#statistical-tests-for-parallel-trends",
    "title": "Empirical Economics",
    "section": "Statistical Tests for Parallel Trends",
    "text": "Statistical Tests for Parallel Trends\n\nIf you have multiple pre-treatment periods, you can run a “placebo” test.\n\nThe idea is to run a DiD analysis using only pre-treatment data. For instance, define a fake “treatment” in period t-2 and use t-3 as the “pre” period.\n\nIn a regression: interact the treatment group dummy with time-period dummies for each pre-treatment period:\n\n\n\n\n\n\n\n\nDiD Placebo Test\n\n\n\\[\n\\begin{align}\n    Y_{it} = &\\beta_0 + \\beta_1 \\text{Treat}_i + \\beta_2 \\text{Post}_t + \\dots + \\\\ &\\delta_{-2} (\\text{Treat} \\times \\text{PrePeriod}_{-2}) + \\delta_{-1} (\\text{Treat} \\times \\text{PrePeriod}_{-1}) + \\\\\n    &\\beta_3(\\text{Treat}_i \\times \\text{Post}_t) + \\epsilon_{it}\n\\end{align}\n\\]\nResult: The coefficients \\(\\delta_{-2}\\) and \\(\\delta_{-1}\\) should be small and statistically insignificant (not different from zero). This shows there was no pre-existing differential trend."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#extension-multiple-periods-staggered-adoption",
    "href": "lectures/lecture7/lecture7.html#extension-multiple-periods-staggered-adoption",
    "title": "Empirical Economics",
    "section": "Extension: Multiple Periods & Staggered Adoption",
    "text": "Extension: Multiple Periods & Staggered Adoption\n\nThe real world is often messier than 2x2.\nOften times, treatments have staggered adoption:\n\nDifferent units receive the treatment at different times.\nFor example, State A adopts a policy in 2010, State B in 2012, State C never does.\n\nThe simple 2x2 Treat \\(\\times\\) Post model is no longer sufficient and can be biased.\nModern methods (e.g., Callaway & Sant’Anna, Sun & Abraham) address these issues by using better defined control groups for each treated unit."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#staggered-adoption",
    "href": "lectures/lecture7/lecture7.html#staggered-adoption",
    "title": "Empirical Economics",
    "section": "Staggered Adoption",
    "text": "Staggered Adoption\n\nMany modern DiD settings involve , where different units get treated at different times.\nThe standard tool for this has been the two-way fixed effects (TWFE) regression:\n\\[\\begin{equation*}\n    Y_{it} = \\alpha_i + \\lambda_t + \\beta^{TWFE} \\cdot D_{it} + \\epsilon_{it}\n\\end{equation*}\\] where \\(D_{it}=1\\) if unit \\(i\\) is treated at time \\(t\\).\nRecent research (Goodman-Bacon, 2021; de Chaisemartin & D’Haultfœuille, 2020; 2022) shows that \\(\\hat{\\beta}^{TWFE}\\) is a weighted average of all possible 2x2 DiD estimators.\nSome of these comparisons are “bad”: they use already-treated units as controls for later-treated units."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#why-are-bad-comparisons-a-problem",
    "href": "lectures/lecture7/lecture7.html#why-are-bad-comparisons-a-problem",
    "title": "Empirical Economics",
    "section": "Why Are Bad Comparisons A Problem?",
    "text": "Why Are Bad Comparisons A Problem?\n\n\n\n\n\n\n\nExample: Bad Comparison\n\n\nImagine Cohort 2010 gets treated in 2010 and Cohort 2012 gets treated in 2012. To estimate the effect on Cohort 2012 in the year 2012, TWFE implicitly uses the observations from Cohort 2010 as part of the “control” group. But Cohort 2010 has already been treated for two years!\n\n\n\n\n\nThis is only valid if treatment effects are constant across cohorts and time. If effects are heterogeneous or dynamic (e.g., they grow over time), this comparison is contaminated.\nThe resulting \\(\\hat{\\beta}^{TWFE}\\) can be a meaningless average, sometimes with negative weights, and may not represent any true ATT."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#sun-and-abraham-2021-solution",
    "href": "lectures/lecture7/lecture7.html#sun-and-abraham-2021-solution",
    "title": "Empirical Economics",
    "section": "Sun and Abraham (2021) Solution",
    "text": "Sun and Abraham (2021) Solution\n\nThe core idea is to avoid aggregation and bad comparisons.\n\n\n\n\n\n\n\n\nSun and Abraham (2021) Procedure\n\n\nStep 1: Define cohorts\nA cohort, \\(G=g\\), is the group of all units that are first treated at the same time period \\(g\\).\nLet \\(G=\\infty\\) (or \\(G=C\\)) be the group.\nStep 2: Estimate Cohort-Specific ATTs\nInstead of one \\(\\beta\\), estimate a separate effect for each cohort \\(g\\) at each time period \\(\\ell\\).\nUse only clean controls: units that are not yet treated. The never-treated group (\\(G=C\\)) is the cleanest control."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#identification-of-cohort-specific-att",
    "href": "lectures/lecture7/lecture7.html#identification-of-cohort-specific-att",
    "title": "Empirical Economics",
    "section": "Identification of Cohort-Specific ATT",
    "text": "Identification of Cohort-Specific ATT\n\nThe estimand of interest is the ATT for cohort \\(g\\) at calendar time \\(\\ell\\) (where \\(\\ell \\ge g\\)). We denote this \\(\\text{ATT}(g, \\ell)\\).\nFor each pair \\((g, \\ell)\\), \\(\\text{ATT}(g, \\ell)\\) is identified by a simple 2x2 DiD comparing cohort \\(g\\) to the clean control group (\\(C\\)) between the pre-treatment period (\\(g-1\\)) and period \\(\\ell\\):\n\n\\[\\begin{align*}\n    \\widehat{\\text{ATT}}(g, \\ell) = \\Big( &E[Y_\\ell | G=g] - E[Y_{g-1} | G=g] \\Big) \\\\\n    - \\Big( &E[Y_\\ell | G=C] - E[Y_{g-1} | G=C] \\Big)\n\\end{align*}\\]\n\nIdentification assumption in this case is a cohort-specific parallel trends assumption.\n\nIn the absence of treatment, the outcome for cohort \\(g\\) would have evolved in parallel to the outcome for the never-treated (or not-yet-treated) group."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#bias-variance-trade-off",
    "href": "lectures/lecture7/lecture7.html#bias-variance-trade-off",
    "title": "Empirical Economics",
    "section": "Bias-variance Trade-off",
    "text": "Bias-variance Trade-off\n\nThe Sun and Abraham (2021) framework is flexible. You can choose your control group to manage the bias-variance trade-off:\nOption A: Never-Treated Controls:\n\nThis is the cleanest option, requiring the weakest parallel trends assumption (only vs. never-treated). It is preferred if the group is large enough.\n\nOption B: Not-Yet-Treated Controls.\n\nThe estimator can use all currently untreated units as a time-varying control group.\nThis substantially increases statistical power and reduces variance. The cost is a slightly stronger (but still plausible) parallel trends assumption against these not-yet-treated units."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#visualizing-staggered-did-event-study-plots",
    "href": "lectures/lecture7/lecture7.html#visualizing-staggered-did-event-study-plots",
    "title": "Empirical Economics",
    "section": "Visualizing Staggered DiD: Event Study Plots",
    "text": "Visualizing Staggered DiD: Event Study Plots\n\nWe have seen this already in Lecture 4\n\nThese plots are standard for visualizing results from models with multiple periods.\n\nX-axis: Time relative to the treatment event (e.g., -3, -2, -1, 0, +1, +2 years).\nY-axis: The estimated DiD coefficient for that relative time period.\nInterpretation:\n\nCoefficients for pre-periods (\\(t &lt; 0\\)) should be near zero (validates parallel trends).\nCoefficients for post-periods (\\(t \\geq 0\\)) show the dynamic causal effect of the policy over time."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#visualization-of-sun-and-abraham-2021-approach",
    "href": "lectures/lecture7/lecture7.html#visualization-of-sun-and-abraham-2021-approach",
    "title": "Empirical Economics",
    "section": "Visualization of Sun and Abraham (2021) approach",
    "text": "Visualization of Sun and Abraham (2021) approach\n\nThis is the dataset used in the study by Sun and Abraham (2021)\n\n\nRPythonStata\n\n\n\n\nCode\nlibrary(fixest)\ndata(base_stagg)\n# Note that in this dataset, year_treated == 10000 for never treated firms \n# and time_to_treatment == -1000\nmodel &lt;- feols(y ~ sunab(year_treated, year) | id + year, data = base_stagg)\n#summary(model)\niplot(model)\n\n\n\n\n\n\nCode\nimport pyfixest as pf\nbase_stagg = r.base_stagg\n\nfit_twfe_saturated = pf.event_study(\n    base_stagg,\n    yname=\"y\",\n    idname=\"id\",\n    tname=\"year\",\n    gname=\"year_treated\",\n    estimator=\"saturated\",\n)\n\nfit_twfe_saturated.aggregate()\nfit_twfe_saturated.iplot_aggregate()\n\n\n\n\n\n\nCode\n* Estimate the Sun and Abraham (2021) estimator in our example:\neventstudyinteract y id year year_treated, fe(id year) cohort(year_treated) control_cohort(10000) event_time(time_to_treatment)"
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#potential-pitfalls-13-ashenfelters-dip",
    "href": "lectures/lecture7/lecture7.html#potential-pitfalls-13-ashenfelters-dip",
    "title": "Empirical Economics",
    "section": "Potential Pitfalls (1/3): Ashenfelter’s Dip",
    "text": "Potential Pitfalls (1/3): Ashenfelter’s Dip\n\nA famous issue where the parallel trends assumption is violated in a specific way.\nThe Aschenfelter’s dip is when there is a notable drop in the outcome for the treatment group just before treatment.\n\n\n\n\n\n\n\n\nExample: Aschenfelter’s Dip\n\n\nIndividuals’ earnings often drop right before they enter a job training program (e.g., due to job loss).\nThis makes it look like the program had a huge effect, but it’s really just a recovery to a normal level."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#potential-pitfalls-23-policy-anticipation-spillovers",
    "href": "lectures/lecture7/lecture7.html#potential-pitfalls-23-policy-anticipation-spillovers",
    "title": "Empirical Economics",
    "section": "Potential Pitfalls (2/3): Policy Anticipation & Spillovers",
    "text": "Potential Pitfalls (2/3): Policy Anticipation & Spillovers\n\nAnticipation Effects: If people know a policy is coming, they may change their behavior before it’s officially implemented. This contaminates the “pre” period and violates parallel trends.\n\nExample: A firm hires fewer people in anticipation of a minimum wage hike.\n\nSpillover Effects: The treatment “spills over” and affects the control group.\n\nExample: A job fair in one city (treatment) draws workers from a neighboring city (control), affecting the control city’s labor market. Your control group is no longer a valid counterfactual."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#potential-pitfalls-33-other-limitations",
    "href": "lectures/lecture7/lecture7.html#potential-pitfalls-33-other-limitations",
    "title": "Empirical Economics",
    "section": "Potential Pitfalls (3/3): Other Limitations",
    "text": "Potential Pitfalls (3/3): Other Limitations\n\nFunctional Form: The basic DiD model assumes the treatment effect is a constant, additive shift.\nData Requirements: Requires panel data (tracking the same units over time) or repeated cross-sectional data.\nBad Control Group: The entire method relies on finding a credible control group that satisfies the parallel trends assumption. This is often the hardest part."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#what-did-we-do-today",
    "href": "lectures/lecture7/lecture7.html#what-did-we-do-today",
    "title": "Empirical Economics",
    "section": "What did we do today?",
    "text": "What did we do today?\n\nPotential Outcomes:\n\nWe have seen an introduction to potential outcomes notation. Potential outcomes define outcomes under the treatment and under the control at the individual level.\n\nThe ATE and the ATT:\n\nWe have seen two estimands – things we would like to estimate – of interest: the ATT and the ATE. We have discovered assumptions leading to identification of one of those in a cross-sectional context\n\nDifference-in-difference:\n\nWe switched to a two-period setting, which offers a more credible assumption of identifying the ATT: parallel trends.\nDiD provides a powerful and intuitive way to estimate causal effects by controlling for time-invariant unobserved differences (selection bias)."
  },
  {
    "objectID": "lectures/lecture7/lecture7.html#what-did-we-do-cont.",
    "href": "lectures/lecture7/lecture7.html#what-did-we-do-cont.",
    "title": "Empirical Economics",
    "section": "What did we do? (Cont.)",
    "text": "What did we do? (Cont.)\n\nDiD in Regression:\n\nWe then saw that the DID estimator can be implemented in a regression framework. A regression framework is useful for S.E.s and inclusion of covariates.\n\nTesting parallel trends:\n\nIn DiD, this assumption is everything. You must convince yourself and your audience that it is plausible.\n\nMulti-period and staggered adoption:\n\nWe have seen extensions of the DiD framework to a multi-period design closely resembling an event study, and we have focused on getting rid of “bad comparisons” in a staggered adoption setting."
  },
  {
    "objectID": "tutorialanswers.html",
    "href": "tutorialanswers.html",
    "title": "Tutorial Answers",
    "section": "",
    "text": "Below, the tutorial solutions can be found:\n\nTutorial 1\nTutorial 2\nTutorial 3"
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "",
    "text": "Code\n                  ESEMEE\n                \n                \n                  Period\n                  1\n                \n                \n                  Timeslot\n                  B/D (Tuesday Morning, Thursday Afternoon)\n                \n                \n                  ECTS\n                  5\n                \n                \n                  Course Type\n                  MSc Course\n                \n                \n                  Programme\n                  All Utrecht University School of Economics MSc Programs\n                \n                \n                  Coordinator/Lecturer\n                  Bas Machielsen\n                \n                \n                  Tutorial Teachers\n                  TBA\n                \n                \n                  \n                  TBA\n                \n                \n                  \n                  Bas Machielsen\n                \n                \n                  Language\n                  English\n                \n                \n                  Entry Requirements\n                  Knowledge of Statistics and Econometrics (Bachelor level); alternatively, attendance at the UU Summer School."
  },
  {
    "objectID": "syllabus/syllabus.html#course-overview",
    "href": "syllabus/syllabus.html#course-overview",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "",
    "text": "Code\n                  ESEMEE\n                \n                \n                  Period\n                  1\n                \n                \n                  Timeslot\n                  B/D (Tuesday Morning, Thursday Afternoon)\n                \n                \n                  ECTS\n                  5\n                \n                \n                  Course Type\n                  MSc Course\n                \n                \n                  Programme\n                  All Utrecht University School of Economics MSc Programs\n                \n                \n                  Coordinator/Lecturer\n                  Bas Machielsen\n                \n                \n                  Tutorial Teachers\n                  TBA\n                \n                \n                  \n                  TBA\n                \n                \n                  \n                  Bas Machielsen\n                \n                \n                  Language\n                  English\n                \n                \n                  Entry Requirements\n                  Knowledge of Statistics and Econometrics (Bachelor level); alternatively, attendance at the UU Summer School."
  },
  {
    "objectID": "syllabus/syllabus.html#course-planning",
    "href": "syllabus/syllabus.html#course-planning",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "Course Planning",
    "text": "Course Planning\n[Insert Table with Lecture Schedule].\nFor tutorials, please refer to MyTimeTable. Log-in with your student ID, click Add Timetable, search for “Empirical Economics”, click “Add timetables”, and select your corresponding tutorial group (e.g. USEMEE-2025-V-1-TUTORIAL-1-05). Optionally, you can also add the lectures and examinations."
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "Course Description",
    "text": "Course Description\nThis course aims to provide students with an understanding of basic econometric methods, and with knowledge how to apply them. These methods allows students to understand the modern empirical economic literature and to independently analyze data to answer a research question. The course starts with a recapitulation of the basic aspects of statistics and the mechanics behind the linear regression model, and proceeds to focus on frequently-used techniques and settings in the empirical economics literature, such as panel data, difference-in-differences, instrumental variables and binary outcome models. Although the course has a theoretical aspect, the main questions in this course are related to why we apply a particular technique and how to interpret the econometric outcome.\nThe course also focuses on research questions related to the different economic tracks of the USE Masters. To support this approach, in the tutorials, students will sometimes work with academic papers demonstrating the application of the techniques."
  },
  {
    "objectID": "syllabus/syllabus.html#learning-objectives",
    "href": "syllabus/syllabus.html#learning-objectives",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply econometrics to analyze research questions using empirical data, and justify the chosen approach.\nUse basic statistics and mathematics to understand and derive the basic properties of several econometric models.\nInterpret and critically evaluate quantitative empirical studies to provide policy recommendations."
  },
  {
    "objectID": "syllabus/syllabus.html#materials",
    "href": "syllabus/syllabus.html#materials",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "Materials",
    "text": "Materials\n\nRequired material: lecture slides & tutorial slides\n\nRecommended (but not required) material: Wooldridge, J.M. (2014), Introduction to Econometrics, EMEA Edition, ISBN 987 1 4080 9375 7.\nRecommended (but not required) material: Cunningham, S. (2024), Causal Inference, The Mixtape. Available here.\nRecommended (but not required) material: Adams, C.P. (2024), Learning Microeconometrics with R. Available here.\nRecommended (but not required) material: Huntington-Klein, N. (2024), The Effect: An Introduction to Research Design and Causality. Available here."
  },
  {
    "objectID": "syllabus/syllabus.html#format",
    "href": "syllabus/syllabus.html#format",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "Format",
    "text": "Format\n\nEach week, 1 lecture and 1 tutorial. These are two-hour sessions on campus. Whereas the lecture focuses on more theoretical aspects, in the tutorials, students will apply the techniques and concepts learned during the lecture. The tutorials also allow you to ask questions about the lecture material.\nChanging tutorial groups: each student is assigned to a tutorial group according to his/her master’s specialization. It is not recommended to switch tutorials. If this is necessary under special circumstances, please contact Studiepunt.Economie."
  },
  {
    "objectID": "syllabus/syllabus.html#examination",
    "href": "syllabus/syllabus.html#examination",
    "title": "Course Manual\nEmpirical Economics\n2025-2026",
    "section": "Examination",
    "text": "Examination\n\nThe course will have a mid-term exam (40%) and an end-term exam (60% of the grade). Both of these will tentatively be conducted on Brightspace, but in person. Utrecht University will provide Chromebooks. The exam is closed book, you can bring a pen/pencil and a calculator. Scrap paper will be provided.\nA mock mid-term and end-term exam will be provided to make you acquainted with the format.\nIn case of illness on the day of the exam, please contact the study advisor immediately: studyadvisor.use@uu.nl.\nRetake policy: There is one retake. If you have a \\(4 \\leq \\text{Grade} \\leq 5.5\\), you are eligible for the retake.\nInspection policy: there will be an inspection organized by the course coordinator following the mid-term and end-term exams. These will be announced on Brightspace.\nGrading: Your final grade will be your raw grade rounded to the nearest \\(1,1.5,2,2.5,3,\\dots, 10\\).\nLast course provision: A written exam (similar to the retake exam) takes place after period 4. Students who passed all courses (including thesis) except Empirical Economics can request the Board of Examiners for this last course provision."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#course-overview",
    "href": "lectures/lecture4/lecture4.html#course-overview",
    "title": "Empirical Economics",
    "section": "Course Overview",
    "text": "Course Overview\n\nStatistics and Probability - Basic Concepts\nStatistics and Probability - Hypothesis Testing\nThe Linear Regression Model\nTime Series Data\nPanel Data (FE) and Control Variables\nBinary Outcome Data\nPotential Outcomes and Difference-in-differences\nHands-on Econometrics in Practice"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#what-is-time-series-data",
    "href": "lectures/lecture4/lecture4.html#what-is-time-series-data",
    "title": "Empirical Economics",
    "section": "What is Time Series Data?",
    "text": "What is Time Series Data?\n\n\n\n\n\n\n\nDefinition: Time Series Data\n\n\nTime series data consists of observations of a variable or several variables over time.\n\n\n\n\n\nKey Feature: The data is ordered chronologically.\n\n\n\n\n\n\n\n\nExamples: Time Series Data\n\n\n\nGross Domestic Product (GDP) measured quarterly\nMonthly inflation rates\nDaily stock prices\nAnnual government budget deficits"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#characteristics-of-time-series-data",
    "href": "lectures/lecture4/lecture4.html#characteristics-of-time-series-data",
    "title": "Empirical Economics",
    "section": "Characteristics of Time Series Data",
    "text": "Characteristics of Time Series Data\n\nTemporal Ordering: Unlike cross-sectional data, the order of observations in time series data matters. The past can affect the future, but the future cannot affect the past.\nSerial Correlation (or Autocorrelation): Observations in a time series are often correlated with their own past values. For example, a high GDP in one quarter may suggest a high GDP in the next quarter.\nSeasonality: Many time series exhibit regular patterns at certain times of the year (e.g., retail sales are higher in the fourth quarter).\nTrends: A time series may have a long-term upward or downward movement."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#objectives-of-time-series-analysis",
    "href": "lectures/lecture4/lecture4.html#objectives-of-time-series-analysis",
    "title": "Empirical Economics",
    "section": "Objectives of Time Series Analysis",
    "text": "Objectives of Time Series Analysis\n\nThere are various objectives of time series econometrics:\n\n\nForecasting: Predicting future values of a variable is a primary objective. For instance, forecasting inflation is crucial for central banks.\nPolicy Analysis: Economists use time series models to assess the impact of policy changes. For example, what is the effect of an interest rate hike on unemployment?\nUnderstanding Dynamic Economic Relationships: Time series analysis helps us understand how economic variables interact over time."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#notation-and-basic-concepts",
    "href": "lectures/lecture4/lecture4.html#notation-and-basic-concepts",
    "title": "Empirical Economics",
    "section": "Notation and Basic Concepts",
    "text": "Notation and Basic Concepts\n\n\\(Y_t\\): The value of the variable Y at time period t.\n\\(Y_{t-1}\\): The value of Y in the previous period (the first lag).\n\\(Y_{t-s}\\): The value of Y s periods ago (the s-th lag).\n\\(\\Delta Y_t = Y_t - Y_{t-1}\\): The first difference of \\(Y\\), which represents the change in \\(Y\\) from the previous period to the current period."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#a-time-series-as-a-stochastic-process",
    "href": "lectures/lecture4/lecture4.html#a-time-series-as-a-stochastic-process",
    "title": "Empirical Economics",
    "section": "A Time Series as a Stochastic Process",
    "text": "A Time Series as a Stochastic Process\n\n\n\n\n\n\n\nDefinition: Time Series (Mathematical)\n\n\nA time series is a set of random variables indexed by time, denoted as \\(\\{Y_1, Y_2, \\dots, Y_T\\}\\) or simply \\(\\{Y_t\\}\\).\n\n\n\n\n\nStochastic vs. Deterministic: We treat a time series as a stochastic process because we don’t know the future values with certainty. We can only talk about their probability distributions.\nOne Realization: The actual data we have (e.g., GDP from 1950-2024) is just one of many possible paths the process could have taken.\nOur goal is to infer the properties of the underlying process from this single realization."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#basic-statistical-properties",
    "href": "lectures/lecture4/lecture4.html#basic-statistical-properties",
    "title": "Empirical Economics",
    "section": "Basic Statistical Properties",
    "text": "Basic Statistical Properties\n\nJust like any random variable, each point in a time series has statistical properties.\n\nMean: The expected value of the process at time t: \\(E(Y_t) = \\mu_t\\)\nVariance: The variance of the process at time t: \\(Var(Y_t) = E[(Y_t - \\mu_t)^2] = \\sigma_t^2\\)\n\nCrucially, in general, the mean and variance could be different at each point in time."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#autocovariance",
    "href": "lectures/lecture4/lecture4.html#autocovariance",
    "title": "Empirical Economics",
    "section": "Autocovariance",
    "text": "Autocovariance\n\nCovariance:\n\nRecall that covariance measures how two variables move together.\n\nAutocovariance\n\nAutocovariance is the covariance of a time series with its own past values. It measures the linear dependence between different points in the series.\n\n\n\n\n\n\n\n\n\nDefinition: Autocovariance\n\n\nThe \\(k\\)-th Order Autocovariance (\\(\\gamma_k\\)):\n\\[\n    \\gamma(t, t-k) = Cov(Y_t, Y_{t-k}) = E[(Y_t - \\mu_t)(Y_{t-k} - \\mu_{t-k})]\n  \\]\n\n\n\n\n\nThis measures the covariance between the series at time t and time t-k."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#other-topics-in-time-series-econometrics",
    "href": "lectures/lecture4/lecture4.html#other-topics-in-time-series-econometrics",
    "title": "Empirical Economics",
    "section": "Other Topics in Time Series Econometrics",
    "text": "Other Topics in Time Series Econometrics\n\nThis lecture has been an introduction. More advanced topics include:\n\nTesting for Stationarity: Formal tests (like the Dickey-Fuller test) to determine if a series has a unit root.\nCointegration: A method for analyzing the long-run relationships between non-stationary variables (the proper way to handle trending variables that are truly related).\nVolatility Modeling (ARCH/GARCH): Modeling the changing variance of a time series, which is crucial in finance."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#what-did-we-do",
    "href": "lectures/lecture4/lecture4.html#what-did-we-do",
    "title": "Empirical Economics",
    "section": "What did we do?",
    "text": "What did we do?\n\nUnivariate Time Series: We looked at ways to analyze a univariate time series, in particular, at:\n\nAutoregressive models: Explain a variable using its own past values.\nMoving Average models: Explain a variable using past random shocks.\n\nThe Linear Model: We focused on the possibility of using OLS on bivariate time series data.\n\nWe ran into the problem of spurious regression: a misleading relationship between two non-stationary time series."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#what-did-we-do-cont.",
    "href": "lectures/lecture4/lecture4.html#what-did-we-do-cont.",
    "title": "Empirical Economics",
    "section": "What did we do? (Cont.)",
    "text": "What did we do? (Cont.)\nDistributed Lag (DL) Models: - We focused on models that investigate the dynamic impact of an explanatory variable over time, allowing us to distinguish between short-run and long-run effects. These models make it more likely that the error term is white noise."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#stationarity-a-key-simplifying-assumption",
    "href": "lectures/lecture4/lecture4.html#stationarity-a-key-simplifying-assumption",
    "title": "Empirical Economics",
    "section": "Stationarity: A Key Simplifying Assumption",
    "text": "Stationarity: A Key Simplifying Assumption\n\nFor analysis to be tractable, we often assume the series is covariance stationary (or weakly stationary).\nThis means the three aforementioned statistical properties do not change over time. Three conditions must hold:\n\n\n\n\n\n\n\n\nDefinition: Stationarity\n\n\n\nConstant Mean: \\(E(Y_t) = \\mu\\) for all t. The series fluctuates around a constant level.\nConstant Variance: \\(Var(Y_t) = \\sigma^2\\) for all t. The volatility of the series is constant.\nConstant Autocovariance: \\(Cov(Y_t, Y_{t-k}) = \\gamma_k\\) for all t. The covariance between two points depends only on the lag \\(k\\) (how far apart they are), not on their position in time."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#autocorrelation-function-acf",
    "href": "lectures/lecture4/lecture4.html#autocorrelation-function-acf",
    "title": "Empirical Economics",
    "section": "Autocorrelation Function (ACF)",
    "text": "Autocorrelation Function (ACF)\n\nThe value of the autocovariance (\\(\\gamma_k\\)) depends on the units of the variable \\(Y\\). This makes it hard to compare across different series.\nAutocorrelation: standardize the autocovariance to get the autocorrelation, which is unit-free.\n\n\n\n\n\n\n\n\nDefinition: Autocorrelation\n\n\nThe \\(k\\)-th Order Autocorrelation (\\(\\rho_k\\)):\n\\[\n    \\rho_k = \\frac{ Cov(Y_t, Y_{t-k}) }{\\sqrt{Var(Y_t) Var(Y_{t-k})}}\n  \\]\nIf the series is stationary, this simplifies to:\n\\[\n    \\rho_k = \\frac{\\gamma_k}{\\gamma_0}\n  \\]\nwhere \\(\\gamma_0\\) is the variance, \\(Var(Y_t)\\)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-p7-interpreting-the-autocorrelation-function-acf",
    "href": "lectures/lecture4/lecture4.html#slide-p7-interpreting-the-autocorrelation-function-acf",
    "title": "Empirical Economics",
    "section": "Slide P7: Interpreting the Autocorrelation Function (ACF)",
    "text": "Slide P7: Interpreting the Autocorrelation Function (ACF)\n\nThe ACF, denoted by \\(\\rho_k\\), measures the “memory” or persistence of a time series.\n\nIt is a value between -1 and +1.\n\\(\\rho_1\\): The correlation between \\(Y_t\\) and \\(Y_{t-1}\\)(“today” and “yesterday”).\n\\(\\rho_k\\): The correlation between \\(Y_t\\) and \\(Y_{t-k}\\)\nA high \\(\\rho_k\\) suggests that a shock in one period will have a persistent effect k periods later.\n\nFor a stationary series, we expect \\(\\rho_k \\rightarrow 0\\) as \\(k\\) gets larger."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-7-visualizing-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#slide-7-visualizing-spurious-regression",
    "title": "Empirical Economics",
    "section": "Slide 7: Visualizing Spurious Regression",
    "text": "Slide 7: Visualizing Spurious Regression\nImagine two variables that follow a “random walk” (we’ll define this more formally later). They are independent by construction.\n(Insert a graph here showing two randomly generated time series that both trend upwards over time. The visual impression should be that they are related.)\nCaption: Two independent random walks can appear to be correlated simply because they are both trending."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-8-a-classic-example-yule-1926",
    "href": "lectures/lecture4/lecture4.html#slide-8-a-classic-example-yule-1926",
    "title": "Empirical Economics",
    "section": "Slide 8: A Classic Example: Yule (1926)",
    "text": "Slide 8: A Classic Example: Yule (1926)\n\nThe Study: Statistician George Udny Yule found a high correlation between the mortality rate in England and Wales and the proportion of marriages solemnized in the Church of England.\nThe Reality: There was no causal link. Both series were trending downwards over time due to unrelated social and economic factors."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-9-why-does-spurious-regression-happen",
    "href": "lectures/lecture4/lecture4.html#slide-9-why-does-spurious-regression-happen",
    "title": "Empirical Economics",
    "section": "Slide 9: Why Does Spurious Regression Happen?",
    "text": "Slide 9: Why Does Spurious Regression Happen?\n\nNon-stationarity: A time series is stationary if its statistical properties (mean, variance, autocorrelation) are constant over time.\nStochastic Trends: Many economic time series are non-stationary and exhibit a “random walk” like behavior, meaning they have a stochastic trend.\nThe Illusion of Correlation: When two independent non-stationary time series are regressed on each other, the shared tendency to trend can create a statistically significant relationship where none exists."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-10-consequences-of-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#slide-10-consequences-of-spurious-regression",
    "title": "Empirical Economics",
    "section": "Slide 10: Consequences of Spurious Regression",
    "text": "Slide 10: Consequences of Spurious Regression\nIf you run a regression of Yt on Xt where both are trending but unrelated:\n\nYou will likely find a high R-squared.\nThe t-statistics for the estimated coefficients will likely be significant.\nYou might conclude that X causes Y, but this conclusion would be meaningless and misleading."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-11-detecting-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#slide-11-detecting-spurious-regression",
    "title": "Empirical Economics",
    "section": "Slide 11: Detecting Spurious Regression",
    "text": "Slide 11: Detecting Spurious Regression\n\n\n\n\n\n\n\nDefinition: Breusch-Godfrey Test\n\n\nThe BG test assesses whether the error term, \\(\\epsilon_t\\), follows an AR(p) process. You, the researcher, choose the number of lags (p) to test for. The assumed model for the error is: \\[\\epsilon_t = \\rho_1 \\epsilon_{t-1} + \\rho_2 \\epsilon_{t-2} + \\dots + \\rho_p \\epsilon_{t-p} + v_t,\\]\nwhere \\(v_t\\) is a white noise term. \\(H_0: \\rho_1, \\dots, \\rho_p=0\\), and \\(H_A\\) is that at least one \\(\\rho\\) coefficient is non-zero: there is serial correlation up to some order \\(p\\).\n\n\n\n\n\nRejection of the BG Test: A key indicator of spurious regression is a rejection of the Breusch-Godfrey test.\nRule of Thumb: If the R-squared from the regression is greater than the DW statistic, you should be highly suspicious of a spurious relationship."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-12-how-to-avoid-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#slide-12-how-to-avoid-spurious-regression",
    "title": "Empirical Economics",
    "section": "Slide 12: How to Avoid Spurious Regression",
    "text": "Slide 12: How to Avoid Spurious Regression\n\nIf the static model \\(Y_t = \\alpha + \\beta X_t + \\epsilon_t\\) is so dangerous, what is the alternative?\nDifferencing the Data: If two variables are non-stationary, their first differences (ΔYt and ΔXt) may be stationary.\nDynamic models: We need models that explicitly account for dynamics—the idea that a change in X can affect Y both today and in the future."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-13-dynamic-discrete-time-processes",
    "href": "lectures/lecture4/lecture4.html#slide-13-dynamic-discrete-time-processes",
    "title": "Empirical Economics",
    "section": "Slide 13: Dynamic Discrete-Time Processes",
    "text": "Slide 13: Dynamic Discrete-Time Processes\n\nIntroducing Dynamics: So far, we have mostly considered static models. Dynamic models explicitly incorporate the passage of time.\nLags Matter: These models allow the current value of a variable to be influenced by past values of itself or other variables."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-14-the-autoregressive-ar-model-ar1",
    "href": "lectures/lecture4/lecture4.html#slide-14-the-autoregressive-ar-model-ar1",
    "title": "Empirical Economics",
    "section": "Slide 14: The Autoregressive (AR) Model: AR(1)",
    "text": "Slide 14: The Autoregressive (AR) Model: AR(1)\nThe simplest dynamic model is the first-order autoregressive, or AR(1), model:\nYt = α + ρYt-1 + ut\n\nYt: The value of Y at time t.\nYt-1: The value of Y in the previous period.\nρ (rho): The autoregressive coefficient, which measures the persistence of the series.\nut: A white noise error term (uncorrelated with past values)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-15-stationarity-of-an-ar1-process",
    "href": "lectures/lecture4/lecture4.html#slide-15-stationarity-of-an-ar1-process",
    "title": "Empirical Economics",
    "section": "Slide 15: Stationarity of an AR(1) Process",
    "text": "Slide 15: Stationarity of an AR(1) Process\nFor the AR(1) model Yt = α + ρYt-1 + ut:\n\nIf |ρ| &lt; 1, the series is stationary. Any shock (ut) will eventually die out.\nIf |ρ| = 1, the series is non-stationary and is called a random walk. Shocks have a permanent effect.\nIf |ρ| &gt; 1, the series is explosive (this is rare in economics)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-16-understanding-the-autoregressive-coefficient-ρ",
    "href": "lectures/lecture4/lecture4.html#slide-16-understanding-the-autoregressive-coefficient-ρ",
    "title": "Empirical Economics",
    "section": "Slide 16: Understanding the Autoregressive Coefficient (ρ)",
    "text": "Slide 16: Understanding the Autoregressive Coefficient (ρ)\n\nSpeed of Adjustment: A value of ρ close to 0 suggests that the effect of a past value dies out quickly.\nPersistence: A value of ρ close to 1 indicates that a shock to the system will persist for a long time. The series will return to its mean slowly."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-17-the-random-walk-model",
    "href": "lectures/lecture4/lecture4.html#slide-17-the-random-walk-model",
    "title": "Empirical Economics",
    "section": "Slide 17: The Random Walk Model",
    "text": "Slide 17: The Random Walk Model\nA special case of the AR(1) model where ρ = 1:\nYt = Yt-1 + ut\n\n“Best Guess”: The best forecast for tomorrow’s value is today’s value.\nPermanent Shocks: The impact of a shock ut is permanent; it is carried forward in all future periods.\nExamples: The prices of financial assets are often modeled as random walks."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-18-higher-order-arp-models",
    "href": "lectures/lecture4/lecture4.html#slide-18-higher-order-arp-models",
    "title": "Empirical Economics",
    "section": "Slide 18: Higher-Order AR(p) Models",
    "text": "Slide 18: Higher-Order AR(p) Models\nWe can include more lags of the dependent variable. An AR(p) model includes p lags:\nYt = α + ρ1Yt-1 + ρ2Yt-2 + … + ρpYt-p + ut\n\nChoosing p: The number of lags (p) can be determined using information criteria like the Akaike Information Criterion (AIC) or the Schwarz Information Criterion (SIC)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-19-the-lag-operator-l",
    "href": "lectures/lecture4/lecture4.html#slide-19-the-lag-operator-l",
    "title": "Empirical Economics",
    "section": "Slide 19: The Lag Operator (L)",
    "text": "Slide 19: The Lag Operator (L)\n\nA Convenient Notation: The lag operator, L, is a useful tool for writing and manipulating dynamic models.\nDefinition: LYt = Yt-1\nProperties:\n\nL²Yt = L(LYt) = LYt-1 = Yt-2\nLpYt = Yt-p\n\nAR(1) in Lag Notation: Yt = α + ρLYt + ut =&gt; (1 - ρL)Yt = α + ut"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-correlogram-acf-plot",
    "href": "lectures/lecture4/lecture4.html#the-correlogram-acf-plot",
    "title": "Empirical Economics",
    "section": "The Correlogram (ACF Plot)",
    "text": "The Correlogram (ACF Plot)\n\nA correlogram is a bar chart that plots the sample autocorrelations (\\(r_k\\)) for different lags (\\(k = 1, 2, 3, \\dots\\)).\n\nIt provides a visual summary of the persistence in a time series. We can see how quickly the correlations die out.\nCorrelograms usually include confidence bands. Autocorrelations that fall outside these bands are considered statistically different from zero.\n\n\n(Todo: Insert a sample correlogram plot here, showing bars for lags 1, 2, 3… and confidence bands)"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-20-introduction-to-moving-average-ma-processes",
    "href": "lectures/lecture4/lecture4.html#slide-20-introduction-to-moving-average-ma-processes",
    "title": "Empirical Economics",
    "section": "Slide 20: Introduction to Moving Average (MA) Processes",
    "text": "Slide 20: Introduction to Moving Average (MA) Processes\n\nA Different Kind of Dependence: While AR models link Yt to past values of Y, moving average (MA) models link Yt to past values of the error term, ut.\nInterpretation: A shock that occurred in the past can still affect the current value of the variable."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-21-the-moving-average-ma-model-ma1",
    "href": "lectures/lecture4/lecture4.html#slide-21-the-moving-average-ma-model-ma1",
    "title": "Empirical Economics",
    "section": "Slide 21: The Moving Average (MA) Model: MA(1)",
    "text": "Slide 21: The Moving Average (MA) Model: MA(1)\nThe first-order moving average, or MA(1), model is:\nYt = μ + ut + θut-1\n\nμ (mu): The mean of the series.\nut: The current shock.\nut-1: The shock from the previous period.\nθ (theta): The moving average coefficient."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-22-interpreting-the-ma1-coefficient-θ",
    "href": "lectures/lecture4/lecture4.html#slide-22-interpreting-the-ma1-coefficient-θ",
    "title": "Empirical Economics",
    "section": "Slide 22: Interpreting the MA(1) Coefficient (θ)",
    "text": "Slide 22: Interpreting the MA(1) Coefficient (θ)\n\nLimited Memory: Unlike an AR(1) model, a shock in an MA(1) model only persists for one period. Yt is affected by ut and ut-1, but Yt+1 will not be affected by ut.\nThe coefficient θ determines the extent to which a past shock affects the current observation."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-23-the-autoregressive-moving-average-arma-model",
    "href": "lectures/lecture4/lecture4.html#slide-23-the-autoregressive-moving-average-arma-model",
    "title": "Empirical Economics",
    "section": "Slide 23: The Autoregressive Moving Average (ARMA) Model",
    "text": "Slide 23: The Autoregressive Moving Average (ARMA) Model\n\nCombining AR and MA: We can combine the features of autoregressive and moving average models to create an ARMA(p,q) model.\nARMA(1,1): Yt = α + ρYt-1 + ut + θut-1\nFlexibility: ARMA models are very flexible and can capture a wide range of time series dynamics."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-24-introduction-to-distributed-lag-dl-models",
    "href": "lectures/lecture4/lecture4.html#slide-24-introduction-to-distributed-lag-dl-models",
    "title": "Empirical Economics",
    "section": "Slide 24: Introduction to Distributed Lag (DL) Models",
    "text": "Slide 24: Introduction to Distributed Lag (DL) Models\n\nDelayed Effects of Explanatory Variables: Distributed lag models are used when we want to model how a change in an explanatory variable (X) affects the dependent variable (Y) over time.\nExample: How does a change in government spending (X) affect GDP (Y) in the current quarter and in future quarters?"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-25-the-finite-distributed-lag-fdl-model",
    "href": "lectures/lecture4/lecture4.html#slide-25-the-finite-distributed-lag-fdl-model",
    "title": "Empirical Economics",
    "section": "Slide 25: The Finite Distributed Lag (FDL) Model",
    "text": "Slide 25: The Finite Distributed Lag (FDL) Model\nA finite distributed lag model of order q is:\nYt = α + β0Xt + β1Xt-1 + … + βqXt-q + ut\n\nβ0: The impact multiplier - the immediate effect of a one-unit change in Xt on Yt.\nβs (s &gt; 0): The dynamic multipliers - the effect of a one-unit change in Xt-s on Yt."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-26-interpreting-the-dl-coefficients",
    "href": "lectures/lecture4/lecture4.html#slide-26-interpreting-the-dl-coefficients",
    "title": "Empirical Economics",
    "section": "Slide 26: Interpreting the DL Coefficients",
    "text": "Slide 26: Interpreting the DL Coefficients\nYt = α + β0Xt + β1Xt-1 + … + βqXt-q + ut\n\nShort-Run Multiplier: The total effect after a certain number of periods (e.g., β0 + β1).\nLong-Run (or Total) Multiplier: The total effect of a sustained one-unit change in X on Y. It is the sum of all the β coefficients: Σ βs."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-27-the-koyck-geometric-distributed-lag-model",
    "href": "lectures/lecture4/lecture4.html#slide-27-the-koyck-geometric-distributed-lag-model",
    "title": "Empirical Economics",
    "section": "Slide 27: The Koyck (Geometric) Distributed Lag Model",
    "text": "Slide 27: The Koyck (Geometric) Distributed Lag Model\n\nThe Problem of Infinite Lags: Sometimes, the effect of X on Y might persist indefinitely, requiring an infinite number of lags. This is not practical to estimate.\nKoyck’s Assumption: The Koyck model assumes that the coefficients (βs) decline geometrically: βs = β0λ^s, where 0 &lt; λ &lt; 1.\nA Simpler Form: This assumption allows the infinite lag model to be transformed into a simpler model that can be estimated: Yt = α(1-λ) + β0Xt + λYt-1 + vt (This is a form of an ARDL model)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#slide-28-the-autoregressive-distributed-lag-ardl-model",
    "href": "lectures/lecture4/lecture4.html#slide-28-the-autoregressive-distributed-lag-ardl-model",
    "title": "Empirical Economics",
    "section": "Slide 28: The Autoregressive Distributed Lag (ARDL) Model",
    "text": "Slide 28: The Autoregressive Distributed Lag (ARDL) Model\n\nA General and Powerful Model: The ARDL model includes lags of both the dependent variable and the explanatory variable(s).\n\n\n\n\n\n\n\n\nDefinition: ARDL Model\n\n\nAn ARDL(p,q) model is defined as follows:\n\\[Y_t = \\alpha + \\sum(ρ_j \\cdot Y_{t-j}) \\text{ [from j=1 to p]} + \\sum(\\beta_j \\cdot X_{t-m}) \\text{ [from m=0 to q]} + u_t\\]\n\n\n\n\n\nIt includes both autoregressive and distributed lag terms.\nARDL models are very flexible and can be used to estimate both short-run and long-run effects.."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-white-noise-process",
    "href": "lectures/lecture4/lecture4.html#the-white-noise-process",
    "title": "Empirical Economics",
    "section": "The White Noise Process",
    "text": "The White Noise Process\n\nThe Simplest Time Series: A process called “white noise” is the building block for more complex models. We often denote it as \\(u_t\\)\n\n\n\n\n\n\n\n\nDefinition: White Noise Process\n\n\nA white noise is a time series such that:\n\n\\(E(u_t) = 0\\) (Zero mean)\n\\(Var(u_t) = \\sigma^2\\) (Constant variance)\n\\(Cov(u_t, u_s) = 0\\) for all \\(t\\neq s\\) (No autocorrelation)\n\n\n\n\n\n\nInterpretation: A white noise process is completely random and unpredictable. Its ACF will be zero for all lags \\(k &gt; 0\\). It is the ideal error term in a time series regression."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-problem-of-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#the-problem-of-spurious-regression",
    "title": "Empirical Economics",
    "section": "The Problem of Spurious Regression",
    "text": "The Problem of Spurious Regression\n\nIf \\(X\\) is stationary, e.g. \\(X \\sim AR(1)\\) with \\(|\\rho|&lt;1\\), this is a valid approach.\nHowever, in different situations, OLS may give a deceptive relationship:\n\nA spurious regression occurs when we find a statistically significant relationship between two time series variables that are actually unrelated.\n\nThis often happens when both variables have a strong trend (are non-stationary), and especially, when \\(X\\) is non-stationary.\n\nThe trend can be deterministic (predictable) or stochastic (random)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#visualizing-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#visualizing-spurious-regression",
    "title": "Empirical Economics",
    "section": "Visualizing Spurious Regression",
    "text": "Visualizing Spurious Regression\nImagine two variables that follow a “random walk” (we’ll define this more formally later). They are independent by construction.\n(Insert a graph here showing two randomly generated time series that both trend upwards over time. The visual impression should be that they are related.)\nCaption: Two independent random walks can appear to be correlated simply because they are both trending."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#interpretation-of-the-autocorrelation-function-acf",
    "href": "lectures/lecture4/lecture4.html#interpretation-of-the-autocorrelation-function-acf",
    "title": "Empirical Economics",
    "section": "Interpretation of the Autocorrelation Function (ACF)",
    "text": "Interpretation of the Autocorrelation Function (ACF)\n\nThe ACF, denoted by \\(\\rho_k\\), measures the “memory” or persistence of a time series.\n\nIt is a value between -1 and +1.\n\\(\\rho_1\\): The correlation between \\(Y_t\\) and \\(Y_{t-1}\\)(“today” and “yesterday”).\n\\(\\rho_k\\): The correlation between \\(Y_t\\) and \\(Y_{t-k}\\)\nA high \\(\\rho_k\\) suggests that a shock in one period will have a persistent effect k periods later.\n\nFor a stationary series, we expect \\(\\rho_k \\rightarrow 0\\) as \\(k\\) gets larger."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-autoregressive-ar-model-ar1",
    "href": "lectures/lecture4/lecture4.html#the-autoregressive-ar-model-ar1",
    "title": "Empirical Economics",
    "section": "The Autoregressive (AR) Model: AR(1)",
    "text": "The Autoregressive (AR) Model: AR(1)\n\nThe simplest dynamic model is the first-order autoregressive, or AR(1), model:\n\n\\[\n  Y_t = α + \\rho Y_{t-1} + u_t\n\\]\n\n\\(Y_t\\): The value of Y at time t.\n\\(Y_{t-1}\\): The value of Y in the previous period.\n\\(\\rho\\): The autoregressive coefficient, which measures the persistence of the series.\n\\(u_t\\): A white noise error term (uncorrelated with past values)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-moving-average-model",
    "href": "lectures/lecture4/lecture4.html#the-moving-average-model",
    "title": "Empirical Economics",
    "section": "The Moving Average Model",
    "text": "The Moving Average Model\n\nIn the MA model, the current value is a function of the current and past random shocks.\n\\[\n  Y_t = \\mu + u_t + \\theta u_{t-1}\n\\]\n\\(\\mu\\): The mean of the series.\n\\(\\theta\\): The moving average coefficient.\nThe MA(1) process has a finite memory. A shock \\(u_{t-1}\\) affects \\(Y_{t-1}\\) and \\(Y_t\\), but has no direct effect on \\(Y_{t+1}\\)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#interpreting-the-ma1-coefficient-theta",
    "href": "lectures/lecture4/lecture4.html#interpreting-the-ma1-coefficient-theta",
    "title": "Empirical Economics",
    "section": "Interpreting the MA(1) Coefficient (\\(\\theta\\))",
    "text": "Interpreting the MA(1) Coefficient (\\(\\theta\\))\n\nLimited Memory: Unlike an AR(1) model, a shock in an MA(1) model only persists for one period. Yt is affected by ut and ut-1, but Yt+1 will not be affected by ut.\nThe coefficient θ determines the extent to which a past shock affects the current observation."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-autoregressive-moving-average-arma-model",
    "href": "lectures/lecture4/lecture4.html#the-autoregressive-moving-average-arma-model",
    "title": "Empirical Economics",
    "section": "The Autoregressive Moving Average (ARMA) Model",
    "text": "The Autoregressive Moving Average (ARMA) Model\n\nCombining AR and MA: We can combine the features of autoregressive and moving average models to create an ARMA(p,q) model.\nFor example, an ARMA(1,1) Model: \\(Y_t = \\alpha + \\rho Y_{t-1} + u_t + \\theta u_{t=1}\\)\nARMA models are very flexible and can capture a wide range of time series dynamics."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#why-model-a-single-series",
    "href": "lectures/lecture4/lecture4.html#why-model-a-single-series",
    "title": "Empirical Economics",
    "section": "Why Model a Single Series?",
    "text": "Why Model a Single Series?\n\nBefore we ask how X affects Y, we must first understand the behavior of Y itself.\nUnivariate models describe the dynamic properties of a single time series using its own past.\nMain uses:\n\nUnderstanding Persistence: How long do shocks to the economy last?\nForecasting: Using the past of a series to predict its future."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#stationarity-of-an-ar1-process",
    "href": "lectures/lecture4/lecture4.html#stationarity-of-an-ar1-process",
    "title": "Empirical Economics",
    "section": "Stationarity of an AR(1) Process",
    "text": "Stationarity of an AR(1) Process\n\nFor the AR(1) model \\(Y_t = \\alpha + \\rho Y_{t-1} + u_t\\):\n\nIf \\(|\\rho| &lt; 1\\):, the series is stationary. Any shock (\\(u_t\\)) will eventually die out.\nIf \\(|\\rho|=1\\), the series is non-stationary and is called a random walk. Shocks have a permanent effect.\nIf \\(|\\rho|&gt;1\\), the series is explosive (this is rare in economics).\n\nSpeed of Adjustment: A value of \\(\\rho\\) close to 0 suggests that the effect of a past value dies out quickly.\nPersistence: A value of \\(\\rho\\) close to 1 indicates that a shock to the system will persist for a long time. The series will return to its mean slowly."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-random-walk-model",
    "href": "lectures/lecture4/lecture4.html#the-random-walk-model",
    "title": "Empirical Economics",
    "section": "The Random Walk Model",
    "text": "The Random Walk Model\n\nA special case of the AR(1) model where \\(\\rho=1\\):\n\\[\n  Y_t = Y_{t-1} + u_t\n\\]\nThe best forecast for tomorrow’s value is today’s value.\nPermanent Shocks: The impact of a shock \\(u_t\\) is permanent; it is carried forward in all future periods.\n\nExamples: The prices of financial assets are often modeled as random walks."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#higher-order-arp-models",
    "href": "lectures/lecture4/lecture4.html#higher-order-arp-models",
    "title": "Empirical Economics",
    "section": "Higher-Order AR(p) Models",
    "text": "Higher-Order AR(p) Models\n\nWe can include more lags of the dependent variable. An \\(AR(p)\\) model includes \\(p\\) lags:\n\n\\[\nY_t = \\alpha + \\rho_1 Y_{t-1} + \\rho_2 Y_{t-2} + \\dots + \\rho_p Y_{t-p} + u_t\n\\]\n\nChoosing p: The number of lags (p) can be determined using information criteria like the Akaike Information Criterion (AIC) or the Schwarz Information Criterion (SIC)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#linear-model-in-time-series",
    "href": "lectures/lecture4/lecture4.html#linear-model-in-time-series",
    "title": "Empirical Economics",
    "section": "Linear Model in Time Series",
    "text": "Linear Model in Time Series\n\nThe model form is identical to its cross-sectional counterpart:\n\\[\n  Y_t = \\alpha + \\beta X_t + \\epsilon_t\n\\]\nThis model posits a contemporaneous relationship between X and Y.\nQuestion: Can we estimate this with OLS and trust the results?\nFor OLS to be a good estimator, the standard assumptions must hold. The most problematic one for time series data is the assumption of :\n\\[\n  Cov(X_t, \\epsilon_t) = 0\n\\]"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#spurious-regression-is-non-stationarity",
    "href": "lectures/lecture4/lecture4.html#spurious-regression-is-non-stationarity",
    "title": "Empirical Economics",
    "section": "Spurious Regression is Non-Stationarity",
    "text": "Spurious Regression is Non-Stationarity\n\nLet’s look at the most common non-stationary process: the random walk.\n\\[\n  X_t = X_{t-1} + u_t\n\\]\n\n\n\n\n\n\n\n\nTheorem: Variance of a Random Walk\n\n\nSuppose \\(X_0=0\\). Let us find the variance of \\(X_t\\).\nWe know that \\(X_1 = u_i\\). \\(X_2 = X_1 + u_2 = u_1 + u_2\\), etc.\nHence, \\(X_t = \\sum u_i\\), and \\(Var(X_t) = Var(u_1 + u_2 + \\dots + u_t)\\).\nSince the \\(u_i\\) are uncorrelated (white noise), the variance of the sum is the sum of the variances: \\[\n    \\begin{align}\n    Var(X_t) &= \\sigma^2_u + \\dots + \\sigma^2_u \\text{ ( t times)} \\\\\n            &= t \\times \\sigma^2_u\n    \\end{align}\n  \\]\nThe variance depends on time t and grows without bound. This is a clear violation of stationarity."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#the-spurious-regression-setup",
    "href": "lectures/lecture4/lecture4.html#the-spurious-regression-setup",
    "title": "Empirical Economics",
    "section": "The Spurious Regression Setup",
    "text": "The Spurious Regression Setup\n\nImagine two independent and unrelated random walks, \\(Yt = Yt-1 + u_t\\) and \\(Xt = Xt-1 + v_t\\), where \\(u_t\\) and \\(v_t\\) are independent white noise processes.\nBy construction, the true \\(\\beta\\) in a regression of \\(Y_t\\) on \\(X_t\\) is zero.\nWhen we run \\(Y_t= \\alpha + \\beta X_t + u_t\\), we know \\(Var(Y_t)=Var(X_t)=t\\times \\sigma^2_u\\).\n\nBoth variables have variances that are trending upwards.\nBut OLS is a “variance-minimizing” estimator. It finds a \\(\\beta\\) that makes the variance of the residuals (Yt - βXt) as small as possible."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#why-ols-finds-a-false-relationship",
    "href": "lectures/lecture4/lecture4.html#why-ols-finds-a-false-relationship",
    "title": "Empirical Economics",
    "section": "Why OLS Finds a False Relationship",
    "text": "Why OLS Finds a False Relationship\nWhen we run Yt = α + βXt + εt:\nWe know Var(Yt) = t * σu² and Var(Xt) = t * σv².\n\nBoth variables have variances that are trending upwards.\n\nOLS is a \"variance-minimizing\" estimator. It finds a β that makes the variance of the residuals (Yt - βXt) as small as possible.\n\nBecause Yt and Xt share a common deterministic trend in their variance, OLS is \"fooled\" into finding a non-zero β that appears to link their movements."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#failure-of-reliable-estimation",
    "href": "lectures/lecture4/lecture4.html#failure-of-reliable-estimation",
    "title": "Empirical Economics",
    "section": "Failure of Reliable Estimation",
    "text": "Failure of Reliable Estimation\n\nImagine two independent and unrelated random walks, \\(Yt = Yt-1 + u_t\\) and \\(Xt = Xt-1 + v_t\\), where \\(u_t\\) and \\(v_t\\) are independent white noise processes.\nBy construction, the true \\(\\beta\\) in a regression of \\(Y_t\\) on \\(X_t\\) is zero.\nWhen we run \\(Y_t= \\alpha + \\beta X_t + u_t\\), we know \\(Var(Y_t)=Var(X_t)=t\\times \\sigma^2_u\\).\n\nTodo: fix this argument\n\nWe also know that the OLS coefficient is \\(\\hat{\\beta}=\\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\\).\nWhen \\(T \\rightarrow \\infty\\) , the denominator term tends to infinity.\nSince this is in the denominator, we might get unusually large \\(\\beta\\) coefficients and \\(t\\)-statistics."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#visualization-ar1-process",
    "href": "lectures/lecture4/lecture4.html#visualization-ar1-process",
    "title": "Empirical Economics",
    "section": "Visualization AR(1) Process",
    "text": "Visualization AR(1) Process"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#visualization-random-walk",
    "href": "lectures/lecture4/lecture4.html#visualization-random-walk",
    "title": "Empirical Economics",
    "section": "Visualization Random Walk",
    "text": "Visualization Random Walk"
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#estimation-and-interpretation-of-ma-coefficients-theta",
    "href": "lectures/lecture4/lecture4.html#estimation-and-interpretation-of-ma-coefficients-theta",
    "title": "Empirical Economics",
    "section": "Estimation and Interpretation of MA Coefficients (\\(\\theta\\))",
    "text": "Estimation and Interpretation of MA Coefficients (\\(\\theta\\))\n\nAs you may have noticed, there are no independent variables in an MA model.\n\nUsually, what we do is assume \\(u_0=0\\), take a guess for \\(\\mu\\) and \\(\\theta\\), and solve for \\(u_t\\): \\(u_t = Y_t - \\mu - \\theta u_{t-1}\\)\nIf the true shocks came from a Normal distribution \\(N(0, \\sigma^2_u)\\), how likely was it to get this specific sequence of inferred shocks? The joint probability of observing this sequence is called the Likelihood.\nWe try to find the most likely \\(\\mu\\) and \\(\\theta\\) that could have generated the actual data.1\n\nUnlike an AR(1) model, a shock in an MA(1) model only persists for one period. Yt is affected by ut and ut-1, but Yt+1 will not be affected by ut.\nThe coefficient \\(\\theta\\) determines the extent to which a past shock affects the current observation.\n\nMore about this in Lecture 6."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#consequences-of-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#consequences-of-spurious-regression",
    "title": "Empirical Economics",
    "section": "Consequences of Spurious Regression",
    "text": "Consequences of Spurious Regression\n\nHence, if you run a regression of \\(Y_t\\) on \\(X_t\\), where both are trending but unrelated:\n\nYou will likely find a high R-squared.\nThe t-statistics for the estimated coefficients will likely be significant.\nYou might conclude that X causes Y, but this conclusion would be meaningless and misleading."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#avoiding-spurious-regression",
    "href": "lectures/lecture4/lecture4.html#avoiding-spurious-regression",
    "title": "Empirical Economics",
    "section": "Avoiding Spurious Regression",
    "text": "Avoiding Spurious Regression\n\nIf the static model \\(Y_t = \\alpha + \\beta X_t + \\epsilon_t\\) is so dangerous, what is the alternative?\nDifferencing the Data: If two variables are non-stationary, their first differences (\\(\\Delta Y_t\\) and \\(\\Delta X_t\\)) may be stationary.\nDynamic models: We need models that explicitly account for dynamics—the idea that a change in X can affect Y both today and in the future.\n\nThis makes it more likely that the error term is white noise."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#distributed-lag-dl-models",
    "href": "lectures/lecture4/lecture4.html#distributed-lag-dl-models",
    "title": "Empirical Economics",
    "section": "Distributed Lag (DL) Models",
    "text": "Distributed Lag (DL) Models\n\nDistributed lag models are used when we want to model how a change in an explanatory variable (X) affects the dependent variable (Y) over time, and we want to focus on potential delayed effects.\n\nExample: How does a change in government spending (X) affect GDP (Y) in the current quarter and in future quarters?\n\n\n\n\n\n\n\n\n\nDefinition: Distributed Lag Model\n\n\nA finite distributed lag model of order \\(q\\) is:\n\\[Y_t = \\alpha + \\beta_0 X_t + \\beta_1 X_{t-1} + \\dots + \\beta_q X_{t-q} + u_t\\] where:\n\\(\\beta_0\\): The impact multiplier - the immediate effect of a one-unit change in \\(X_t\\) on \\(Y_t\\).\n\\(\\beta_s (s &gt; 0)\\): The dynamic multipliers - the effect of a one-unit change in \\(X_{t-s}\\) on \\(Y_t\\)."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#interpreting-the-dl-coefficients",
    "href": "lectures/lecture4/lecture4.html#interpreting-the-dl-coefficients",
    "title": "Empirical Economics",
    "section": "Interpreting the DL Coefficients",
    "text": "Interpreting the DL Coefficients\n\\[Y_t = \\alpha + \\beta_0 X_t + \\beta_1 X_{t-1} + \\dots + \\beta_q X_{t-q} + u_t\\]\n\nShort-Run Multiplier: The total effect after a certain number of periods (e.g., β0 + β1).\nLong-Run (or Total) Multiplier: The total effect of a sustained one-unit change in X on Y. It is the sum of all the β coefficients: Σ βs."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#interpretation-ardl-coefficients",
    "href": "lectures/lecture4/lecture4.html#interpretation-ardl-coefficients",
    "title": "Empirical Economics",
    "section": "Interpretation ARDL Coefficients",
    "text": "Interpretation ARDL Coefficients\nTo do."
  },
  {
    "objectID": "lectures/lecture4/lecture4.html#breusch-godfrey-test",
    "href": "lectures/lecture4/lecture4.html#breusch-godfrey-test",
    "title": "Empirical Economics",
    "section": "Breusch-Godfrey Test",
    "text": "Breusch-Godfrey Test\n\n\n\n\n\n\n\nBreusch-Godfrey Test: Procedure\n\n\nThe BG test is performed in three simple steps:\n\nRun the Main Regression: First, estimate your original model using OLS, regardless of what it is. Obtain the series of residuals,\nRun an Auxiliary Regression. Run a new regression where the residual \\(e_t\\) is the dependent variable. The independent variables are:\n\nAll the original regressors from the main model.\nThe lagged residuals up to order \\(p\\): e_{t-1}, , e_{t-p}\n\nObtain the R-squared: Calculate the R-squared from this auxiliary regression.\n\n\n\nThe Breusch-Godfrey test uses a Lagrange Multiplier (LM) statistic: \\(LM = T \\times R^2_{aux}\\), where \\(T\\) is the number of observations used in the auxiliary regression and \\(R^2_{aux}\\) is the \\(R^2\\) from the auxiliary regression in Step 2.\nUnder the null hypothesis of no serial correlation, the LM statistic follows a \\(\\chi^2\\) distribution with p degrees of freedom (where p is the number of lagged residuals included).\nIf \\(LM &gt; \\chi^2_{crit}\\), you reject the null hypothesis. You conclude there is significant evidence of serial correlation."
  }
]