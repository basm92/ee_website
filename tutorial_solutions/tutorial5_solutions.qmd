---
title: "Solutions Tutorial 5"
format: html
---

```{r setup}
#| echo: false

library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```


## The Omitted Variable Bias Problem in Pooled OLS

The Pooled OLS model ignores the panel structure of the data and can suffer from omitted variable bias if unobserved individual characteristics are correlated with the regressors. The Fixed Effects model is designed to solve this problem.

1.  Load the `wagepan` dataset (and declare it as a panel dataset using `nr` as the individual identifier and `year` as the time identifier).
2.  Estimate a **Pooled OLS** model to predict `lwage` using `exper` (experience), `expersq` (experience squared), and `union`.
3.  Estimate a **Fixed Effects ("within")** model using the same variables.
4.  Present the results of both models side-by-side in a single table.
5.  **Interpret the difference:** Pay close attention to the coefficient for the `union` variable. Why is the estimated return to union membership different in the FE model compared to the Pooled OLS model? What does this suggest about how union members may differ from non-union members in ways not captured by the other variables?

The coefficient for `union` decreases in magnitude by about 50% when estimated by fixed effects (0.167 vs. 0.083). When comparing differences in wages _for the same person_ when a person switches union membership. This comparison is arguably more sensible than comparing union members and non-union members, since these might differ in ways that are not captured by the `experience` control variables. For example, `union` workers might work disproportionally in higher paid professions, such as aviation. 

:::{.panel-tabset}

### Python

```{python}
#| echo: true
import pandas as pd
import pyfixest as pf
# 1.  Load the `wagepan` dataset (and declare it as a panel dataset using `nr` as the individual identifier and `year` as the time identifier).
# Declaring as a panel not necessary in Python
wagepan = pd.read_stata("../tutorials/datafiles/WAGEPAN.DTA")

# 2.  Estimate a **Pooled OLS** model to predict `lwage` using `exper` (experience), `expersq` (experience squared), and `union`.
pooled_ols = pf.feols("lwage ~ exper + expersq + union", data = wagepan)

# 3.  Estimate a **Fixed Effects ("within")** model using the same variables.
fixed_ef = pf.feols("lwage ~ exper + expersq + union | nr", data = wagepan)

# 4. Present the results of both models side-by-side in a single table.
pf.etable([pooled_ols, fixed_ef])
```


### R

```{r}
#| echo: true
# 1.  Load the `wagepan` dataset (and declare it as a panel dataset using `nr` as the individual identifier and `year` as the time identifier).
library(haven); library(fixest)
# Declaring as a panel not necessary in Python
wagepan <- read_dta("../tutorials/datafiles/WAGEPAN.DTA")

# 2.  Estimate a **Pooled OLS** model to predict `lwage` using `exper` (experience), `expersq` (experience squared), and `union`.
pooled_ols <- feols(lwage ~ exper + expersq + union, data = wagepan)

# 3.  Estimate a **Fixed Effects ("within")** model using the same variables.
fixed_ef <- feols(lwage ~ exper + expersq + union | nr, data = wagepan)

# 4. Present the results of both models side-by-side in a single table.
etable(list(pooled_ols, fixed_ef))
```

### Stata

```{stata}
#| echo: true
#| eval: false

* 1. Load the dataset
use "../tutorials/datafiles/WAGEPAN.DTA", clear

* 2. Estimate a Pooled OLS model
regress lwage exper expersq union

* Store the results for table
estimates store pooled_ols

* 3. Estimate a Fixed Effects ("within") model
xtset nr  // declare panel structure with nr as ID variable
xtreg lwage exper expersq union, fe

* Store the results for table
estimates store fixed_ef

* 4. Present results side-by-side
estimates table pooled_ols fixed_ef, b(%9.4f) se(%9.4f) stats(N r2 r2_a)

* Alternative with more formatting options (requires estout package)
* ssc install estout, replace  // install if needed
esttab pooled_ols fixed_ef, cells(b(star fmt(4)) se(par fmt(4))) ///
       stats(N r2 r2_a, fmt(%9.0g %9.3f %9.3f)) ///
       mtitle("Pooled OLS" "Fixed Effects") ///
       star(* 0.10 ** 0.05 *** 0.01)
```

:::


## Choosing Your Model 

While the Fixed Effects (FE) model is robust to correlation between unobserved effects and regressors, the Random Effects (RE) model is more efficient *if* its key assumption (that this correlation is zero) holds. The Hausman test is the standard tool for making this choice.

1.  Using the `wagepan` panel data, estimate a **Random Effects** model where `lwage` is a function of the time-varying predictors `married` and `union`, and the (mostly) time-invariant predictor `educ`.^[In Python, the random effects model can be estimated using the `linearmodels` library. In R, you can use the `plm` library. In Stata, `xtreg y x1 x2, re`.]
2.  Estimate the corresponding **Fixed Effects** model with the same variables.^[Tip: use the same library so that a Hausman test can be easily done.]
3.  Perform a **Hausman test** to formally compare the two models.
4.  **Conclude:** Based on the p-value of the test, which model should you choose? Clearly state the null hypothesis of the Hausman test and explain what your result implies about the relationship between the unobserved individual effects ($\alpha_i$) and the explanatory variables in your model.

The null hypothesis is consistency of both the RE and FE estimators, and the alternative hypothesis is that only the FE estimator is consistent. The test rejects the null hypothesis, implying there is a correlation between the individual $\alpha_i$'s and the explanatory variables in our model. Hence, the RE model suffers from omitted variable bias and its estimates cannot be trusted. 

:::{.panel-tabset}

### Python

```{python}
#| echo: true
#| collapse: true

import pandas as pd
import pyfixest as pf
import numpy as np
from scipy.stats import chi2
from linearmodels import RandomEffects
from linearmodels import PanelOLS
from linearmodels.panel import compare

# Load example dataset (or use your own panel data)
wagepan = pd.read_stata("../tutorials/datafiles/WAGEPAN.DTA")
wagepan_panel = wagepan.set_index(['nr', 'year'])

# 1. Fit Random Effects Model
formula = 'lwage ~ married + union + educ'
re_model = RandomEffects.from_formula(formula, wagepan_panel).fit()
print(re_model)

# 2. Fit Fixed Effects Model (This time also through linearmodels)
formula = 'lwage ~ married + union + EntityEffects' # educ cannot be estimated
fe_model = PanelOLS.from_formula(formula, wagepan_panel).fit()
print(fe_model)

# 3.  Perform a **Hausman test** to formally compare the two models.
# Manual Hausman Test
# Extract coefficients and covariance matrices
beta_fe, beta_re = fe_model.params, re_model.params[:2]
cov_fe, cov_re = fe_model.cov, re_model.cov.iloc[:2, :2]

# Compute difference and covariance
diff = beta_fe - beta_re
cov_diff = cov_fe - cov_re  # FE covariance is "more efficient"

# Hausman test statistic
hausman_stat = diff.T @ np.linalg.inv(cov_diff) @ diff
df = len(diff)  # degrees of freedom
p_value = 1 - chi2.cdf(hausman_stat, df)

print(f"Hausman Statistic: {hausman_stat:.4f}")
print(f"P-value: {p_value:.4f}")
```


### R

```{r}
#| echo: true
#| collapse: true
#| warning: false
#| message: false
#1.  Using the `wagepan` panel data, estimate a **Random Effects** model where `lwage` is a function of the time-varying predictors `married` and `union`, and the (mostly) time-invariant predictor `educ`.
library(haven); library(fixest); library(plm)
wagepan <- read_dta('../tutorials/datafiles/WAGEPAN.DTA')
formula <- 'lwage ~ married + union + educ'
random <- plm(formula, data=wagepan, index=c("nr", "year"), model="random")  #random model
summary(random)
#2.  Estimate the corresponding **Fixed Effects** model with the same variables.
fixed <- plm(formula, data=wagepan, index=c("nr", "year"), model="within")  #fixed model
summary(fixed)
#3.  Perform a **Hausman test** to formally compare the two models.
phtest(fixed,random) #Hausman test
```



### Stata

```{stata}
#| echo: true
#| eval: false
use "../tutorials/datafiles/WAGEPAN.DTA", clear
xtset nr  // declare panel structure with nr as ID variable

* 1. Random Effects
xtreg lwage exper expersq union, re

* Store the results for test
estimates store re

* 2. Estimate a Fixed Effects ("within") model
xtreg lwage exper expersq union, fe

* Store results
estimates store fe

* 3. Hausman test
hausman fe, re
```

:::


## Visualizing Unobserved Heterogeneity

The "fixed effects" ($\alpha_i$) estimated in an FE model represent the time-invariant, unobserved characteristics of each individual (e.g., innate ability, motivation, family background) that affect their wage. Visualizing the distribution of these effects can reveal the extent of this heterogeneity.

1.  Estimate a Fixed Effects model for `lwage` using `exper`, `expersq`, and `union` as predictors.
2.  Extract the individual fixed effects from your estimated model.
3.  Create a **histogram or density plot** to visualize the distribution of these fixed effects.
4.  **Interpret the plot:** What does the shape and spread of the distribution tell you about the sample? Does it appear that unobserved, time-constant individual differences are a significant factor in explaining wage variation?

The histogram shows a wide spread of fixed effects, suggesting substantial unobserved time-constant heterogeneity across individuals. Since these effects capture persistent individual differences not explained by the predictors, their significant variation implies that individual-specific factors play an important role in wage determination. The roughly bell-shaped but somewhat skewed distribution indicates most individuals cluster around the mean effect, with some outliers showing particularly high or low baseline wages.

:::{.panel-tabset}

### Python

```{python}
#| echo: true
import pandas as pd
import pyfixest as pf
import matplotlib.pyplot as plt

# Load data
df = pd.read_stata("../tutorials/datafiles/WAGEPAN.DTA")

# Estimate FE model
fit = pf.feols("lwage ~ exper + expersq + union | nr", data=df)

# Extract fixed effects
fe = pd.DataFrame(fit.fixef())

# Plot distribution
plt.hist(fe, bins=30, density=True, alpha=0.7)
plt.xlabel("Individual Fixed Effects")
plt.title("Distribution of Fixed Effects")
plt.show()
```

### R

```{r}
#| echo: true
library(fixest); library(ggplot2); library(haven)

# Load data (adjust path as needed)
df <- haven::read_dta("../tutorials/datafiles/WAGEPAN.DTA")

# Estimate FE model
fit <- feols(lwage ~ exper + expersq + union | nr, data = df)

# Extract fixed effects
fe <- fixef(fit)$nr

# Plot distribution
ggplot(data.frame(fe), aes(x = fe)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.7) +
  labs(x = "Individual Fixed Effects", 
       title = "Distribution of Fixed Effects") +
  theme_minimal()

```

### Stata

```{stata}
#| eval: false
#| echo: true
* Load data (adjust path as needed)
use "../tutorials/datafiles/WAGEPAN.DTA", clear

* Estimate FE model
xtset nr
xtreg lwage exper expersq union, fe

* Store fixed effects
predict fe, u

* Plot distribution
histogram fe, bin(30) frequency normal ///
    title("Distribution of Fixed Effects") ///
    xtitle("Individual Fixed Effects")
```

:::

## Time-Invariant Variables

A critical drawback of the Fixed Effects model is that it cannot estimate the coefficients of time-invariant variables. The "within" transformation, which subtracts the individual-specific mean, wipes out any variable that is constant over time for an individual.

1.  Attempt to estimate a **Fixed Effects** model for `lwage` that includes both time-varying (`union`, `married`) and time-invariant (`black`, `hisp`) predictors.
2.  Observe the model output. What happens to the coefficients for the `black` and `hisp` variables?

The coefficients cannot be estimated, and are dropped from the model. 

3.  Now, estimate a **Random Effects** model using the exact same set of predictors.
4.  **Explain the difference:** Why can the RE model provide estimates for `black` and `hisp` while the FE model cannot? Relate your answer directly to the underlying mathematical transformation of each estimator.

The RE model is able to estimate the effects of time-invariant variables, while the FE model is not, due to collinearity of the time-invariant variables with the fixed effects. 


:::{.panel-tabset}

### Python

```{python}
#| echo: true
import pandas as pd
import pyfixest as pf
import matplotlib.pyplot as plt
from linearmodels import RandomEffects

# Load data
df = pd.read_stata("../tutorials/datafiles/WAGEPAN.DTA")

# Estimate FE model
fe = pf.feols("lwage ~ union + married + black + hisp | nr", data=df)
fe.summary()

# Estimate RE model
wagepan_panel = df.set_index(['nr', 'year'])
re_model = RandomEffects.from_formula("lwage ~ union + married + black + hisp", wagepan_panel).fit()
print(re_model)
```

### R

```{r}
#| echo: true
#| collapse: true
wagepan <- read_dta('../tutorials/datafiles/WAGEPAN.DTA')
formula <- 'lwage ~ union + married + black + hisp'
#1 Estimate FE.
fixed <- plm(formula, data=wagepan, index=c("nr", "year"), model="within")  #fixed model
summary(fixed)

# 2. Random Effects
random <- plm(formula, data=wagepan, index=c("nr", "year"), model="random")  #random model
summary(random)
```

### Stata

```{stata}
#| eval: false
#| echo: true
* Load data
use "../tutorials/datafiles/WAGEPAN.DTA", clear

* Declare panel structure
xtset nr year

* 1. Fixed Effects estimation
xtreg lwage union married black hisp, fe
estimates store fixed

* 2. Random Effects estimation
xtreg lwage union married black hisp, re
estimates store random

* (Optional) Display both results for comparison
estimates table fixed random, b se stats(N r2)
```

:::

## Efficiency of FD vs FE Estimators

- The FD error:
  
  The FD estimator works by differencing the equation to eliminate the fixed effect $c_i$:
  
  $\Delta y_{it} = \beta \Delta x_{it} + \Delta \epsilon_{it}$
  
  where $\Delta y_{it} = y_{it} - y_{i,t-1}$ and so on.
  
  Let's focus on the transformed error term, $\Delta \epsilon_{it}$:
  
  $\Delta \epsilon_{it} = \epsilon_{it} - \epsilon_{i,t-1}$
  
  Now, we substitute the definition of the random walk process for $\epsilon_{it}$:
  
  $\Delta \epsilon_{it} = (\epsilon_{i,t-1} + \nu_{it}) - \epsilon_{i,t-1}$
  
  This simplifies to:
  
  $\Delta \epsilon_{it} = \nu_{it}$
  
  Since $\nu_{it}$ is defined as white noise, it is, by definition, **not serially correlated**. Therefore, the error term in the first-differenced model satisfies the classical OLS assumption of no serial correlation.

- The FE error:
  
  Let's analyze the transformed error term, $\tilde{\epsilon}_{it} = \epsilon_{it} - \bar{\epsilon}_i$, where $\bar{\epsilon}_i = \frac{1}{T} \sum_{s=1}^{T} \epsilon_{is}$.
  
  The de-meaned error at time $t$ is:
  $\tilde{\epsilon}_{it} = \epsilon_{it} - \frac{1}{T}(\epsilon_{i1} + \epsilon_{i2} + ... + \epsilon_{iT})$
  
  And the de-meaned error at time $t-1$ is:
  $\tilde{\epsilon}_{i,t-1} = \epsilon_{i,t-1} - \frac{1}{T}(\epsilon_{i1} + \epsilon_{i2} + ... + \epsilon_{iT})$
  
  To check for serial correlation, we need to determine if the covariance between $\tilde{\epsilon}_{it}$ and $\tilde{\epsilon}_{i,t-1}$ is zero.
  
  $Cov(\tilde{\epsilon}_{it}, \tilde{\epsilon}_{i,t-1}) = E[(\epsilon_{it} - \bar{\epsilon}_i)(\epsilon_{i,t-1} - \bar{\epsilon}_i)]$
  
  Because $\epsilon_{it}$ is a random walk, it is strongly serially correlated with its own past values. For instance, $\epsilon_{it} = \epsilon_{i,t-1} + \nu_{it}$, and $\epsilon_{i,t-1} = \epsilon_{i,t-2} + \nu_{i,t-1}$, and so on. This means that $\epsilon_{it}$ is a sum of all past white noise shocks.
  
  When we de-mean this series, the transformed error $\tilde{\epsilon}_{it}$ becomes a complex combination of all the original error terms for that individual. Both $\tilde{\epsilon}_{it}$ and $\tilde{\epsilon}_{i,t-1}$ share the same subtracted mean, $\bar{\epsilon}_i$, and their non-mean components, $\epsilon_{it}$ and $\epsilon_{i,t-1}$, are themselves highly correlated. The de-meaning process does not remove the underlying correlation structure of a random walk.
  
  As a result, the covariance between the de-meaned errors, $Cov(\tilde{\epsilon}_{it}, \tilde{\epsilon}_{i,t-1})$, will be non-zero. The transformed error term in the FE model **remains serially correlated**.
  
## Interpreting the Hausman Test Statistic

In mathematical terms, the null hypothesis ($H_0$) of the Hausman test is that the unobserved individual-specific effects, $\alpha_i$, are **not correlated** with the explanatory variables, $x_{it}$.

This can be formally stated as:

$H_0: Cov(\alpha_i, x_{it}) = 0$

Under this null hypothesis, both the Fixed Effects and Random Effects estimators are consistent, but the Random Effects estimator is more efficient. The alternative hypothesis ($H_A$) is that a correlation does exist, meaning $Cov(\alpha_i, x_{it}) \neq 0$. Under the alternative, only the Fixed Effects estimator remains consistent.

A large, statistically significant test statistic from a Hausman test leads to the rejection of the null hypothesis. This directly implies that the Random Effects estimates are inconsistent due to the following chain of logic:

1.  **Consistency of the Estimators:**
    *   **Fixed Effects (FE):** The FE estimator is consistent whether the individual effects ($\alpha_i$) are correlated with the regressors or not. It achieves consistency by eliminating the $\alpha_i$ term from the equation entirely (through de-meaning or first-differencing).
    *   **Random Effects (RE):** The RE estimator's consistency *critically depends* on the assumption that the individual effects ($\alpha_i$) are uncorrelated with the regressors. This is the very condition being tested by the null hypothesis. If this assumption is violated, the RE model suffers from omitted variable bias because it fails to properly account for the influence of the correlated $\alpha_i$ term. This bias makes the RE estimator inconsistent.

2.  **What the Hausman Test Measures:** The test statistic is constructed based on the difference between the coefficient vectors from the two models: $(\hat{\beta}_{FE} - \hat{\beta}_{RE})$.
    *   If the null hypothesis is **true** ($Cov(\alpha_i, x_{it}) = 0$), both estimators are consistent. This means as the sample size grows, both $\hat{\beta}_{FE}$ and $\hat{\beta}_{RE}$ should converge to the same true parameter values. Any difference between them should be small and statistically insignificant (due to random sampling variation).
    *   If the null hypothesis is **false** ($Cov(\alpha_i, x_{it}) \neq 0$), the two estimators will converge to different values. The FE estimator will converge to the true $\beta$, but the RE estimator will converge to a different, biased value. Therefore, the difference $(\hat{\beta}_{FE} - \hat{\beta}_{RE})$ will be systematic and statistically significant.

3.  **Conclusion from a Significant Result:** A large and statistically significant Hausman test statistic indicates that the difference between the FE and RE coefficients is too large to be attributed to chance. Since we know the FE estimator is consistent under either scenario, the systematic difference must be due to the failure of the RE estimator. By rejecting the null hypothesis, you are concluding that the core assumption required for the RE model's consistency is violated. Therefore, the Random Effects estimates are inconsistent.

## Random Effects and the Role of $\theta$

1. As the variance of the individual effect approaches zero ($\sigma^2_\alpha \to 0$)

*   **Behavior of $\theta$:** As $\sigma^2_\alpha \to 0$, the term $T \sigma^2_\alpha$ also goes to zero. The formula for $\theta$ becomes:
    $\theta \to 1 - \sqrt{\frac{\sigma^2_\epsilon}{(0 + \sigma^2_\epsilon)}} = 1 - \sqrt{1} = 0$
    The value of $\theta$ approaches **zero**.

*   **Model Simplification:** The Random Effects transformation is $y_{it} - \theta \bar{y}_i$. When $\theta=0$, this becomes $y_{it} - 0 \cdot \bar{y}_i = y_{it}$. The data is left untransformed. Therefore, the Random Effects model simplifies to the **Pooled OLS** model. This is logical, as a zero variance for the individual effect implies there are no unique individual effects to control for.

2. As the number of time periods becomes very large ($T \to \infty$)

*   **Behavior of $\theta$:** As $T \to \infty$, the denominator $(T \sigma^2_\alpha + \sigma^2_\epsilon)$ also approaches infinity. This causes the fraction inside the square root to approach zero:
    $\frac{\sigma^2_\epsilon}{(T \sigma^2_\alpha + \sigma^2_\epsilon)} \to 0$
    The formula for $\theta$ thus becomes:
    $\theta \to 1 - \sqrt{0} = 1$
    The value of $\theta$ approaches **one**.

*   **Model Equivalence:** When $\theta=1$, the Random Effects transformation $y_{it} - \theta \bar{y}_i$ becomes $y_{it} - \bar{y}_i$. This is the "de-meaning" or "within" transformation. Consequently, the Random Effects estimator becomes equivalent to the **Fixed Effects (FE) estimator**.
