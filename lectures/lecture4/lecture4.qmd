---
title: "Empirical Economics"
subtitle: "Lecture 4: The Linear Model and Time Series"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 4 - Linear Model and Time Series'
resources:
  - demo.pdf
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Introduction

## Course Overview

- Statistics and Probability - Basic Concepts
- Statistics and Probability - Hypothesis Testing
- The Linear Regression Model
- Time Series Data
- Panel Data (FE) and Control Variables
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Hands-on Econometrics in Practice


## What is Time Series Data?

:::{.callout-note title="Definition: Time Series Data"}
Time series data consists of observations of a variable or several variables over time.
:::

- **Key Feature:** The data is ordered chronologically.

:::{.callout-tip title="Examples: Time Series Data"}
- Gross Domestic Product (GDP) measured quarterly
- Monthly inflation rates
- Daily stock prices
- Annual government budget deficits
:::

---

## Characteristics of Time Series Data

- **Temporal Ordering:** Unlike cross-sectional data, the order of observations in time series data matters. The past can affect the future, but the future cannot affect the past.
- **Serial Correlation (or Autocorrelation):** Observations in a time series are often correlated with their own past values. For example, a high GDP in one quarter may suggest a high GDP in the next quarter.
- **Seasonality:** Many time series exhibit regular patterns at certain times of the year (e.g., retail sales are higher in the fourth quarter).
- **Trends:** A time series may have a long-term upward or downward movement.

## Objectives of Time Series Analysis

- There are various objectives of time series econometrics:

1. **Forecasting:** Predicting future values of a variable is a primary objective. For instance, forecasting inflation is crucial for central banks.
2. **Policy Analysis:** Economists use time series models to assess the impact of policy changes. For example, what is the effect of an interest rate hike on unemployment?
3. **Understanding Dynamic Economic Relationships:** Time series analysis helps us understand how economic variables interact over time.

## Notation and Basic Concepts

- $Y_t$:  The value of the variable Y at time period t.
- $Y_{t-1}$: The value of Y in the previous period (the first lag).
- $Y_{t-s}$: The value of Y s periods ago (the s-th lag).
- $\Delta Y_t = Y_t - Y_{t-1}$: The first difference of $Y$, which represents the change in $Y$ from the previous period to the current period.

# Introduction to Dynamic Processes

## A Time Series as a Stochastic Process

:::{.callout-note title="Definition: Time Series (Mathematical)"}
A time series is a set of random variables indexed by time, denoted as $\{Y_1, Y_2, \dots, Y_T\}$ or simply $\{Y_t\}$. 
:::

- Stochastic vs. Deterministic: We treat a time series as a stochastic process because we don't know the future values with certainty. We can only talk about their probability distributions.

- One Realization: The actual data we have (e.g., GDP from 1950-2024) is just one of many possible paths the process could have taken. 
- Our goal is to infer the properties of the underlying process from this single realization.

## Basic Statistical Properties

- Just like any random variable, each point in a time series has statistical properties.

  - Mean: The expected value of the process at time t: $E(Y_t) = \mu_t$
  - Variance: The variance of the process at time t: $Var(Y_t) = E[(Y_t - \mu_t)^2] = \sigma_t^2$

- Crucially, in general, the mean and variance could be different at each point in time.


## Autocovariance 

- Covariance: 
  - Recall that covariance measures how two variables move together.

- Autocovariance
  - Autocovariance is the covariance of a time series with its own past values. It measures the linear dependence between different points in the series.
  
:::{.callout-note title="Definition: Autocovariance"}
The $k$-th Order Autocovariance ($\gamma_k$):

  $$
    \gamma(t, t-k) = Cov(Y_t, Y_{t-k}) = E[(Y_t - \mu_t)(Y_{t-k} - \mu_{t-k})]
  $$
:::

- This measures the covariance between the series at time t and time t-k.

## Stationarity: A Key Simplifying Assumption

- For analysis to be tractable, we often assume the series is covariance stationary (or weakly stationary).

- This means the three aforementioned statistical properties do not change over time. Three conditions must hold:

:::{.callout-note title="Definition: Stationarity"}
- Constant Mean: $E(Y_t) = \mu$ for all t. The series fluctuates around a constant level.
- Constant Variance: $Var(Y_t) = \sigma^2$ for all t. The volatility of the series is constant.
- Constant Autocovariance: $Cov(Y_t, Y_{t-k}) = \gamma_k$ for all t. The covariance between two points depends only on the lag $k$ (how far apart they are), not on their position in time.
:::

## Autocorrelation Function (ACF)

- The value of the autocovariance ($\gamma_k$) depends on the units of the variable $Y$. This makes it hard to compare across different series.

- Autocorrelation: standardize the autocovariance to get the autocorrelation, which is unit-free.

:::{.callout-note title="Definition: Autocorrelation"}

The $k$-th Order Autocorrelation ($\rho_k$): 

  $$
    \rho_k = \frac{ Cov(Y_t, Y_{t-k}) }{\sqrt{Var(Y_t) Var(Y_{t-k})}}
  $$

If the series is stationary, this simplifies to:

  $$
    \rho_k = \frac{\gamma_k}{\gamma_0}
  $$
  
where $\gamma_0$ is the variance, $Var(Y_t)$. 
  
:::

## Interpretation of the Autocorrelation Function (ACF)

- The ACF, denoted by $\rho_k$, measures the "memory" or persistence of a time series.
  - It is a value between -1 and +1.
  - $\rho_1$: The correlation between $Y_t$ and $Y_{t-1}$("today" and "yesterday").
  - $\rho_k$: The correlation between $Y_t$ and $Y_{t-k}$
  - A high $\rho_k$ suggests that a shock in one period will have a persistent effect k periods later.

- For a stationary series, we expect $\rho_k \rightarrow 0$ as $k$ gets larger.

## The Correlogram (ACF Plot)

- A correlogram is a bar chart that plots the sample autocorrelations ($r_k$) for different lags ($k = 1, 2, 3, \dots$).
  - It provides a visual summary of the persistence in a time series. We can see how quickly the correlations die out.
  - Correlograms usually include confidence bands. Autocorrelations that fall outside these bands are considered statistically different from zero.

(Todo: Insert a sample correlogram plot here, showing bars for lags 1, 2, 3... and confidence bands)

# Modeling a Single Time Series

## Why Model a Single Series?

- Before we ask how X affects Y, we must first understand the behavior of Y itself.
- **Univariate models** describe the dynamic properties of a single time series using its own past.
- Main uses:
    1.  **Understanding Persistence:** How long do shocks to the economy last?
    2.  **Forecasting:** Using the past of a series to predict its future.

## The White Noise Process

- The Simplest Time Series: A process called "white noise" is the building block for more complex models. We often denote it as $u_t$

:::{.callout-note title="Definition: White Noise Process"}
A white noise is a time series such that:

- $E(u_t) = 0$ (Zero mean)
- $Var(u_t) = \sigma^2$ (Constant variance)
- $Cov(u_t, u_s) = 0$ for all $t\neq s$ (No autocorrelation)
:::

- Interpretation: A white noise process is completely random and unpredictable. Its ACF will be zero for all lags $k > 0$. It is the ideal error term in a time series regression.

## The Autoregressive (AR) Model: AR(1)

- The simplest dynamic model is the first-order autoregressive, or AR(1), model:

$$
  Y_t = α + \rho Y_{t-1} + u_t
$$

- $Y_t$: The value of Y at time t.
- $Y_{t-1}$: The value of Y in the previous period.
- $\rho$: The autoregressive coefficient, which measures the persistence of the series.
- $u_t$: A white noise error term (uncorrelated with past values).

## Visualization AR(1) Process

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import polars as pl
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

T = 100      # Number of time periods to simulate
rho = 0.9    # Autoregressive parameter (must be < 1 for stationarity)
c = 2        # A constant/intercept term
sigma = 1.0  # Standard deviation of the white noise error term

# Generate all the random shocks at once using NumPy
np.random.seed(42) # for reproducibility
epsilon = np.random.normal(loc=0, scale=sigma, size=T)

# Initialize a Python list to store our time series values
y_values = [0] * T

# Set the initial value. A good choice is the unconditional mean of the process
# E[Y] = c / (1 - rho), to start the series near its long-run equilibrium.
y_values[0] = c / (1 - rho)

# Loop to generate the AR(1) series recursively
for t in range(1, T):
    # Apply the AR(1) formula
    y_values[t] = c + rho * y_values[t-1] + epsilon[t]

# Create a DataFrame with a time index and the simulated series
df = pl.DataFrame({
    'time': range(T),
    'ar_process': y_values
})

# Set the style and size for the plot
sns.set_theme(style="whitegrid")
plt.figure(figsize=(14, 7))

# Create the line plot
lineplot = sns.lineplot(
    data=df,
    x='time',
    y='ar_process',
    label=f'AR(1) Process (ρ={rho})'
)

# Add a horizontal line for the theoretical mean of the process
mean_value = c / (1 - rho)
plt.axhline(
    mean_value,
    color='red',
    linestyle='--',
    label=f'Theoretical Mean: {mean_value:.2f}'
)

# Add titles and labels for clarity
plt.title('Simulation of a Stationary AR(1) Process', fontsize=16)
plt.xlabel('Time Period', fontsize=12)
plt.ylabel('Value', fontsize=12)
plt.legend()

# Show the plot
plt.show()
```


## Stationarity of an AR(1) Process

- For the AR(1) model $Y_t = \alpha + \rho Y_{t-1} + u_t$:
  - If $|\rho| < 1$:, the series is **stationary**. Any shock ($u_t$) will eventually die out.
  - If $|\rho|=1$, the series is **non-stationary** and is called a **random walk**. Shocks have a permanent effect.
  - If $|\rho|>1$, the series is **explosive** (this is rare in economics).

- **Speed of Adjustment:** A value of $\rho$ close to 0 suggests that the effect of a past value dies out quickly.
- **Persistence:** A value of $\rho$ close to 1 indicates that a shock to the system will persist for a long time. The series will return to its mean slowly.

## The Random Walk Model

- A special case of the AR(1) model where $\rho=1$:

  $$
    Y_t = Y_{t-1} + u_t
  $$

- The best forecast for tomorrow's value is today's value.
- **Permanent Shocks:** The impact of a shock $u_t$ is permanent; it is carried forward in all future periods.
  - **Examples:** The prices of financial assets are often modeled as random walks.

## Visualization Random Walk

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import polars as pl
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

T = 500      # Number of time periods to simulate
rho = 1    # Autoregressive parameter (must be < 1 for stationarity)
c = 2        # A constant/intercept term
sigma = 1.0  # Standard deviation of the white noise error term

# Generate all the random shocks at once using NumPy
np.random.seed(42) # for reproducibility
epsilon = np.random.normal(loc=0, scale=sigma, size=T)

# Initialize a Python list to store our time series values
y_values = [0] * T

# Set the initial value. A good choice is the unconditional mean of the process
# E[Y] = c / (1 - rho), to start the series near its long-run equilibrium.
y_values[0] = c / (1 - rho)

# Loop to generate the AR(1) series recursively
for t in range(1, T):
    # Apply the AR(1) formula
    y_values[t] = c + rho * y_values[t-1] + epsilon[t]

# Create a DataFrame with a time index and the simulated series
df = pl.DataFrame({
    'time': range(T),
    'ar_process': y_values
})

# Set the style and size for the plot
sns.set_theme(style="whitegrid")
plt.figure(figsize=(14, 7))

# Create the line plot
lineplot = sns.lineplot(
    data=df,
    x='time',
    y='ar_process',
    label='Random Walk')'
)

# Add a horizontal line for the theoretical mean of the process
mean_value = c / (1 - rho)
plt.axhline(
    mean_value,
    color='red',
    linestyle='--',
    label=f'Theoretical Mean: {mean_value:.2f}'
)

# Add titles and labels for clarity
plt.title('Simulation of a Random Walk', fontsize=16)
plt.xlabel('Time Period', fontsize=12)
plt.ylabel('Value', fontsize=12)
plt.legend()

# Show the plot
plt.show()
```

## Higher-Order AR(p) Models

- We can include more lags of the dependent variable. An $AR(p)$ model includes $p$ lags:

$$
Y_t = \alpha + \rho_1 Y_{t-1} + \rho_2 Y_{t-2} + \dots + \rho_p Y_{t-p} + u_t
$$

- **Choosing p:** The number of lags (p) can be determined using information criteria like the Akaike Information Criterion (AIC) or the Schwarz Information Criterion (SIC).

## The Moving Average Model

- In the MA model, the current value is a function of the current and past **random shocks**.

  $$
    Y_t = \mu + u_t + \theta u_{t-1}
  $$

- $\mu$: The mean of the series.
- $\theta$: The moving average coefficient.
- The MA(1) process has a **finite memory**. A shock $u_{t-1}$ affects $Y_{t-1}$ and $Y_t$, but has no direct effect on $Y_{t+1}$. 

## Interpreting the MA(1) Coefficient ($\theta$)

*   **Limited Memory:** Unlike an AR(1) model, a shock in an MA(1) model only persists for one period. Yt is affected by ut and ut-1, but Yt+1 will not be affected by ut.
*   **The coefficient θ** determines the extent to which a past shock affects the current observation.

## The Autoregressive Moving Average (ARMA) Model

- **Combining AR and MA:** We can combine the features of autoregressive and moving average models to create an ARMA(p,q) model.
- For example, an ARMA(1,1) Model: $Y_t = \alpha + \rho Y_{t-1} + u_t + \theta u_{t=1}$
- ARMA models are very flexible and can capture a wide range of time series dynamics.


# Relationship Between Time Series

## Linear Model in Time Series

- The model form is identical to its cross-sectional counterpart:

  $$
    Y_t = \alpha + \beta X_t + \epsilon_t
  $$

- This model posits a contemporaneous relationship between X and Y.
- Question: Can we estimate this with OLS and trust the results?
- For OLS to be a good estimator, the standard assumptions must hold. The most problematic one for time series data is the assumption of :
  
  $$
    Cov(X_t, \epsilon_t) = 0
  $$
    
## The Problem of Spurious Regression

- If $X$ is stationary, e.g. $X \sim AR(1)$ with $|\rho|<1$, this is a valid approach. 

- However, in different situations, OLS may give a **deceptive relationship:**
  - A spurious regression occurs when we find a statistically significant relationship between two time series variables that are actually unrelated.
- This often happens when both variables have a strong trend (are non-stationary), and especially, when $X$ is non-stationary. 
  - The trend can be deterministic (predictable) or stochastic (random).

## Spurious Regression is Non-Stationarity

- Let's look at the most common non-stationary process: the random walk.

  $$
    X_t = X_{t-1} + u_t
  $$

:::{.callout-note title="Theorem: Variance of a Random Walk"}
Suppose $X_0=0$. Let us find the variance of $X_t$. We know that $X_1 = u_i$. $X_2 = X_1 + u_2 = u_1 + u_2$, etc. Hence, $X_t = \sum u_i$.  

Hence $Var(X_t) = Var(u_1 + u_2 + \dots + u_t)$. 

Since the $u_i$ are uncorrelated (white noise), the variance of the sum is the sum of the variances:
  $$
    \begin{align}
    Var(X_t) &= \sigma^2_u + \dots + \sigma^2_u \text{ ( t times)} \\
            &= t \times \sigma^2_u
    \end{align}
  $$

The variance depends on time t and grows without bound. This is a clear violation of stationarity.

:::

## Failure of Reliable Estimation

- Imagine two independent and unrelated random walks, $Yt = Yt-1 + u_t$ and $Xt = Xt-1 + v_t$, where $u_t$ and $v_t$ are independent white noise processes.

- By construction, the true $\beta$ in a regression of $Y_t$ on $X_t$ is zero.

- When we run $Y_t= \alpha + \beta X_t + u_t$,  we know $Var(Y_t)=Var(X_t)=t\times \sigma^2_u$. 

- We also know that the OLS coefficient is $\hat{\beta}=\frac{\text{Cov}(X,Y)}{\text{Var}(X)}$. 
 - When $T \rightarrow \infty$ , this term also tends to infinity. 
 - Since this is in the denominator, we might get unusually large $\beta$ coefficients and $t$-statistics. 


## Slide 10: Consequences of Spurious Regression

If you run a regression of Yt on Xt where both are trending but unrelated:

*   You will likely find a **high R-squared**.
*   The **t-statistics** for the estimated coefficients will likely be **significant**.
*   You might conclude that X causes Y, but this conclusion would be **meaningless and misleading**.

## Slide 11: Detecting Spurious Regression

*   **A Low Durbin-Watson Statistic:** A key indicator of spurious regression is a very low Durbin-Watson (DW) statistic, often close to zero.
*   **Rule of Thumb:** If the R-squared from the regression is greater than the DW statistic, you should be highly suspicious of a spurious relationship.

## Slide 12: How to Avoid Spurious Regression

- If the static model $Y_t = \alpha + \beta X_t + \epsilon_t$ is so dangerous, what is the alternative?

- **Differencing the Data:** If two variables are non-stationary, their first differences (ΔYt and ΔXt) may be stationary.
- **Dynamic models**: We need models that explicitly account for dynamics—the idea that a change in X can affect Y both today and in the future.

## Slide 24: Introduction to Distributed Lag (DL) Models

*   **Delayed Effects of Explanatory Variables:** Distributed lag models are used when we want to model how a change in an explanatory variable (X) affects the dependent variable (Y) over time.
*   **Example:** How does a change in government spending (X) affect GDP (Y) in the current quarter and in future quarters?


## Slide 25: The Finite Distributed Lag (FDL) Model

A finite distributed lag model of order q is:

**Yt = α + β0Xt + β1Xt-1 + ... + βqXt-q + ut**

*   **β0:** The **impact multiplier** - the immediate effect of a one-unit change in Xt on Yt.
*   **βs (s > 0):** The **dynamic multipliers** - the effect of a one-unit change in Xt-s on Yt.

## Slide 26: Interpreting the DL Coefficients

**Yt = α + β0Xt + β1Xt-1 + ... + βqXt-q + ut**

*   **Short-Run Multiplier:** The total effect after a certain number of periods (e.g., β0 + β1).
*   **Long-Run (or Total) Multiplier:** The total effect of a sustained one-unit change in X on Y. It is the sum of all the β coefficients: **Σ βs**.

## Slide 27: The Koyck (Geometric) Distributed Lag Model

*   **The Problem of Infinite Lags:** Sometimes, the effect of X on Y might persist indefinitely, requiring an infinite number of lags. This is not practical to estimate.
*   **Koyck's Assumption:** The Koyck model assumes that the coefficients (βs) decline geometrically: βs = β0λ^s, where 0 < λ < 1.
*   **A Simpler Form:** This assumption allows the infinite lag model to be transformed into a simpler model that can be estimated:
    **Yt = α(1-λ) + β0Xt + λYt-1 + vt** (This is a form of an ARDL model).

## Slide 28: The Autoregressive Distributed Lag (ARDL) Model

*   **A General and Powerful Model:** The ARDL model includes lags of both the dependent variable and the explanatory variable(s).
*   **ARDL(p,q):**
    **Yt = α + Σ(ρi * Yt-i) [from i=1 to p] + Σ(βj * Xt-j) [from j=0 to q] + ut**
*   **Advantages:** ARDL models are very flexible and can be used to estimate both short-run and long-run effects.


## Other Topics in Time Series Econometrics

This lecture has been an introduction. More advanced topics include:

*   **Testing for Stationarity:** Formal tests (like the Dickey-Fuller test) to determine if a series has a unit root.
*   **Cointegration:** A method for analyzing the long-run relationships between non-stationary variables (the proper way to handle trending variables that are truly related).
*   **Volatility Modeling (ARCH/GARCH):** Modeling the changing variance of a time series, which is crucial in finance.


# Summary

## What did we do?


*   **Spurious Regression:** 
  - A misleading relationship between two non-stationary time series. Beware of high R-squared and low Durbin-Watson statistics.
*   **Autoregressive (AR) Models:**
  - Explain a variable using its own past values. The concept of persistence is key.
*   **Moving Average (MA) Models:**
  - Explain a variable using past random shocks.
*   **Distributed Lag (DL) Models:**
  - Model the dynamic impact of an explanatory variable over time, allowing us to distinguish between short-run and long-run effects.



## What did we do? (Cont.)

# The End