---
title: "Empirical Economics"
subtitle: "Lecture 4: Panel Data I"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 4 - Panel Data I'
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Outline

## Course Overview

- Linear Model I
- Linear Model II
- Time Series and Prediction
- Panel Data I
- Panel Data II
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Instrumental Variables


## This lecture

- Motivation
- Advantage of Panel Data
- Main features of panel models
- The individual specific effect
- Strict exogeneity
- Between and within variation
- Least Squares Dummy Variable estimator
- Within estimator (or fixed effect estimator)

- **Material:** Wooldridge Chapter 14: 14.1, 14.2, 14.3

# Motivation

## Stock and Watson (1988)

- Imagine a government is considering increasing the tax on alcohol to reduce traffic-related deaths. A crucial question for economists is: how can we estimate the causal impact of such a policy?

- To answer this, we need to overcome a fundamental challenge: states with higher beer taxes might also have other characteristics that influence traffic fatalities, such as better roads or stricter law enforcement. 
  - A simple comparison across states at a single point in time might therefore be misleading.

- This is where panel data becomes an invaluable tool.

## Stock and Watson (1988) (Cont.)

- We need two requirements on our data:

  - Requirement 1: Information Before and After the Policy Change (Time Dimension) We need to observe traffic fatality rates in states before and after any changes in beer taxes. This allows us to see what happens within a state when the policy changes.

  - Requirement 2: Information from Multiple Entities (Cross-Sectional Dimension) We need data from the same states over consecutive periods. This allows us to compare the changes in fatality rates in states that changed their beer tax to those that did not.
  
```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 10
# Load necessary packages
library(AER)
library(plm)
library(dplyr)
library(ggplot2)
library(patchwork)

# Load the dataset
data("Fatalities")

Fatalities <- Fatalities |>
  mutate(fatal_rate = fatal/pop*10000)
# Plot fatality rates over time for a few states
p1 <- Fatalities %>%
  filter(state %in% c("ca", "fl", "tx", "ny")) %>%
  ggplot(aes(x = year, y = fatal_rate, group = state, color = state)) +
  geom_line() +
  labs(title = "Traffic Fatality Rate Over Time for Selected States",
       x = "Year",
       y = "Fatality Rate (per 10,000)") +
  theme_minimal()

# Scatter plot of fatality rate vs. beer tax for 1988
p2 <- Fatalities %>%
  filter(year == "1988") %>%
  ggplot(aes(x = beertax, y = fatal_rate)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Fatality Rate vs. Beer Tax in 1988",
       x = "Beer Tax",
       y = "Fatality Rate (per 10,000)") +
  theme_minimal()

p1 + p2
```

## Meekes, Hassink, Kalb (2024)

- How to estimate the impact of the Covid-outbreak March 2020 on behaviour by individual persons?

- The Netherlands: Lockdown from March 16th 2020 onwards.

- What data do we need to investigate the impact of the lockdown?
  - Requirement 1: We need information from before the lockdown and during the lockdown. Dimension: time
  - Requirement 2: We need information from the same persons (or firms) in consecutive periods. Dimension: cross-sectional dimension.

- Methodological claim: empirical analyses that are not based on panel data are in general terms not very strong (= the results can easily be falsified).
- The two figures below give an impression about what happened in 2020 in the Netherlands.

## Meekes, Hassink, Kalb (2024) (Cont.)

![](screen1.png){width=80%}

## Meekes, Hassink, Kalb (2024) (Cont.)

![](screen2.png){width=80%}

# Advantages of Panel Data

## Advantages of Panel Data

:::{.callout-tip title="Examples: Why Panel Data Is Needed?"}

"Year of Birth" cohorts are followed across time. The research question is "do households sell their house when they become old?" The figure below cannot address this question because from one cross-section to another, it is not possible to disentangle cohort effects from age effects.

![](screen3.png){width=50% fig-align="center"}

::: 

## Home Ownership

:::{.callout-tip title="Examples: Why Panel Data Is Needed?"}

The figure below is constructed by panel data. The figure indicates strong cohort effects! For each birth cohort, in various years ($t= 1,2,3,\dots,12$) the average Dutch home ownership is given.

![](screen4.png){width=40% fig-align="center"}

From the cross section it looks like (on average) home ownership rate peaks at around 69%. However, this not necessarily the same for two different cohorts. E.g., compare the 1953 with the 1948 cohort.

:::

## Advantages of Panel Data

- Estimation of dynamic models (or transition models) is impossible in the case of a time series of cross sections (panel data).

:::{.callout-tip title="Example: Why Panel Data is Needed?"}

Let's assume that a cross-section study suggests that female labor force participation is equal to 50%. There are two extreme possibilities that we cannot distinguish between cross-sections:

- Possibility 1: 50% of the females are always employed (annual job turnover rate is 0%)
- Possibility 2: In a homogenous population, there is a 50% turnover rate each year.

We need panel data to solve this issue.

:::

## Advantages of Panel Data (Cont.)

- The primary reason for using panel data is to solve the statistical problem of omitted variables. See the figure below. For each of the *N* individuals (here three individuals) there is a separate scatter diagram.
  - The slope of the solid line is the slope of the regression equation of OLS on all data of all individuals together.
  - The slope of the dashed line is the slope of the regression equation in which it is corrected for the individual effect.
  - In the figures below, the slope of the dashed lines is different from the slope of the solid line.

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 10

# Load necessary libraries
library(ggplot2)
library(gridExtra)

# --- Figure 1.1: Positive within-group and overall correlation ---

# Generate data for Figure 1.1
set.seed(123)
fig1_data <- data.frame(
  x = c(rnorm(5, 1), rnorm(5, 3), rnorm(5, 5), rnorm(5, 7)),
  y = c(rnorm(5, 1, 0.5), rnorm(5, 3, 0.5), rnorm(5, 5, 0.5), rnorm(5, 7, 0.5)),
  group = factor(rep(1:4, each = 5))
)

# Create plot for Figure 1.1
fig1_plot <- ggplot(fig1_data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(color = "Overall OLS"), linetype = "solid") +
  geom_smooth(method = "lm", se = FALSE, aes(group = group, color = "Within-Group"), linetype = "dashed") +
  stat_ellipse(aes(group = group), level = 0.95) +
  scale_color_manual(name = "Regression Lines", values = c("Overall OLS" = "black", "Within-Group" = "black")) +
  labs(title = "Fig. 1.1: Positive Correlation", x = "x", y = "y") +
  theme_minimal() +
  theme(legend.position = "none")

# --- Figure 1.2: No overall correlation, positive within-group correlation ---

# Generate data for Figure 1.2
set.seed(123)
fig2_data <- data.frame(
  x = c(rnorm(5, 1), rnorm(5, 3), rnorm(5, 5), rnorm(5, 7)),
  y = c(rnorm(5, 3, 0.5), rnorm(5, 3, 0.5), rnorm(5, 3, 0.5), rnorm(5, 3, 0.5)),
  group = factor(rep(1:4, each = 5))
)
fig2_data$y <- fig2_data$y + as.numeric(fig2_data$group) * 0.5

# Create plot for Figure 1.2
fig2_plot <- ggplot(fig2_data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(color = "Overall OLS"), linetype = "solid") +
  geom_smooth(method = "lm", se = FALSE, aes(group = group, color = "Within-Group"), linetype = "dashed") +
  stat_ellipse(aes(group = group), level = 0.95) +
  scale_color_manual(name = "Regression Lines", values = c("Overall OLS" = "black", "Within-Group" = "black")) +
  labs(title = "Fig. 1.2: Spurious Correlation", x = "x", y = "y") +
  theme_minimal() +
  theme(legend.position = "none")

# --- Figure 1.3: Negative within-group correlation, positive overall correlation ---

# Generate data for Figure 1.3
set.seed(123)
fig3_data <- data.frame(
  x = c(rnorm(5, 1), rnorm(5, 3), rnorm(5, 5), rnorm(5, 7)),
  y = c(rnorm(5, 7, 0.5), rnorm(5, 5, 0.5), rnorm(5, 3, 0.5), rnorm(5, 1, 0.5)),
  group = factor(rep(1:4, each = 5))
)

# Create plot for Figure 1.3
fig3_plot <- ggplot(fig3_data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(color = "Overall OLS"), linetype = "solid") +
  geom_smooth(method = "lm", se = FALSE, aes(group = group, color = "Within-Group"), linetype = "dashed") +
  stat_ellipse(aes(group = group), level = 0.95) +
  scale_color_manual(name = "Regression Lines", values = c("Overall OLS" = "black", "Within-Group" = "black")) +
  labs(title = "Fig. 1.3: Omitted Variable Bias", x = "x", y = "y") +
  theme_minimal() +
  theme(legend.position = "none")


# --- Arrange plots in a grid ---
grid.arrange(fig1_plot, fig2_plot, fig3_plot, ncol = 3)
```

# Panel Data Structure

## Panel Data Models

- **Specification 1: Cross section:** In the first week we considered cross sections:
  - A random sample of *N* firms may have the following regression equation:

  $$
    \text{Profit} = \beta_0 + \beta_1 \text{Innovation} + \beta_2 \text{Firm Size} + u
  $$

- Cross-sectional dimension $N$.
  - Important: the explanatory variables may be correlated.
  - There is one intercept $\beta_0$. This equation can be reformulated as by adding a subscript $i$ for the $i$-th individual firm:

  $$
    \text{Profit}_i = \beta_0 + \beta_1 \text{Innovation}_i + \beta_2 \text{Firm Size}_i + u_i
  $$
  
## Time Series Dimension

- **Specification 2: Time Series**: In the third week we considered the following static time-series model. It is based on a data set containing outcomes for one firm, which is observed over *T* periods.

  $$
    \text{Profit}_t = \beta_0 + \beta_1 \text{Innovation}_t + \beta_2 \text{Firm Size}_t + u_t
  $$

- Time dimension $T$. 
- Again, there is one intercept $\beta_0$
- In this equation,  we add a subscript $t$ for the $t$-th period:


## Panel Data Specification 

- **Specification 3: Panel data** is a combination of the previous two equations: 

  $$
    \text{Profit}_{it} = \beta_0 + \beta_1 \text{Innovation}_{it} + \beta_2 \text{Firm Size}_{it} + u_{it}
  $$
  
- for $i=1,\dots, N$, $t=1,\dots, T$. 

- Again, the only intercept is $\beta_0$. It has a cross-sectional dimension N and a time dimension T.
  - Subscript i refers to individual (firm) and subscript t denotes time.
  - Important: the explanatory variables innovation and firm size may be correlated.
- **Issue 1:** Can equation (3) be generalized by N intercepts $\alpha_i$. Does each firm (subscript $i$) have their own intercept?

## Panel Data Specification (Cont.)

  $$
    \text{Profit}_{it} = \alpha_i + \beta_1 \text{Innovation}_{it} + \beta_2 \text{Firm Size}_{it} + u_{it}
  $$

- **Issue 2:** Is there any correlation between these $N$ intercepts $\alpha_i$ and each of the explanatory variables innovation and firm size?
- **Issue 3:** Should variables that remain constant within individual firms be treated differently? E.g. in the following specification, firm size does not change across time. Thus, frmsizeᵢ has no subscript $t$, in case the size of the firms is constant in all of the $T$ periods.
  $$
    \text{Profit}_{it} = \beta_0 + \beta_1 \text{Innovation}_{it} + \beta_2 \text{Firm Size}_{i} + u_{it}
  $$

-  **Issue 4:** Are the explanatory variables of the regression equation strictly exogenous? This is an econometric issue that is required for unbiased estimators. It will be explained below further.

## Structure of Panel Data{#sec-structure}

- Imagine tracking the GDP and foreign investment for 3 countries over 4 years.

:::{style="font-size: 1.5em;"}

| Country (i) | Year (t) | GDP ($y_{it}$) | Investment ($X_{it}$) |
|:-----------:|:--------:|:--------------:|:---------------------:|
|     USA     |   2019   |      21.4      |         0.25          |
|     USA     |   2020   |      20.9      |         0.16          |
|     USA     |   2021   |      23.0      |         0.36          |
|     USA     |   2022   |      25.4      |         0.13          |
|   Germany   |   2019   |      3.8       |         0.14          |
|   Germany   |   2020   |      3.8       |         0.13          |
|   ...       |   ...    |      ...       |          ...          |
|    Japan    |   ...    |      ...       |          ...          |

:::

- Here, $N=3$ and $T=4$.
- The data has both a **cross-sectional** dimension (comparing USA, Germany, Japan in one year) and a **time-series** dimension (tracking the USA from 2019-2022).

## The Individual-Specific Effect

- Suppose the following "true model":

  $$
    y_{it} = a_i + \beta_1x_{1it} + ... + \beta_kx_{kit} + u_{it} \quad i=1,...,N;t=1,...,T
  $$
- Where:
  - $a_i$ is the individual-specific effect (a random variable)
  - $u_{it}$ is the idiosyncratic (i.i.d.: identically and independently distributed) error term with expected value zero and constant variance.
  - *N*: cross-sectional dimension; *T*: time-dimension
  - There are *k* different explanatory variables.

## The Individual-Specifc Effect

- The constant $a_i$ captures all individual-specific variables that are not observed by the researcher; e.g. motivation (it is referred to as **unobserved heterogeneity**).

- It is possible that $E(a_i | x_{i11},..,x_{i1k},..,x_{iT1},...,x_{iTk}) \neq 0$ (e.g. in an equation where wage is the dependent variable, `motivation` (subsumed in $a_i$) might be correlated with the RHS-variable `experience`).

## Strict Exogeneity

:::{.callout-note title="Assumption TS.2 (Strict exogeneity)"}

For each $t$, the expected value of $u_t$ given ALL of the $k$ explanatory variables FOR ALL $T$ time periods, is equal to zero: $E[u_t | X] = 0$
  
:::

:::{.callout-note title="Assumption TS.2 (Contemporaneous exogeneity)"}

For each *t*, the expected value of $u_t$, given ALL of the *k* explanatory variables in period *t*, is equal to zero: $E(u_t | x_{t1},...,x_{tk}) = E(u_t | \mathbf{x}_t) = 0$

This assumption implies that the error term in period *t* is uncorrelated with all *k* regressors in the same period, $t$: $Corr(u_t,x_{tj})=0 \quad j=1,...,k$

:::

# Fixed Effects Regression

## Fixed Effects

- **Core Idea:** Treat the individual-specific effects, $\alpha_i$, as **parameters to be estimated**. Each individual gets their own intercept.

:::{.callout-note title="Definition: Fixed Effects Model"}

$$ y_{it} = (\beta_0 + \alpha_i) + \beta_1 X_{it} + \epsilon_{it} $$
or more simply:
$$ y_{it} = \alpha_i + \beta_1 X_{it} + \epsilon_{it} $$
(Here, the $\alpha_i$ represent the individual-specific intercepts)

:::

- The term "fixed effects" implies that we are making no assumptions about the distribution of the $\alpha_i$ or their correlation with $X_{it}$. We allow them to be correlated with the regressors.

## The "Within" Estimator for FE

- It is easy to estimate a fixed effects model: it is actually OLS estimation on transformed data
- This fact allows us to estimate the FE model without explicitly estimating $N$ different intercepts? 
- The goal is to eliminate the fixed effect $\alpha_i$ in $Y_{it} = \alpha_i + \beta_1 X_{it} + \epsilon_{it}$

## Within Transformation

:::{.callout-note title="Definition: Within Transformation"}

**Step 1:** For each individual $i$, calculate the time-average of their variables:
$$ \bar{y}_i = \frac{1}{T} \sum_{t=1}^{T} y_{it} \quad \text{and} \quad \bar{X}_i = \frac{1}{T} \sum_{t=1}^{T} X_{it} $$

**Step 2:** Subtract the individual-specific average from the original model:

  $$
  \begin{align}
    y_{it} - \bar{y}_i &= \beta_1 (X_{it} - \bar{X}_i) + (u_{it} - \bar{u}_i) \\
    \ddot{y}_{it} &= \beta_1 \ddot{x}_{it} + \ddot{u}_{it}
  \end{align}
  $$
The fixed effect $\alpha_i$ is time-constant, so $\alpha_i - \bar{\alpha}_i = \alpha_i - \alpha_i = 0$. It drops out!

**Step 3:** Run OLS on the "de-meaned" data:
$$ (y_{it} - \bar{y}_i) \text{ on } (X_{it} - \bar{X}_i) $$
This gives a consistent estimate of $\beta_1$.
:::


## Least Squares Dummy Variable Model

- An alternative, but **equivalent**, way to get FE estimates is the LSDV model.

:::{.callout-note title="LSDV Model"}

Create a dummy (0/1) variable for each individual $i$ (except for one, to avoid the dummy variable trap).

Run a single OLS regression including these $N-1$ dummy variables.

$$ y_{it} = \beta_0 + \beta_1 X_{it} + d_1\alpha_1 + d_2\alpha_2 + ... + d_{N-1}\alpha_{N-1} + \epsilon_{it} $$

The estimated coefficient $\beta_1$ from the LSDV model is **identical** to the one from the "Within" estimator.

:::

- The coefficients on the dummies ($\alpha_i$) are the estimated fixed effects.
- LSDV is impractical for panels with very large $N$ (e.g., thousands of individuals) due to computational burden. The "Within" estimator is more efficient.

## Interpretation of FE Coefficients

:::{.callout-tip title="Interpretation of Fixed Effects models"}

In a Fixed Effects model, the coefficient $\beta_1$ measures:

The average change in $y$ for a one-unit increase in $X$ **within** a given individual over time.

:::

- The FE estimator uses only the variation *within* each individual (firm/country/etc.) to estimate the coefficients.
- It effectively ignores variation *between* individuals. You are comparing individual A at time 1 to individual A at time 2, not to individual B.

## Time-Constant Variables

- The parameter vector $\beta$ is identified ('can be estimated') due to time-variation in $x_{it}$ for each individual: the variables differ across time at the level of the individual.
  - Individual-specific variables that are constant over time (e.g. gender, year of birth) cannot be included in the model. Their parameters are not identified, as they are incorporated in the individual effect $a_i$.

- Estimator $\hat{\beta}_{within}$ and $\hat{a}_i$ are consistent if $T$ and $N$ are large.
- If $T$ is small and $N$ is large, then $\hat{\beta}_{within}$ is still consistent. $\hat{a}_i$ is inconsistent because of the small number of observations ($T$).

## Consistency of Fixed Effects 

- Why is $\hat{\beta}_{within}$ a consistent estimator? Let's assume for simplicity we have a bivariate model: $y_{it} = x_{it}\beta + a_i + u_{it}$ or
  - $\ddot{y}_{it} = \ddot{x}_{it}\beta + \ddot{u}_{it}$ (5)
- Consistency requires that the error term is uncorrelated with the explanatory variable:
  $Corr(\ddot{x}_{it},\ddot{u}_{it}) = Corr(x_{it} - \bar{x}_i, u_{it} - \bar{u}_i) = 0$
  - It means that $u_{it}$ is uncorrelated with $\bar{x}_i$
  - It means that $u_{it}$ is uncorrelated with $x_{i1},...,x_{iT}$
  - Uncorrelated with x in the past, present and future..

- Conclusion: strict exogeneity is needed (this excludes lagged dependent variables and feedback effects).
  - Contemporaneous exogeneity is too weak to prove consistency of the fixed effects estimator $\beta_{within}$ because it does not exclude correlation between $u_{it}$ and $x_{i1},...,x_{iT}$.

## Visualization Fixed Effects

```{r}
#| echo: false
#| fig.align: 'center'
#| fig.width: 7
#| fig.height: 6

# Load required packages
library(ggplot2)
library(dplyr)
library(lmtest)
library(plm)

# Set seed for reproducibility
set.seed(123)

# Parameters
n_countries <- 3
n_years <- 20
countries <- c("Country A", "Country B", "Country C")

# Create panel data
panel_data <- expand.grid(country = countries, year = 1:n_years) %>%
  mutate(
    # Common slope (0.5) but different intercepts by country
    intercept = case_when(
      country == "Country A" ~ 1,
      country == "Country B" ~ 3,
      country == "Country C" ~ 5
    ),
    x = runif(n(), 0, 10),
    epsilon = rnorm(n(), 0, 0.5),
    y = intercept + 0.5 * x + epsilon
  )

# Estimate fixed effects model
fe_model <- plm(y ~ x, data = panel_data, index = c("country", "year"), model = "within")

# Create visualization
ggplot(panel_data, aes(x = x, y = y, color = country)) +
  geom_point(alpha = 0.7) +  # Plot actual points
  geom_abline(aes(intercept = 1, slope = 0.5), color = "#F8766D", linetype = "solid", size = 1) +  # Country A line
  geom_abline(aes(intercept = 3, slope = 0.5), color = "#00BA38", linetype = "solid", size = 1) +  # Country B line
  geom_abline(aes(intercept = 5, slope = 0.5), color = "#619CFF", linetype = "solid", size = 1) +  # Country C line
  labs(title = "Fixed Effects Regression Visualization",
       subtitle = "Same slope (0.5) but different intercepts by country",
       x = "Independent Variable (x)",
       y = "Dependent Variable (y)",
       color = "Country") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("#F8766D", "#00BA38", "#619CFF"))

```


## Pros and Cons of the Fixed Effects Model

- **Pros:**
    - **Controls for all time-invariant omitted variables**, whether observed or unobserved. This is its most powerful feature. If you are worried that unobserved `ability` is correlated with both `education` (your X) and `wage` (your Y), FE solves this problem because `ability` is constant for an individual.
    - It is consistent even if the unobserved effect $\alpha_i$ is correlated with the regressors $X_{it}$.

- **Cons:**
    - **Cannot estimate the effect of time-invariant variables.** The "Within" transformation wipes them out. 
    - For example, you cannot estimate the effect of `gender` or `race` on wages using a standard FE model, because these variables do not change over time for an individual.
    - May be less efficient than the Random Effects model if its stricter assumptions hold.
    
## Fixed Effects in Software

- Arguably, the fixed effects model is one of the most often-used models in modern econometrics.

- Standard statistical software such as `lm()` in R or `sm.OLS()` in Python can implement FE using the LSDV method, but this is often tedious. 

- The `fixest` (R) and `pyfixest` (Python) package provide a very easy way to estimate FE proceding from a dataset that looks like the one on [Slide @sec-structure]. 

- This is how that works in practice: 

:::panel-tabset

### R

```{r}
#| eval: false
#| echo: true
library(fixest)
model <- feols(y ~ x1 + x2 | fe1 + fe2, data = dataset) 
# fixed effects separated from ind. vars. by |
summary(model)
```

### Python

```{python}
#| echo: true
#| eval: false
import pyfixest as pf

fit = pf.feols(fml="y ~ x1 + x2 | fe1 + fe2", data=data)
# fixed effects separated from the ind. vars. by | 
fit.summary()
```

### Stata

```{stata}
#| echo: true
#| eval: false

xtset country year *\In order: $i$ variable, $t$ variable
xtreg y x1 x2, fe1 fe2 *\fixed effects separated by a comma
```

:::


## Between vs. Within Variation

- **Between variation:** the cross-sectional variation (across individuals). For example:

| | Profit (= dependent variable) | Innovation (=explanatory variable) |
|---|---|---|
| Firm A | 500 thousand Euros | 1 percent |
| Firm B | 750 thousand Euros | 3 percent |

- **Between variation** (across firms): as a result of the increased innovation (from 1 percent to 3 percent) the profits increase from 500 thousand Euros to 750 thousand Euros.

## Between vs. Within Variation (Cont.)

- **Within variation:** the time-series variation (for a given individual). So the variation within individuals. For example:

| | Profit (= dependent variable) | Innovation (=explanatory variable) |
|---|---|---|
| Time 1 | 500 thousand Euros | 1 percent |
| Time 2 | 550 thousand Euros | 3 percent |

- **Within variation** (for a given firm): as a result of the increased innovation (from 1 percent to 3 percent (thus by 2 percentage points) the profits increase from 500 thousand Euros to 550 thousand Euros from $t=1$ to $t=2$

- Economists are usually interested in the within variation more than in the between variation.
- To measure the within variation of *x* on *y*, we need to control for individual effects. Consequently, it allows for correlation between the individual effect and the explanatory variable.


# First Differences Estimator

## First Differences Estimator

- The regression equation is: $y_{it} = a_i + \beta_1x_{1it} + ... + \beta_kx_{kit} + u_{it}$
- Recall that in regression equations the explanatory variables $x_{it}$ are allowed to be correlated. E.g. education and experience are correlated in a wage equation.
- This is also the case in the fixed effects model. Equation (10) allows for correlation between the explanatory variable $a_i$ and the other explanatory variables $x_{1it}, \dots, x_{kit}$.
- Aside from the FE and LSDV estimators, there is another method to estimate parameter $\beta$ consistently: the **first differences estimator**. 


## First Differences: Definition

- Let’s assume there is only one explanatory variable $x$
- It is possible to consistently estimate the $\beta$ parameter by taking first differences
  - $y_{it} = \beta x_{it} + a_i + u_{it}$ (the regression equation in period t)
  - $y_{i,t-1} = \beta x_{i,t-1} + a_i + u_{i,t-1}$ (the regression equation in period t-1) 

- The **first difference** is: 
  - $y_{it} – y_{i,t-1} = \beta x_{it} – \beta x_{i,t-1} + a_i – a_i + u_{it} – u_{i,t-1}$, or:
  - $\Delta y_{it} = \beta\Delta x_{it} + \Delta u_{it} \qquad i = 1,...,N; t = 2,...,T$ (11)
- It means that all variables of equation (11) have the same first-differences transformation $\Delta$

## First Differences: Removal of $\alpha_i$

- By taking first differences, the individual effect $a_i$ is removed from the model.
- We calculate the OLS estimator for equation (11): the so-called first-difference estimator of the regression parameter $\beta$
  - Denote the first-difference estimator by $\hat{\beta}_{fdif}$
  - Note that the first period $t=1$ is used for $\Delta y_{i2}, \Delta x_{i2}, \Delta u_{i2}$,

## Consistency of First Differences

- Why is $\hat{\beta}_{fdif}$ a consistent (or unbiased) estimator? Let’s assume for simplicity we have a bivariate model: $y_{it} = x_{it}\beta + a_i + u_{it}$ and $\Delta y_{it} = \Delta x_{it}\beta + \Delta u_{it}$
- Consistency requires that the error term is uncorrelated with the explanatory variable:
    $Corr(\Delta u_{it}, \Delta x_{it}) = Corr(u_{it} – u_{i,t-1}, x_{it} – x_{i,t-1}) = 0$

- It means that $u_{it}$ is uncorrelated with $x_{i,t-1}, x_{it}, x_{i,t+1}$. In other words, strict exogeneity is needed (no lagged dependent variable, no feedback effects) for consistency (unbiasedness) of the first-difference estimator.

## Contemporaneous Exogeneity

- Note that contemporaneous exogeneity is too weak to prove consistency of the first-difference estimator $\hat{\beta}_{fdif}$, because it does not exclude consistency between $u_{it}$ and $x_{i,t-1}$.
  - Furthermore, it is assumed that the error term $u_{it}$ follows a random walk, i.e.:
    - $u_{it} = u_{i,t-1} + e_{it}$
    - $E(e_{it}) = 0$; $Var(e_{it}) = \sigma_e^2$ (expected value of zero; constant variance)
    - $e_{it}$ is independent over time and across individuals
    
- Then one can show that (here less important):
  - $\hat{e}_{it} = \Delta y_{it} – \Delta x_{it}\hat{\beta}_{fdif}$
  - $\hat{\sigma}_e^2 = \frac{\sum_{i=1}^{N} \sum_{t=1}^{T} \hat{e}_{it}^2}{N(T-1) - k}$
  - Consistent estimates of $Var(\hat{\beta}_{fdif})$ (using $\hat{\sigma}_e^2$)

## Dynamics in FD

- In other words, OLS estimation of the FD equation gives the correct standard errors of $\hat{\beta}_{fdif}$.
- Suppose that there is autocorrelation and heteroskedasticity in $e_{it}$. In that case $Var(\hat{\beta}_{fdif})$ is incorrect, because $\hat{\sigma}_e^2$ is incorrect: robust Newey-West standard errors are necessary.
    - Note that it is a different estimator for $Var(\hat{\beta}_{fdif})$ than the robust standard error (that only corrects for heteroskedasticity)

# FE vs. FD

## Fixed effects versus First Differences

- **What do the estimators have in common?**
- They allow for correlation between $a_i$ and the explanatory variables $x_{i1}...x_{iT}$: $E(a_i | x_{i1},...,x_{iT},...,x_{ki1},...,x_{kiT}) \neq 0$
- The assumption of strict exogeneity (which means that the regression equation contains no feedback mechanism; there is no lag of dependent variable)

- **Consequence:** The parameter estimates of both estimators should be about the same (if the assumption of strict exogeneity is true).

## Fixed effects versus First Differences (Cont.)

- **How do they differ?** Assumption about the error term u:
    - Fixed effects estimator: $u_{it}$ is independent over time and across individuals
    - First difference estimator (see week 4): $u_{it} = u_{i,t-1} + e_{it}$
        - $e_{it}$ is independent over time and across individuals
        - Thus $\Delta u_{it}$ is independent over time and across individuals
        - Thus it is assumed that $u_{it}$ follows a random walk, i.e.:
            $u_{it} = u_{i,t-1} + e_{it}$
            $E(e_{it}) = 0$; $Var(e_{it}) = \sigma_e^2$ (expected value of zero; constant variance)

- **Consequence:** Fixed-effects estimator gives smaller standard errors if the specification is correct and there is strict exogeneity. This estimator is more efficient.

- First-difference estimator is preferred if there is a unit root in the error terms: $u_{it} = u_{i,t-1} + e_{it}$

## Issue: First differences or fixed effects? (II)

- If the regression equation is correctly specified,the within estimation procedure and the 'first-difference estimation' procedure should yield similar estimates for the parameters $\beta$.

  $y_{it} = \beta_1x_{1it} +...+ \beta_kx_{kit} + a_i + u_{it}$

- Question: which of the two estimation procedures is preferable?
  - Answer: It depends on the time series behavior of $u_{it}$. If it is a white noise error term, use the within estimation procedure $\hat{\beta}_{within}$.
  - If it follows a random walk ($u_{it} = u_{i,t-1} + e_{it}$), use the first-difference procedure $\hat{\beta}_{fdif}$.
  
## Issue: First differences or fixed effects? (III)

- ***Motivation for the procedure***
  - **Assumption:** strict exogeneity (no feedback, no lagged dependent variables).

- For ease of exposition here we take one explanatory variable x

- Fixed-effects estimator: $u_{it}$ is identically and independently distributed.
- First-difference estimator: $\Delta u_{it}$ is identically and independently distributed.
  - In other words, $u_{it}$ has a unit root.

## Correlation of Error Terms in FD

- The correlation between the error terms of a first-difference estimator is as follows.

:::{.callout-note title="Correlation of $u_{it}$ in the FD model"}

**Assumption:** uᵢₜ is identically and independently distributed:

$Cov(u_{it},u_{is}) = 0$ for $t \neq s$ (same individual i)
$Cov(u_{it},u_{it}) = \sigma_u^2$ for $t = s$

Next, we estimate the model:
$\Delta y_{it} = \Delta x_{it}\beta + \Delta u_{it} \quad i=1,...,N; t = 2,...,T$

The correlation between $\Delta u_{it}$ and $\Delta u_{i,t-1}$ is:

$Corr(\Delta u_{it}, \Delta u_{i,t-1}) = Corr(u_{it} - u_{i,t-1}, u_{i,t-1} - u_{i,t-2}) = -0.5$.

Hence, the intertemporal correlation of the FD-estimator is -0.5 if $u_{it}$ is i.i.d.
:::

## Motivation for FD Autocorrelation

:::{.callout-note title="Correlation of $u_{it}$ in the FD model"}
Covariance for the same individual across time:

$Cov[aX+bY,cW +dZ]= acCov[X,W]+ adCov[X,Z]+bcCov[Y,W]+bdCov[Y,Z]$

$$
\begin{align}
Cov(u_{it}-u_{i,t-1}, u_{i,t-1}-u_{i,t-2})  &= \\
Cov(u_{it},u_{i,t-1}) &+ Cov(u_{it},-u_{i,t-2}) + Cov(-u_{i,t-1},u_{i,t-1}) + \\  &Cov(-u_{i,t-1},-u_{i,t-2}) \\
&= 0 + 0 - Var(u_{i,t-1}) + 0
&= -\sigma_u^2
\end{align}
$$

$$
\begin{align}
Var(u_{it} - u_{i,t-1}) &= Var(u_{it}) + Var(-u_{i,t-1}) + 2Cov(u_{it},-u_{i,t-1}) \\
&= \sigma_u^2 + \sigma_u^2 + 0 = 2\sigma_u^2
\end{align}
$$

$$
\begin{align}
Corr(\Delta u_{it}, \Delta u_{i,t-1}) &= \frac{Cov(\Delta u_{it}, \Delta u_{i,t-1})}{\sqrt{Var(\Delta u_{it}) \cdot Var(\Delta u_{i,t-1})}} \\
&= \frac{-\sigma_u^2}{2\sigma_u^2} = -0.5
\end{align}
$$

:::


## Fixed Effects or First Differences Procedure

:::{.callout-note title="FE vs. FE Procedure"}

To check for FE versus FD, follow the below procedure:

- Step 1: Run a first-difference regression equation of $\Delta y_{it}$ on $\Delta x_{it}$
- Step 2: Predict the residuals (which gives $\Delta \hat{u}_{it}$) and run a Breusch-Godfrey test for autocorrelation of $\Delta \hat{u}_{it}$ on $\Delta \hat{u}_{i,t-1}$ and $\Delta x_{it}$.

- Step 3A: If the estimated coefficient on $\Delta \hat{u}_{i,t-1}$ is about -0.5 (i.e. -0.5 is within the 95% confidence interval) then there is an indication that $u_{it}$ is an independent error term ($u_{it}$ is i.i.d.). Conclusion: prefer within-estimates (fixed effects).
- Step 3B: If the estimated coefficient on $\Delta \hat{u}_{i,t-1}$ is not equal to -0.5 (i.e. -0.5 is outside the 95% confidence interval) then there is an indication that $u_{it}$ is not an independent error term. Conclusion: prefer first-differences.

- If the two procedures yield dramatically different estimates for $\beta$, the two conclusions are possible, either:
  - For some RHS-variables, the assumption of strict exogeneity does not hold.
  - The regression model is incorrectly specified. Some important time-varying regressors are missing in the equation.
- It is useful to compare the results of both regression procedures.

:::


## Some useful Stata commands

*   Use the `tsset` command at the beginning of the do-file:
    *   `tsset i t`
    *   ‘i’ is the name of the variable that refers to the individual
    *   ‘t’ is the name of the variable that refers to time
    *   alternative: `xtset i t`

*   summary statistics that take into account both dimensions (individuals and time):
    *   `xtsum y x`

*   First differences
    *   `reg d.y d.x`
    *   `reg d.y d.x, robust`
    *   `reg d.y d.x, cluster(number)`
    *   Some students make the following mistake:
        `xtreg d.y d.x`


## Estimation Procedure

**Estimation procedure:**
* We started with the first-difference estimator β̂fdif
* Next, we checked for autocorrelation, using the Breusch Godfrey test.
* The parameter on the lagged residual was statistically different from zero (t-value: -8.93).
* We re-estimated the model with the first-difference estimator, using clustered standard errors.

## What did we do?

- **Issues:**
- Within variation versus between variation.
- Within effects versus between effects.
- Estimators for within effects: the fixed effects and the first-difference estimator.
- Advantage: we correct for unobserved effects, which are allowed to correlate with the explanatory variables.
- Estimator for between effects: Pooled OLS estimator; Random effects (next lecture) 




# The End






