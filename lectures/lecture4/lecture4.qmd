---
title: "Empirical Economics"
subtitle: "Lecture 4: The Linear Model and Time Series"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 4 - Linear Model and Time Series'
resources:
  - demo.pdf
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Introduction

## Course Overview

- Statistics and Probability - Basic Concepts
- Statistics and Probability - Hypothesis Testing
- The Linear Regression Model
- Time Series Data
- Panel Data (FE) and Control Variables
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Hands-on Econometrics in Practice


## What is Time Series Data?

:::{.callout-note title="Definition: Time Series Data"}
Time series data consists of observations of a variable or several variables over time.
:::

- **Key Feature:** The data is ordered chronologically.

:::{.callout-tip title="Examples: Time Series Data"}
- Gross Domestic Product (GDP) measured quarterly
- Monthly inflation rates
- Daily stock prices
- Annual government budget deficits
:::

---

## Characteristics of Time Series Data

- **Temporal Ordering:** Unlike cross-sectional data, the order of observations in time series data matters. The past can affect the future, but the future cannot affect the past.
- **Serial Correlation (or Autocorrelation):** Observations in a time series are often correlated with their own past values. For example, a high GDP in one quarter may suggest a high GDP in the next quarter.
- **Seasonality:** Many time series exhibit regular patterns at certain times of the year (e.g., retail sales are higher in the fourth quarter).
- **Trends:** A time series may have a long-term upward or downward movement.

## Objectives of Time Series Analysis

- There are various objectives of time series econometrics:

1. **Forecasting:** Predicting future values of a variable is a primary objective. For instance, forecasting inflation is crucial for central banks.
2. **Policy Analysis:** Economists use time series models to assess the impact of policy changes. For example, what is the effect of an interest rate hike on unemployment?
3. **Understanding Dynamic Economic Relationships:** Time series analysis helps us understand how economic variables interact over time.

## Notation and Basic Concepts

- $Y_t$:  The value of the variable Y at time period t.
- $Y_{t-1}$: The value of Y in the previous period (the first lag).
- $Y_{t-s}$: The value of Y s periods ago (the s-th lag).
- $\Delta Y_t = Y_t - Y_{t-1}$: The first difference of $Y$, which represents the change in $Y$ from the previous period to the current period.

# Introduction to Dynamic Processes

## A Time Series as a Stochastic Process

:::{.callout-note title="Definition: Time Series (Mathematical)"}
A time series is a set of random variables indexed by time, denoted as $\{Y_1, Y_2, \dots, Y_T\}$ or simply $\{Y_t\}$. 
:::

- Stochastic vs. Deterministic: We treat a time series as a stochastic process because we don't know the future values with certainty. We can only talk about their probability distributions.

- One Realization: The actual data we have (e.g., GDP from 1950-2024) is just one of many possible paths the process could have taken. 
- Our goal is to infer the properties of the underlying process from this single realization.

## Basic Statistical Properties

- Just like any random variable, each point in a time series has statistical properties.

  - Mean: The expected value of the process at time t: $E(Y_t) = \mu_t$
  - Variance: The variance of the process at time t: $Var(Y_t) = E[(Y_t - \mu_t)^2] = \sigma_t^2$

- Crucially, in general, the mean and variance could be different at each point in time.


## Autocovariance 

- Covariance: 
  - Recall that covariance measures how two variables move together.

- Autocovariance
  - Autocovariance is the covariance of a time series with its own past values. It measures the linear dependence between different points in the series.
  
:::{.callout-note title="Definition: Autocovariance"}
The $k$-th Order Autocovariance ($\gamma_k$):

  $$
    \gamma(t, t-k) = Cov(Y_t, Y_{t-k}) = E[(Y_t - \mu_t)(Y_{t-k} - \mu_{t-k})]
  $$
:::

- This measures the covariance between the series at time t and time t-k.

## Stationarity: A Key Simplifying Assumption

- For analysis to be tractable, we often assume the series is covariance stationary (or weakly stationary).

- This means the three aforementioned statistical properties do not change over time. Three conditions must hold:

:::{.callout-note title="Definition: Stationarity"}
- Constant Mean: $E(Y_t) = \mu$ for all t. The series fluctuates around a constant level.
- Constant Variance: $Var(Y_t) = \sigma^2$ for all t. The volatility of the series is constant.
- Constant Autocovariance: $Cov(Y_t, Y_{t-k}) = \gamma_k$ for all t. The covariance between two points depends only on the lag $k$ (how far apart they are), not on their position in time.
:::

## Autocorrelation Function (ACF)

- The value of the autocovariance ($\gamma_k$) depends on the units of the variable $Y$. This makes it hard to compare across different series.

- Autocorrelation: standardize the autocovariance to get the autocorrelation, which is unit-free.

:::{.callout-note title="Definition: Autocorrelation"}

The $k$-th Order Autocorrelation ($\rho_k$): 

  $$
    \rho_k = \frac{ Cov(Y_t, Y_{t-k}) }{\sqrt{Var(Y_t) Var(Y_{t-k})}}
  $$

If the series is stationary, this simplifies to:

  $$
    \rho_k = \frac{\gamma_k}{\gamma_0}
  $$
  
where $\gamma_0$ is the variance, $Var(Y_t)$. 
  
:::

## Interpretation of the Autocorrelation Function (ACF)

- The ACF, denoted by $\rho_k$, measures the "memory" or persistence of a time series.
  - It is a value between -1 and +1.
  - $\rho_1$: The correlation between $Y_t$ and $Y_{t-1}$("today" and "yesterday").
  - $\rho_k$: The correlation between $Y_t$ and $Y_{t-k}$
  - A high $\rho_k$ suggests that a shock in one period will have a persistent effect k periods later.

- For a stationary series, we expect $\rho_k \rightarrow 0$ as $k$ gets larger.

## The Correlogram (ACF Plot)

- A correlogram is a bar chart that plots the sample autocorrelations ($r_k$) for different lags ($k = 1, 2, 3, \dots$).
  - It provides a visual summary of the persistence in a time series. We can see how quickly the correlations die out.
  - Correlograms usually include confidence bands. Autocorrelations that fall outside these bands are considered statistically different from zero.

(Todo: Insert a sample correlogram plot here, showing bars for lags 1, 2, 3... and confidence bands)

# Modeling a Single Time Series

## Why Model a Single Series?

- Before we ask how X affects Y, we must first understand the behavior of Y itself.
- **Univariate models** describe the dynamic properties of a single time series using its own past.
- Main uses:
    1.  **Understanding Persistence:** How long do shocks to the economy last?
    2.  **Forecasting:** Using the past of a series to predict its future.

## The White Noise Process

- The Simplest Time Series: A process called "white noise" is the building block for more complex models. We often denote it as $u_t$

:::{.callout-note title="Definition: White Noise Process"}
A white noise is a time series such that:

- $E(u_t) = 0$ (Zero mean)
- $Var(u_t) = \sigma^2$ (Constant variance)
- $Cov(u_t, u_s) = 0$ for all $t\neq s$ (No autocorrelation)
:::

- Interpretation: A white noise process is completely random and unpredictable. Its ACF will be zero for all lags $k > 0$. It is the ideal error term in a time series regression.

## The Autoregressive (AR) Model: AR(1)

- The simplest dynamic model is the first-order autoregressive, or AR(1), model:

$$
  Y_t = α + \rho Y_{t-1} + u_t
$$

- $Y_t$: The value of Y at time t.
- $Y_{t-1}$: The value of Y in the previous period.
- $\rho$: The autoregressive coefficient, which measures the persistence of the series.
- $u_t$: A white noise error term (uncorrelated with past values).

## Visualization AR(1) Process

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import polars as pl
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

T = 100      # Number of time periods to simulate
rho = 0.9    # Autoregressive parameter (must be < 1 for stationarity)
c = 2        # A constant/intercept term
sigma = 1.0  # Standard deviation of the white noise error term

# Generate all the random shocks at once using NumPy
np.random.seed(42) # for reproducibility
epsilon = np.random.normal(loc=0, scale=sigma, size=T)

# Initialize a Python list to store our time series values
y_values = [0] * T

# Set the initial value. A good choice is the unconditional mean of the process
# E[Y] = c / (1 - rho), to start the series near its long-run equilibrium.
y_values[0] = c / (1 - rho)

# Loop to generate the AR(1) series recursively
for t in range(1, T):
    # Apply the AR(1) formula
    y_values[t] = c + rho * y_values[t-1] + epsilon[t]

# Create a DataFrame with a time index and the simulated series
df = pl.DataFrame({
    'time': range(T),
    'ar_process': y_values
})

# Set the style and size for the plot
sns.set_theme(style="whitegrid")
plt.figure(figsize=(14, 7))

# Create the line plot
lineplot = sns.lineplot(
    data=df,
    x='time',
    y='ar_process',
    label=f'AR(1) Process (ρ={rho})'
)

# Add a horizontal line for the theoretical mean of the process
mean_value = c / (1 - rho)
plt.axhline(
    mean_value,
    color='red',
    linestyle='--',
    label=f'Theoretical Mean: {mean_value:.2f}'
)

# Add titles and labels for clarity
plt.title('Simulation of a Stationary AR(1) Process', fontsize=16)
plt.xlabel('Time Period', fontsize=12)
plt.ylabel('Value', fontsize=12)
plt.legend()

# Show the plot
plt.show()
```


## Stationarity of an AR(1) Process

- For the AR(1) model $Y_t = \alpha + \rho Y_{t-1} + u_t$:
  - If $|\rho| < 1$:, the series is **stationary**. Any shock ($u_t$) will eventually die out.
  - If $|\rho|=1$, the series is **non-stationary** and is called a **random walk**. Shocks have a permanent effect.
  - If $|\rho|>1$, the series is **explosive** (this is rare in economics).

- **Speed of Adjustment:** A value of $\rho$ close to 0 suggests that the effect of a past value dies out quickly.
- **Persistence:** A value of $\rho$ close to 1 indicates that a shock to the system will persist for a long time. The series will return to its mean slowly.

## The Random Walk Model

- A special case of the AR(1) model where $\rho=1$:

  $$
    Y_t = Y_{t-1} + u_t
  $$

- The best forecast for tomorrow's value is today's value.
- **Permanent Shocks:** The impact of a shock $u_t$ is permanent; it is carried forward in all future periods.
  - **Examples:** The prices of financial assets are often modeled as random walks.

## Visualization Random Walk

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import polars as pl
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

T = 500      # Number of time periods to simulate
rho = 1    # Autoregressive parameter (must be < 1 for stationarity)
c = 2        # A constant/intercept term
sigma = 5.0  # Standard deviation of the white noise error term

# Generate all the random shocks at once using NumPy
np.random.seed(42) # for reproducibility
epsilon = np.random.normal(loc=0, scale=sigma, size=T)

# Initialize a Python list to store our time series values
y_values = [0] * T

# Set the initial value. A good choice is the unconditional mean of the process
# E[Y] = c / (1 - rho), to start the series near its long-run equilibrium.
y_values[0] = 0

# Loop to generate the AR(1) series recursively
for t in range(1, T):
    # Apply the AR(1) formula
    y_values[t] = c + rho * y_values[t-1] + epsilon[t]

# Create a DataFrame with a time index and the simulated series
df = pl.DataFrame({
    'time': range(T),
    'ar_process': y_values
}, strict=False)

# Set the style and size for the plot
sns.set_theme(style="whitegrid")
plt.figure(figsize=(14, 7))

# Create the line plot
lineplot = sns.lineplot(
    data=df,
    x='time',
    y='ar_process',
    label='Random Walk'
)

# Add titles and labels for clarity
plt.title('Simulation of a Random Walk', fontsize=16)
plt.xlabel('Time Period', fontsize=12)
plt.ylabel('Value', fontsize=12)
plt.legend()

# Show the plot
plt.show()
```

## Higher-Order AR(p) Models

- We can include more lags of the dependent variable. An $AR(p)$ model includes $p$ lags:

$$
Y_t = \alpha + \rho_1 Y_{t-1} + \rho_2 Y_{t-2} + \dots + \rho_p Y_{t-p} + u_t
$$

- **Choosing p:** The number of lags (p) can be determined using information criteria like the Akaike Information Criterion (AIC) or the Schwarz Information Criterion (SIC).

## The Moving Average Model

- In the MA model, the current value is a function of the current and past **random shocks**.

  $$
    Y_t = \mu + u_t + \theta u_{t-1}
  $$

- $\mu$: The mean of the series.
- $\theta$: The moving average coefficient.
- The MA(1) process has a **finite memory**. A shock $u_{t-1}$ affects $Y_{t-1}$ and $Y_t$, but has no direct effect on $Y_{t+1}$. 

## Estimation and Interpretation of MA Coefficients ($\theta$)

- As you may have noticed, there are no independent variables in an MA model. 
  - Usually, what we do is assume $u_0=0$, take a guess for $\mu$ and $\theta$, and solve for $u_t$: $u_t = Y_t - \mu - \theta u_{t-1}$
  - If the true shocks came from a Normal distribution $N(0, \sigma^2_u)$, how likely was it to get this specific sequence of inferred shocks? The joint probability of observing this sequence is called the Likelihood.
  - We try to find the most likely $\mu$ and $\theta$ that could have generated the actual data.[^1]
  
- Unlike an AR(1) model, a shock in an MA(1) model only persists for one period. Yt is affected by ut and ut-1, but Yt+1 will not be affected by ut.
- The coefficient $\theta$ determines the extent to which a past shock affects the current observation.

[^1]: More about this in Lecture 6. 

## The Autoregressive Moving Average (ARMA) Model

- **Combining AR and MA:** We can combine the features of autoregressive and moving average models to create an ARMA(p,q) model.
- For example, an ARMA(1,1) Model: $Y_t = \alpha + \rho Y_{t-1} + u_t + \theta u_{t=1}$
- ARMA models are very flexible and can capture a wide range of time series dynamics.


# Relationship Between Time Series

## Linear Model in Time Series

- The model form is identical to its cross-sectional counterpart:

  $$
    Y_t = \alpha + \beta X_t + \epsilon_t
  $$

- This model posits a contemporaneous relationship between X and Y.
- Question: Can we estimate this with OLS and trust the results?
- For OLS to be a good estimator, the standard assumptions must hold. The most problematic one for time series data is the assumption of :
  
  $$
    Cov(X_t, \epsilon_t) = 0
  $$
    
## The Problem of Spurious Regression

- If $X$ is stationary, e.g. $X \sim AR(1)$ with $|\rho|<1$, this is a valid approach. 

- However, in different situations, OLS may give a **deceptive relationship:**
  - A spurious regression occurs when we find a statistically significant relationship between two time series variables that are actually unrelated.
- This often happens when both variables have a strong trend (are non-stationary), and especially, when $X$ is non-stationary. 
  - The trend can be deterministic (predictable) or stochastic (random).

## Spurious Regression is Non-Stationarity

- Let's look at the most common non-stationary process: the random walk.

  $$
    X_t = X_{t-1} + u_t
  $$

:::{.callout-note title="Theorem: Variance of a Random Walk"}
Suppose $X_0=0$. Let us find the variance of $X_t$. 

We know that $X_1 = u_i$. $X_2 = X_1 + u_2 = u_1 + u_2$, etc. 

Hence, $X_t = \sum u_i$, and $Var(X_t) = Var(u_1 + u_2 + \dots + u_t)$. 

Since the $u_i$ are uncorrelated (white noise), the variance of the sum is the sum of the variances:
  $$
    \begin{align}
    Var(X_t) &= \sigma^2_u + \dots + \sigma^2_u \text{ ( t times)} \\
            &= t \times \sigma^2_u
    \end{align}
  $$

The variance depends on time t and grows without bound. This is a clear violation of stationarity.

:::

## Failure of Reliable Estimation

- Imagine two independent and unrelated random walks, $Yt = Yt-1 + u_t$ and $Xt = Xt-1 + v_t$, where $u_t$ and $v_t$ are independent white noise processes.

- By construction, the true $\beta$ in a regression of $Y_t$ on $X_t$ is zero.

- When we run $Y_t= \alpha + \beta X_t + u_t$,  we know $Var(Y_t)=Var(X_t)=t\times \sigma^2_u$. 

Todo: fix this argument

- We also know that the OLS coefficient is $\hat{\beta}=\frac{\text{Cov}(X,Y)}{\text{Var}(X)}$. 
 - When $T \rightarrow \infty$ , the denominator term tends to infinity. 
 - Since this is in the denominator, we might get unusually large $\beta$ coefficients and $t$-statistics. 


## Consequences of Spurious Regression

- Hence, if you run a regression of $Y_t$ on $X_t$, where both are trending but unrelated:
  - You will likely find a **high R-squared**.
  - The **t-statistics** for the estimated coefficients will likely be **significant**.
  - You might conclude that X causes Y, but this conclusion would be **meaningless and misleading**.

## Slide 11: Detecting Spurious Regression

:::{.callout-note title="Definition: Breusch-Godfrey Test"}
The BG test assesses whether the error term, $\epsilon_t$, follows an AR(p) process. You, the researcher, choose the number of lags (p) to test for. The assumed model for the error is:
$$\epsilon_t = \rho_1 \epsilon_{t-1} + \rho_2 \epsilon_{t-2} + \dots + \rho_p \epsilon_{t-p} + v_t,$$

where $v_t$ is a white noise term. $H_0: \rho_1, \dots, \rho_p=0$, and $H_A$ is that at least one $\rho$ coefficient is non-zero: there is serial correlation up to some order $p$. 
:::

- **Rejection of the BG Test:** A key indicator of spurious regression is a rejection of the Breusch-Godfrey test. 
- **Rule of Thumb:** If the R-squared from the regression is greater than the DW statistic, you should be highly suspicious of a spurious relationship.

## Breusch-Godfrey Test

:::{.callout-note title="Breusch-Godfrey Test: Procedure"}
The BG test is performed in three simple steps:

- Run the Main Regression: First, estimate your original model using OLS, regardless of what it is. Obtain the series of residuals,
- Run an Auxiliary Regression. Run a new regression where the residual $e_t$ is the dependent variable. The independent variables are:
  - All the original regressors from the main model. 
  - The lagged residuals up to order $p$: e_{t-1}, \dots, e_{t-p}
- Obtain the R-squared: Calculate the R-squared from this auxiliary regression. 

---

- The Breusch-Godfrey test uses a Lagrange Multiplier (LM) statistic: $LM = T \times R^2_{aux}$, where $T$ is the number of observations used in the auxiliary regression and $R^2_{aux}$ is the $R^2$ from the auxiliary regression in Step 2.

- Under the null hypothesis of no serial correlation, the LM statistic follows a $\chi^2$ distribution with p degrees of freedom (where p is the number of lagged residuals included).

- If $LM > \chi^2_{crit}$, you reject the null hypothesis. You conclude there is significant evidence of serial correlation.
  
:::


## Avoiding Spurious Regression

- If the static model $Y_t = \alpha + \beta X_t + \epsilon_t$ is so dangerous, what is the alternative?

- **Differencing the Data:** If two variables are non-stationary, their first differences ($\Delta Y_t$ and $\Delta X_t$) may be stationary.
- **Dynamic models**: We need models that explicitly account for dynamics—the idea that a change in X can affect Y both today and in the future. 
  - This makes it more likely that the error term is white noise. 

## Distributed Lag (DL) Models

- Distributed lag models are used when we want to model how a change in an explanatory variable (X) affects the dependent variable (Y) over time, and we want to focus on potential **delayed effects**. 
  - **Example:** How does a change in government spending (X) affect GDP (Y) in the current quarter and in future quarters?

:::{.callout-note title="Definition: Distributed Lag Model"}
A finite distributed lag model of order $q$ is:

$$Y_t = \alpha + \beta_0 X_t + \beta_1 X_{t-1} + \dots + \beta_q X_{t-q} + u_t$$
where:

$\beta_0$: The **impact multiplier** - the immediate effect of a one-unit change in $X_t$ on $Y_t$.

$\beta_s (s > 0)$: The **dynamic multipliers** - the effect of a one-unit change in $X_{t-s}$ on $Y_t$.

:::

## Interpreting the DL Coefficients

$$Y_t = \alpha + \beta_0 X_t + \beta_1 X_{t-1} + \dots + \beta_q X_{t-q} + u_t$$

- **Short-Run Multiplier:** The total effect after a certain number of periods (e.g., β0 + β1).
- **Long-Run (or Total) Multiplier:** The total effect of a sustained one-unit change in X on Y. It is the sum of all the β coefficients: **Σ βs**.

## Slide 27: The Koyck (Geometric) Distributed Lag Model

*   **The Problem of Infinite Lags:** Sometimes, the effect of X on Y might persist indefinitely, requiring an infinite number of lags. This is not practical to estimate.
*   **Koyck's Assumption:** The Koyck model assumes that the coefficients (βs) decline geometrically: βs = β0λ^s, where 0 < λ < 1.
*   **A Simpler Form:** This assumption allows the infinite lag model to be transformed into a simpler model that can be estimated:
    **Yt = α(1-λ) + β0Xt + λYt-1 + vt** (This is a form of an ARDL model).

## Slide 28: The Autoregressive Distributed Lag (ARDL) Model

*   **A General and Powerful Model:** The ARDL model includes lags of both the dependent variable and the explanatory variable(s).

:::{.callout-note title="Definition: ARDL Model"}

An ARDL(p,q) model is defined as follows:

$$Y_t = \alpha + \sum(ρ_j \cdot Y_{t-j}) \text{ [from j=1 to p]} + \sum(\beta_j \cdot X_{t-m}) \text{ [from m=0 to q]} + u_t$$
:::

- It includes both autoregressive and distributed lag terms. 
- ARDL models are very flexible and can be used to estimate both short-run and long-run effects..

## Interpretation ARDL Coefficients

To do. 


## Other Topics in Time Series Econometrics

- This lecture has been an introduction. More advanced topics include:

  - **Testing for Stationarity:** Formal tests (like the Dickey-Fuller test) to determine if a series has a unit root.
  - **Cointegration:** A method for analyzing the long-run relationships between non-stationary variables (the proper way to handle trending variables that are truly related).
  - **Volatility Modeling (ARCH/GARCH):** Modeling the changing variance of a time series, which is crucial in finance.


# Summary

## What did we do?

- **Univariate Time Series**: We looked at ways to analyze a univariate time series, in particular, at:
  - **Autoregressive models**: Explain a variable using its own past values.
  -  **Moving Average models**: Explain a variable using past random shocks.

- **The Linear Model**: We focused on the possibility of using OLS on bivariate time series data. 
  - We ran into the problem of **spurious regression:** a misleading relationship between two non-stationary time series.


## What did we do? (Cont.)

**Distributed Lag (DL) Models:**
  - We focused on models that investigate the dynamic impact of an explanatory variable over time, allowing us to distinguish between short-run and long-run effects. These models make it more likely that the error term is white noise. 
  




# The End