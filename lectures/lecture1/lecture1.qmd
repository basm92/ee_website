---
title: "Empirical Economics"
subtitle: "Lecture 1: The Linear Model I"
format:
  revealjs: 
    theme: [default, theme.scss]
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 1 - The Linear Model I'
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")

# Helper to create data
create_norm_data <- function(mean = 0, sd = 1) {
  x <- seq(mean - 4 * sd, mean + 4 * sd, length.out = 400)
  y <- dnorm(x, mean, sd)
  data.frame(x, y)
}
# Helper function to plot the distribution with shaded rejection regions
plot_test <- function(norm_data, crit_vals, title) {
  
  # Create data for shading each tail INDEPENDENTLY
  shade_left_tail <- subset(norm_data, x < crit_vals[1])
  shade_right_tail <- subset(norm_data, x > crit_vals[2])

  ggplot(norm_data, aes(x = x, y = y)) +
    geom_line(color = "black", size = 1) +
    # Plot the left tail area (if it exists)
    geom_area(data = shade_left_tail, aes(y = y), fill = "firebrick", alpha = 0.5) +
    # Plot the right tail area (if it exists)
    geom_area(data = shade_right_tail, aes(y = y), fill = "firebrick", alpha = 0.5) +
    labs(
      title = title,
      x = "Test Statistic (e.g., Z-score)",
      y = ""
    ) +
    theme_minimal() +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      panel.grid = element_blank()
    )
}

# Generate some plausible data for wage and education
set.seed(123)
n <- 100
educ <- round(rnorm(n, 13, 2))
educ[educ < 8] <- 8
educ[educ > 20] <- 20
# u is the error term, correlated with nothing but adds noise
u <- rnorm(n, 0, 3.5)
# wage = beta0 + beta1*educ + u
wage <- 1.5 + 1.2 * educ + u
wage[wage < 2] <- 2
dat <- data.frame(wage = wage, educ = educ)

# Add experience as another variable
exper <- round(rnorm(n, 15, 5))
exper[exper < 0] <- 0
# wage_mlr = beta0 + beta1*educ + beta2*exper + u
wage_mlr <- 1.5 + 1.1 * educ + 0.2 * exper + rnorm(n, 0, 3)
dat_mlr <- data.frame(wage = wage_mlr, educ = educ, exper = exper)
```


# Course Set-Up

## Introduction

- Empirical Economics

- Two central aspects:
  - Econometrics and Econometric Theory
  - Empirical Practice in the form of programming

- Central course objective: to make you understand enough econometric theory, and have you obtain enough experience to :
  - Write a succesful thesis (in particular)
  - Conduct an empirical economics research project (in general)
  
## Course Layout and Schedule

- Simple course organization: 8 Lectures and 8 Tutorials
  - Always: 1 lecture (focused on theory) followed by 1 tutorial (recap and practice)
  - Tutorials: Wednesday and Thursday, 13:15-15:15-17:15 (see [mytimetable](http://mytimetable.uu.nl))
  
- One **mid-term** (7 Oct) and one **end-term exam** (7 Nov)
- Organized on Remindo.
- Featuring both multiple choice and open questions akin to the tutorials

## Info for students with a disability

- You can submit a request for examination accommodations such as exam time extension via OSIRIS Student tab Cases.

- Do this by Friday September 12 at the latest. We can then arrange everything before your first exams.

- Do you already have provisions from the UU? Then you do not need to submit a request via OSIRIS. 
  - More information can be found at [www.uu.nl/disability](www.uu.nl/disability)
  - Questions: students.uu.nl/studyadvisoruse 

## Lecture Schedule

- Everything is in your timetable, but..
- Lectures: On Friday
  - **One Exception**: No Lecture on Friday 17 October
  - **Instead**: Tuesday 11 October 11:00
- Lectures start at:
  - 13:15 (12 Sept, 19 Sept, 3 Oct, 24 October)
  - 15:15 (5 Sept, 26 Sept, 10 October)
  
- **Two Q&A Sessions Before Mid-term and End-term Exams**: 
  - Friday 3 October 15:00 (following lecture)
  - Friday 24 October 15:00 (following lecture)
  

# Outline

## Course Overview

- Linear Model I
- Linear Model II
- Time Series and Prediction
- Panel Data I
- Panel Data II
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Instrumental Variables


## What do we do today?

- First two lectures devoted to the linear model.

- Prequisite knowledge:
  - How do we model the processes that might have generated our data?
  - How do we summarize and describe data, and try to uncover what process may have generated it?
  - Probability and statistics

- This lecture and remaining lectures:
  - How do we uncover patterns between variables?
  - The linear model and further econometrics
  
- **Material:** Wooldridge Chapters 1 and 2

  
# Prediction vs. Causation

## Prediction vs. Causation 

- **Two Distinct Goals: Prediction and Causation**

- In econometrics, we build models with two primary objectives in mind:
  - Predicting an outcome or understanding the causal relationship between variables.
  - It is crucial to distinguish between these goals as they dictate our modeling approach and the interpretation of our results.

## Prediction

- Prediction is the world of $\hat{y}$
   - The primary goal of prediction is to forecast the value of a dependent variable (Y) as accurately as possible based on a set of independent variables (X).

- Our focus is on $\hat{y}$: This is the *predicted value* of Y generated by our model.
  - We build a model that minimizes the difference between the actual values (Y) and the predicted values ($\hat{y}$). This difference is known as the residual.
  - We are less concerned with the individual coefficients of the independent variables, as long as the model as a whole produces accurate forecasts. 

:::{.callout-tip title='Example: Prediction'}
Predicting next quarter's GDP growth using indicators like inflation, consumer confidence, and employment data. The main goal is the accuracy of the GDP forecast (ŷ), not necessarily the isolated impact of each indicator.
:::

## Causation 

- The goal of causal inference is to determine the specific impact of one variable on another, holding all other relevant factors constant.

- Our focus is on $\hat{\beta}$: This is the *estimated coefficient* of an independent variable. It quantifies the direction and magnitude of the relationship between an independent variable and the dependent variable.
  - We aim to obtain an unbiased estimate of the true underlying relationship ($\beta$). The "hat" on β means that it is an estimate derived from our sample data.
  - What matters most is whether our coefficient $\hat{\beta}$ is a "good approximation" of the true $\beta$. We are deeply concerned about issues like omitted variable bias, as they can lead to incorrect conclusions about the causal effect of a variable.

:::{.callout-tip title="Example: Causation"}
Estimating the effect of an additional year of education ($X$) on an individual's wages ($Y$). We are interested in the specific value of $\beta$ for education, which would tell us the expected increase in wages for one more year of schooling, assuming other factors are held constant.
:::

## Correlation vs. Causation

- If we are interested in causation, we have to be aware of the difference between _correlation_ and _causation_.

:::{.callout-tip title="Example: A Causal Effect" style="font-size:0.7em;"}

Banerjee and Duflo (2015) examined the causal effect of a comprehensive anti-poverty program. 

Can a "big push" program, which provides a combination of a productive asset, training, and support, have a lasting causal impact on the lives of the ultra-poor?

To answer this, the researchers used a Randomized Controlled Trial (RCT) across six countries.

- A large number of villages were randomly selected to either receive the program (the "treatment group") or not (the "control group").
- This random assignment helps ensure that, on average, the two groups were similar in all other aspects before the program began.
- Therefore, any significant differences observed between the two groups after the program can be causally attributed to the program itself, rather than other factors.

The study found that, even years after the program ended, the treatment group had significantly higher consumption levels and increased income and assets.

Because of the RCT design, the researchers could confidently conclude that the program *caused* these improvements.

:::

## Correlation

- Correlations can be [spurious](https://www.tylervigen.com/spurious-correlations), illustrating a key principle in economics: **correlation does not imply causation.**

:::{.callout-tip title='Example: Correlation without Causation' style='font-size: 0.7em;'}

A significant amount of modern finance research focuses on the relationship between a company's Environmental, Social, and Governance (ESG) scores and its financial performance.

Studies have documented a positive correlation between high ESG scores and strong financial performance. 

Companies that score well on environmental and social metrics also tend to be more profitable.

It does not necessarily mean that high ESG scores *cause* better financial performance. The relationship could be driven by other factors:

  - **Reverse Causality:** It might be that more profitable and successful firms have the resources to invest in improving their environmental and social impact, which in turn leads to higher ESG scores.
  - **Confounding Variables:** A third factor, such as high-quality management, could be responsible for both high ESG scores and strong financial performance. A well-managed company is likely to be both profitable and attentive to its social and environmental responsibilities.

:::


# What is Econometrics?

## What is Econometrics?

- **Econometrics** is the use of statistical methods to:

  1.  **Estimate** economic relationships.
  2.  **Test** economic theories.
  3.  **Evaluate** and implement government and business policy.
  4.  **Forecast** economic variables.

- It's where economic theory meets real-world data. 
- Theory proposes relationships (e.g., Law of Demand), but econometrics tells us the magnitude and statistical significance of these relationships.

## Why study econometrics?

- It allows you to **quantify** the relationships that you learn about in your other economics courses.
  
  - *By how much* does demand fall if we raise the price by 10%?
  - What is the effect of an additional year of education on future wages?
  - It helps distinguish between **correlation** and **causation**.
  - It is an essential tool for empirical research in economics and finance, and a highly valued skill in the job market.


## The Nature of Economic Data

- The type of data we have determines the econometric methods we should use.
  - **Cross-Sectional Data:** A snapshot of many different individuals, households, firms, countries, etc., at a *single point in time*.
  - **Time Series Data:** Observations on a single entity (e.g., a country, a company) collected over *multiple time periods*.
  - **Pooled Cross-Sections:** A combination of two or more cross-sectional datasets from different time periods. The individuals are different in each period.
  - **Panel (or Longitudinal) Data:** The *same* cross-sectional units are followed over time.
  
## Examples of Economic Data

:::{.callout-tip title="Examples of Economic Data"}
Cross-sectional data: A survey of 500 individuals in 2023, with data on their wage, education, gender, and age.

Time-series data: Data on Dutch GDP, inflation, and unemployment from 1950 to 2023.

Pooled cross-sections: A random survey of households in 1990, and another *different* random survey of households in 2020.

Panel data: Tracking the wage, education, and city of residence for the same 500 individuals every year from 2010 to 2020.
:::

# The Concept of a Model

## The Population Regression Function (PRF)

- In econometrics, we are interested in *relationships* between variables. 
  - Let's say we are interested in the relationship between wages ($y$) and years of education ($x$). Economic theory suggests a positive relationship.

- We can model the *average* wage for a given level of education. This is the **Population Regression Function (PRF)**:

$$
E(y | x) = \beta_0 + \beta_1 x
$$

- Where:
  - $E(y | x)$ is the **expected value (average) of y, given a value of x**.
  - $\beta_0$ is the **population intercept**.
  - $\beta_1$ is the **population slope**. These are unknown constants (parameters) that we want to estimate.

- The PRF represents the true, but unknown, relationship in the population.

## Example: Visualization of PRF

```{r}
#| fig.align: 'center'
#| fig.width: 8

# Define the PRF parameters
beta_0 <- 20   # Population intercept (wage when education = 0)
beta_1 <- 5    # Population slope (wage increase per year of education)

# Create a sequence of education levels
education <- seq(0, 20, length.out = 100)

# Calculate the expected wage (PRF)
expected_wage <- beta_0 + beta_1 * education

# Create a data frame for plotting
prf_data <- data.frame(education, expected_wage)

# Calculate positions for annotations
intercept_x <- 0
intercept_y <- beta_0
slope_x <- 15  # Choose a point to demonstrate slope
slope_y <- beta_0 + beta_1 * slope_x

# Plot the Population Regression Function with properly placed annotations
ggplot(prf_data, aes(x = education, y = expected_wage)) +
  geom_line(color = "blue", linewidth = 1.5) +
  
  # Add intercept annotation at actual intercept point
  geom_point(aes(x = intercept_x, y = intercept_y), color = "red", size = 3) +
  annotate("text", x = intercept_x + 1, y = intercept_y + 5,
           label = paste0("Intercept (β0) = ", beta_0),
           color = "darkred", hjust = 0) +
  
  # Add slope annotation showing rise over run
  geom_segment(aes(x = slope_x, xend = slope_x, 
                   y = beta_0 + beta_1 * (slope_x - 1), yend = slope_y),
               color = "green", linewidth = 1, linetype = "dashed") +
  geom_segment(aes(x = slope_x - 1, xend = slope_x, 
                   y = beta_0 + beta_1 * (slope_x - 1), yend = beta_0 + beta_1 * (slope_x - 1)),
               color = "green", linewidth = 1, linetype = "dashed") +
  annotate("text", x = slope_x + 1, y = (slope_y + beta_0 + beta_1 * (slope_x - 1))/2,
           label = paste0("Slope (β1) = ", beta_1, "\n(Δy/Δx = ", beta_1, "/1)"),
           color = "darkgreen", hjust = 0) +
  
  labs(title = "Population Regression Function (PRF)",
       subtitle = expression(paste("E(wage | education) = ", beta[0], " + ", beta[1], " education")),
       x = "Education Level (years)",
       y = "Expected Wage (Euro)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  scale_y_continuous(limits = c(0, max(expected_wage) * 1.1)) +
  scale_x_continuous(breaks = seq(0, 20, by = 2))
```

## The Stochastic Error Term

- Of course, not everyone with the same level of education has the same wage. Other factors matter (experience, innate ability, location, luck, etc.).
- We capture all these other unobserved factors in a **stochastic error term**, $u$.

- Our individual-level population model is:

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

- Where: 
  - $y_i$ is the wage of individual $i$.
  - $x_i$ is the education of individual $i$.
  - $u_i$ is the error term for individual $i$. It represents the deviation of individual $i$'s actual wage from the population average, $E(y|x_i)$.

## From Population to Sample

- We can't observe the entire population. We only have a sample of data.
- Our goal is to use the sample data to *estimate* the unknown population parameters $\beta_0$ and $\beta_1$.

- The **Sample Regression Function (SRF)** is our estimate of the PRF:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x
$$

- Where: 
  - $\hat{y}$ (y-hat) is the **predicted** or **fitted** value of y.
  - $\hat{\beta}_0$ and $\hat{\beta}_1$ are the **estimators** of $\beta_0$ and $\beta_1$. They are statistics calculated from our sample data.

## Example Regression in a Sample

:::{.callout-tip title="Example: Sample Data and Regression"}

```{r}
#| fig.align: 'center'
#| fig.width: 8
#| fig.height: 3

ggplot(dat, aes(x=educ, y=wage)) +
  geom_point(alpha=0.6) +
  labs(title="Sample Data and a Potential Regression Line", x="Education (years)", y="Wage ($/hour)") +
  theme_minimal() +
  geom_smooth(method="lm", se=FALSE, aes(color="OLS Line")) +
  scale_color_manual(name="", values="blue")
```

:::

# Derivation of the OLS Estimator

## Residuals

- How do we choose the "best" values for $\hat{\beta}_0$ and $\hat{\beta}_1$? We want a line that fits the data as closely as possible.

:::{.callout-note title="Definition: Residual"}

We define the **residual**, $\hat{u}_i$, as the difference between the actual value $y_i$ and the fitted value $\hat{y}_i$:
$$
\hat{u}_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)
$$

:::

- Note that the *residual* is *different from the error term $u_i$*. 
  - A residual is the observable difference between the actual data point and the value predicted by your regression model, whereas the unobserved error term $u_i$ is the theoretical difference between the actual data point and the true, unknown population value.

## OLS Method

- The **Ordinary Least Squares (OLS)** method chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the **Sum of Squared Residuals (SSR)**:

:::{.callout-note title="Definition: OLS Optimzation Problem"}

$$
\min_{\hat{\beta}_0, \hat{\beta}_1} SSR = \sum_{i=1}^{n} \hat{u}_i^2 = \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
$$
:::

- We square the residuals so that positive and negative errors don't cancel out, and because it penalizes larger errors more heavily.
- This loss function leads to a linear model and it has better statistical properties than other loss functions.

## Example: Residuals

```{r}
#| fig.align: 'center'
#| fig.width: 9
# Generate sample data
n <- 20
education <- round(runif(n, 8, 16))
wage <- 20 + 5 * education + rnorm(n, 0, 10)

data <- data.frame(education, wage)

# True PRF parameters (unknown in practice)
beta0_true <- 20
beta1_true <- 5

# Two potential regression lines (not optimal)
beta0_1 <- 15  # First guess: intercept too low
beta1_1 <- 6   # First guess: slope too steep

beta0_2 <- 25  # Second guess: intercept too high
beta1_2 <- 4   # Second guess: slope too flat

# Calculate predicted values and residuals for both models
data$pred1 <- beta0_1 + beta1_1 * data$education
data$resid1 <- data$wage - data$pred1
SSR1 <- sum(data$resid1^2)

data$pred2 <- beta0_2 + beta1_2 * data$education
data$resid2 <- data$wage - data$pred2
SSR2 <- sum(data$resid2^2)

# Create plots
plot1 <- ggplot(data, aes(x = education, y = wage)) +
  geom_point() +
  geom_abline(intercept = beta0_1, slope = beta1_1, color = "red", linewidth = 1) +
  geom_segment(aes(xend = education, yend = pred1), color = "blue", linetype = "dashed") +
  labs(title = paste("Potential Regression Line 1\nSSR =", round(SSR1, 1)),
       subtitle = sprintf("y = %.1f + %.1f x", beta0_1, beta1_1),
       x = "Education (years)", y = "Wage ($)") +
  theme_minimal() +
  ylim(min(wage)-10, max(wage)+10)

plot2 <- ggplot(data, aes(x = education, y = wage)) +
  geom_point() +
  geom_abline(intercept = beta0_2, slope = beta1_2, color = "darkgreen", linewidth = 1) +
  geom_segment(aes(xend = education, yend = pred2), color = "blue", linetype = "dashed") +
  labs(title = paste("Potential Regression Line 2\nSSR =", round(SSR2, 1)),
       subtitle = sprintf("y = %.1f + %.1f x", beta0_2, beta1_2),
       x = "Education (years)", y = "Wage ($)") +
  theme_minimal() +
  ylim(min(wage)-10, max(wage)+10)

# Arrange plots side by side
grid.arrange(plot1, plot2, ncol = 2,
             top = "Comparing Two Potential Regression Lines and Their Residuals",
             bottom = "Blue dashed lines show residuals (vertical distances from points to line)")
```

## Derivation of OLS

- To minimize the SSR, we use calculus: take the partial derivatives with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$ and set them to zero. 
- These are the **First Order Conditions (FOCs)**.

:::{.callout-note title="OLS First Order Conditions"}

1.  $\frac{\partial SSR}{\partial \hat{\beta}_0} = -2 \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \implies \sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0$
2.  $\frac{\partial SSR}{\partial \hat{\beta}_1} = -2 \sum_{i=1}^{n} x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \implies \sum x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0$

:::

## OLS Solution

- Solving this system of two equations for the two unknowns ($\hat{\beta}_0$, $\hat{\beta}_1$) gives the OLS estimator formulas:

:::{.callout-note title="Theorem: OLS Estimates for $\beta_0$ and $\beta_1$"}
$$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{\text{Sample Covariance}(x,y)}{\text{Sample Variance}(x)}
$$

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$

where $\bar{x}$ and $\bar{y}$ are the sample means of $x$ and $y$.

:::

- Note that for $\beta_1$ to be defined, the Sample Variance in $x \neq 0$, referred to as **no perfect multicollinearity**.

## Algebraic Properties of OLS

- The OLS estimators have some important algebraic properties that come directly from the FOCs:

- **The sum of the OLS residuals is zero:**
  - This implies that the sample average of the residuals, $\bar{u}$, is also zero.

$$\sum_{i=1}^{n} \hat{u}_i = 0$$
  


## Algebraic Properties of OLS (Cont.)

- **The sample covariance between the regressor ($x$) and the OLS residuals ($\hat{u}$) is zero:**
  - This means the part of $y$ that we can't explain with $x$ (the residual) is uncorrelated with $x$ in our sample.

$$\sum_{i=1}^{n} x_i \hat{u}_i = 0$$



- **The point $(\bar{x}, \bar{y})$ is always on the OLS regression line.**
  - From the formula $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$, we can write $\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}$.


## Interpreting OLS Coefficients

- Let's run our wage-education regression: $\text{Wage}_i = \hat{\beta}_0 + \hat{\beta}_1 \text{Educ}_i$

::: {.panel-tabset}

### R

```{r}
#| code-fold: true
#| echo: true
#| collapse: true

slr_model <- lm(wage ~ educ, data = dat)
# The coefficients are:
summary(slr_model)
```

### Python

```{python}
#| code-fold: true
#| echo: true
#| collapse: true

import statsmodels.api as sm
X = r.dat.educ
y = r.dat.wage
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()
print(model.summary())
```

### Stata

```{stata}
#| eval: false
#| code-fold: true
#| collapse: true

reg wage educ
```

:::

- So our estimated SRF is: $\widehat{Wage} = `r round(coef(slr_model)[1], 2)` + `r round(coef(slr_model)[2], 2)` \times Educ$

## Interpretation

- Slope ($\hat{\beta}_1 \approx `r round(coef(slr_model)[2], 2)`$): "For each additional year of education, we estimate the hourly wage to increase by **€1.15**, on average." This is the key policy parameter.
- Intercept ($\hat{\beta}_0 \approx `r round(coef(slr_model)[1], 2)`$): "For an individual with zero years of education, we predict an hourly wage of **€1.79**."

## Units and Functional Form

- The values of the coefficients depend on the units of measurement of $y$ and $x$. We've used a **level-level** model ($y$ and $x$ are in their natural units).

- Suppose we measured wage in cents instead of euros.
  - The new dependent variable is $Wage_{cents} = 100 \times Wage$.
  - The new regression would be:
    $\widehat{Wage_{cents}} = (100 \times \hat{\beta}_0) + (100 \times \hat{\beta}_1) \times Educ$
  - Both the intercept and slope would be 100 times larger. The *interpretation* is the same, just the units change ("an extra year of education increases wage by 125 cents").

## Units and Functional Form (Cont.)

- What if we measured education in months instead of years?
  - The interpretation of $\hat{\beta}_1$ would become "the estimated change in wage for an additional *month* of education." The coefficient value would be $\frac{1}{12}$ of its original value:

:::{.callout-tip title="Example: Education in Months"}
From our definition, $Educ_{years} = \frac{1}{12} Educ_{months}$. Let's substitute this into the original estimated equation:

\begin{align*}
\widehat{Wage} &= \hat{\beta}_0 + \hat{\beta}_1 Educ_{years} \\
&= \hat{\beta}_0 + \hat{\beta}_1 \left( \frac{1}{12} Educ_{months} \right) \\
&= \hat{\beta}_0 + \left( \frac{\hat{\beta}_1}{12} \right) Educ_{months}
\end{align*}
:::

## Why use different functional forms?

- So far, we've assumed a linear relationship: a one-unit change in $x$ leads to the same change in $y$, regardless of the value of $x$.
  - But often, relationships are not linear. We use transformations (like logarithms) to:
  - **Model Non-Linear Relationships**: Capture effects that are proportional or diminishing.
  - **Change the Interpretation**: Analyze percentage changes instead of unit changes.
  - **Improve Statistical Properties**: Stabilize the variance of the error term or make the distribution of a variable more symmetric.


```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 12
#| fig-height: 4
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# --- Linear Relationship Plot ---

# Create data for a linear relationship
linear_data <- data.frame(x = 1:10, y = 2 * (1:10) + 1)

# Generate the ggplot object for the linear plot
linear_plot <- ggplot(linear_data, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  # Add segments and annotations to show constant change in y
  geom_segment(aes(x = 2, y = 5, xend = 4, yend = 5), linetype = "dashed", color = "red") +
  geom_segment(aes(x = 4, y = 5, xend = 4, yend = 9), linetype = "dashed", color = "red") +
  annotate("text", x = 3, y = 4.5, label = "Δx = 2", color = "red") +
  annotate("text", x = 4.5, y = 7, label = "Δy = 4", color = "red") +
  geom_segment(aes(x = 6, y = 13, xend = 8, yend = 13), linetype = "dashed", color = "red") +
  geom_segment(aes(x = 8, y = 13, xend = 8, yend = 17), linetype = "dashed", color = "red") +
  annotate("text", x = 7, y = 12.5, label = "Δx = 2", color = "red") +
  annotate("text", x = 8.5, y = 15, label = "Δy = 4", color = "red") +
  labs(title = "Linear Relationship",
       subtitle = "Constant change in y for a given change in x",
       x = "X",
       y = "Y") +
  theme_minimal()

# --- Logarithmic Relationship Plot ---

# Create data for a logarithmic relationship
log_data <- data.frame(x = 1:10, y = log(1:10))

# Generate the ggplot object for the logarithmic plot
log_plot <- ggplot(log_data, aes(x = x, y = y)) +
  geom_line(color = "green") +
  geom_point(color = "green") +
  # Add segments and annotations to show varying change in y
  geom_segment(aes(x = 2, y = log(2), xend = 4, yend = log(2)), linetype = "dashed", color = "purple") +
  geom_segment(aes(x = 4, y = log(2), xend = 4, yend = log(4)), linetype = "dashed", color = "purple") +
  annotate("text", x = 3, y = log(2) - 0.1, label = "Δx = 2", color = "purple") +
  annotate("text", x = 4.5, y = (log(2) + log(4))/2, label = paste("Δy ≈", round(log(4) - log(2), 2)), color = "purple") +
  geom_segment(aes(x = 8, y = log(8), xend = 10, yend = log(8)), linetype = "dashed", color = "purple") +
  geom_segment(aes(x = 10, y = log(8), xend = 10, yend = log(10)), linetype = "dashed", color = "purple") +
  annotate("text", x = 9, y = log(8) - 0.1, label = "Δx = 2", color = "purple") +
  annotate("text", x = 10.5, y = (log(8) + log(10))/2, label = paste("Δy ≈", round(log(10) - log(8), 2)), color = "purple") +
  labs(title = "Logarithmic Relationship",
       subtitle = "Change in y depends on the value of x",
       x = "X",
       y = "Y") +
  theme_minimal()

# --- Arrange Plots Side-by-Side ---

# Arrange the two plots in a grid with two columns
grid.arrange(linear_plot, log_plot, ncol = 2)
```


## The Log-Level Model: $\log(y)$ on $x$

- Here, we transform the dependent variable $y$: $\log(y) = \beta_0 + \beta_1 x + u$

- **Interpretation of $\beta_1$**: A one-unit increase in $x$ is associated with a $(100 \times \beta_1)\%$ change in $y$.

:::{.callout-note title="Interpretation of $\beta$ in the Log-Level Model"}
To see this, take the derivative of the equation with respect to $x$:
  $$ \frac{d(\log(y))}{dx} = \beta_1 $$

Recall the calculus rule/approximation: for small changes, $\Delta \log(y) \approx \frac{\Delta y}{y}$.

For a one-unit change in $x$ ($\Delta x = 1$):
  $$ \beta_1 = \frac{\Delta \log(y)}{\Delta x} \approx \frac{\Delta y / y}{1} $$
  
:::

## Example Log-Level

:::{.callout-tip title="Example: Log-Level Interpretation"}

In a log-level model, $\beta_1$ is the *proportional* change in $y$.

We multiply by 100 to get a percentage.

If $\widehat{\log(Wage)} = 1.5 + 0.08 \times Educ$, an additional year of education is associated with an approximate $0.08 \times 100 = 8\%$ increase in wage.

:::

## The Level-Log Model: $y$ on $\log(x)$

- Here, we transform the independent variable $x$: $y = \beta_0 + \beta_1 \log(x) + u$

- **Interpretation of $\beta_1$**: A 1% increase in $x$ is associated with a $(\beta_1 / 100)$ unit change in $y$.

:::{.callout-note title="Interpretation of $\beta$ in Level-Log Model"}

To see this, take the derivative of the equation with respect to $\log(x)$:
  $$ \frac{dy}{d(\log(x))} = \beta_1 $$
  
A change in $\log(x)$ is approximately the proportional change in $x$: $\Delta \log(x) \approx \frac{\Delta x}{x}$.

- So, $\Delta y \approx \beta_1 \Delta(\log(x)) \approx \beta_1 \frac{\Delta x}{x}$.
- If we consider a 1% change in $x$, then $\frac{\Delta x}{x} = 0.01$.
- The resulting change in $y$ is: $\Delta y \approx \beta_1 \times (0.01) = \frac{\beta_1}{100}$.
  
:::

## Example Level-Log Model

:::{.callout-tip title="Example: Level-Log Model"}

Suppose that the estimated regression model linking advertising expenditure to monthly sales revenue is:

  $$
    \text{Monthly Sales Revenue} = 50 + 12 \times \log(\text{Monthly Advertising Spending})
  $$

In this model, Monthly Sales Revenue (Y) is measured in thousands of euros (Level), and Monthly Advertising Spend (X) is measured in euros (Log).

A 1% increase in Monthly Advertising Spend is associated with a $12/100 = 0.12$ increase in Monthly Sales Revenue. Since Sales Revenue is measured in thousands of euros, a 1% increase in advertising spend is associated with a **€120 increase** in monthly sales revenue ($0.12 \times €1,000 = €120$).
:::

## The Log-Log Model: $\log(y)$ on $\log(x)$

- This model is very common in economics: $\log(y) = \beta_0 + \beta_1 \log(x) + u$

:::{.callout-note title="Interpretation of $\beta$ in the Log-Log Model"}

A 1% increase in $x$ is associated with a $\beta_1\%$ change in $y$. To see this, from the model, we can write:
  $$ \beta_1 = \frac{d(\log(y))}{d(\log(x))} $$
  
Using the same approximations as before:
  $$ \beta_1 \approx \frac{\Delta y / y}{\Delta x / x} = \frac{\%\Delta y}{\%\Delta x} $$
  
- If we set the percentage change in $x$ to 1% ($\%\Delta x=1$), then the percentage change in $y$ is just $\beta_1$.

:::

## Example Log-Log Form

:::{.callout-tip title="Example: Interpretation of $\beta_1$ in the Log-Log Model"}

Suppose we have estimated $\log(\text{Sales}) = 4.8 - 1.2 \times \log(\text{Price})$ for a product. Then, a 1% increase in price is associated with a 1.2% decrease in sales. 

:::

## Other Forms: Polynomials

- We can also add polynomial terms (like $x^2$, $x^3$, etc.) to capture more complex non-linear patterns, such as diminishing returns.
  - **Model (Quadratic):** $y = \beta_0 + \beta_1 x + \beta_2 x^2 + u$
  - This is another example where if $x_i$ differs among individuals, different individuals have different marginal effects. 

:::{.callout-note title="Interpretation of the Quadratic Model"}
The effect of a change in $x$ on $y$ now *depends on the level of $x$*.

The marginal effect of $x$ on $y$ is the derivative with respect to $x$:
  $$ \frac{\Delta y}{\Delta x} \approx \frac{dy}{dx} = \beta_1 + 2 \beta_2 x $$
  
A one-unit change in $x$ is associated with a change in $y$ of approximately $\beta_1 + 2 \beta_2 x$.

:::



## Example Polynomial

:::{.callout-tip title="Example: Polynomial Regression"}

Suppose we have estimated a model for annual income based on a worker's age:

$\widehat{Income} = 20,000 + 1,500 \times Age - 20 \times Age^2$.

The effect of the *first* year of work experience (e.g., going from age 20 to 21) is approximately:
$1,500 + 2(-20)(20) = 1,500 - 800 = \$700$.

The effect of gaining one more year of experience when a worker is 40 (i.e., going from age 40 to 41) is:
$1,500 + 2(-20)(40) = 1,500 - 1,600 = -\$100$.

This captures the common life-cycle pattern of earnings: income rises with age and experience, but at a decreasing rate, and may eventually begin to decline after a certain point. This demonstrates the *diminishing returns* to age and experience on income.
:::

## Dummy Variables

- A dummy variable (or indicator variable), $D_i$, is a special variable: a binary variable that takes the value 1 if an observation $i$ belongs to a specific category, and 0 otherwise. 
  - This allows us to incorporate qualitative information (e.g., gender, policy status, geographic region) into a regression model.

:::{.callout-tip title="Example: Wage Conditional on Gender Dummy"}

Consider a regression of wage on a single dummy variable for gender:

  $$
    Wage_i = \beta_0 + \beta_1 Female_i + u_i
  $$
  
where $Female_i = 1$ if person `i` is female, and $Female_i = 0$ if male.

:::

## Interpretation 

- To interpret the coefficients, we take the conditional expectation of `wage` for each group:

:::{.callout-tip title="Example: Wage on Gender (Cont.)"}
- **For Males ($Female_i = 0$):**
  - $E[Wage_i | Female_i=0] = \beta_0 + \beta_1(0) = \beta_0$
  - The expected wage for the reference group (males) is simply the intercept, $\beta_0$.

- **For Females ($Female_i = 1$):**
  - $E[Wage_i | Female_i=1] = \beta_0 + \beta_1(1) = \beta_0 + \beta_1$
  - The expected wage for the treatment group (females) is $\beta_0 + \beta_1$.
:::

- The coefficient $\beta_1$ represents the **difference** in the expected outcome between the two groups:

  $$
    E[Wage_i | Female_i=1] - E[wage_i | Female_i=0] = (\beta_0 + \beta_1) - \beta_0 = \beta_1
  $$

- Now, $\beta_1$ is the average difference in wages between females and males.

## Summary of Interpretations

:::{style="font-size: 1.2em;"}

| Model Name | Equation | Interpretation of $\hat{\beta}_1$ |
| :--- | :--- | :--- |
| **Level-Level** | $y = \beta_0 + \beta_1 x$ | A 1-unit change in $x$ leads to a $\hat{\beta}_1$ unit change in $y$. |
| **Log-Level** | $\log(y) = \beta_0 + \beta_1 x$ | A 1-unit change in $x$ leads to a $(100 \times \hat{\beta}_1)\%$ change in $y$.^[This is only valid for a small $\beta_1$.] |
| **Level-Log** | $y = \beta_0 + \beta_1 \log(x)$ | A 1% change in $x$ leads to a $(\hat{\beta}_1/100)$ unit change in $y$. |
| **Log-Log** | $\log(y) = \beta_0 + \beta_1 \log(x)$ | A 1% change in $x$ leads to a $\hat{\beta}_1\%$ change in $y$. |
| **Polynomial** | $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots$ | A 1-unit change in $x$ leads to a $\hat{\beta}_1 + 2 \hat{\beta}_2 x$ change in $y$. |
| **Dummies** | $y = \beta_0 + \beta_1 D_i$ | Relative to the reference category, the 1 category has a $\hat{\beta}_1$ lower/higher $y$. |

:::

# Goodness of Fit

## Goodness-of-Fit

- How well does our estimated line explain the variation in our dependent variable, $y$?

- We can partition the total variation in $y$ into two parts: the part explained by the model, and the part that is not explained.

:::{.callout-note title="Partition of Variation in $Y$"}
**SST (Total Sum of Squares):** Total variation in $y$.
    $SST = \sum (y_i - \bar{y})^2$
    
**SSE (Explained Sum of Squares):** Variation explained by the regression.
    $SSE = \sum (\hat{y}_i - \bar{y})^2$
    
**SSR (Sum of Squared Residuals):** Unexplained variation.
    $SSR = \sum \hat{u}_i^2$

If the regression equation includes a constant term, it is a mathematical property that **SST = SSE + SSR**.

:::

## Goodness-of-Fit: $R^2$

- We want to encapsulate "goodness-of-fit" into one number. 

:::{.callout-note title="Definition: $R^2$"}

The **R-squared** measures the proportion of the total sample variation in $y$ that is "explained" by the regression model.

$$
R^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}
$$

:::

- $R^2$ is always between 0 and 1.
- A higher $R^2$ means the model fits the data better in-sample.
- **Caution:** A high $R^2$ is not the ultimate goal of econometrics! We care more about getting an unbiased estimate of the causal effect $\beta_1$.

## $R^2$ Visualization

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 9
# Load the required libraries
library(ggplot2)
library(patchwork)

# --- Create the High R-squared Dataset ---

# Set a seed for reproducibility
set.seed(42)

# Generate data with a strong positive correlation
x_high <- 1:100
y_high <- 2 * x_high + rnorm(100, mean = 0, sd = 15)
high_r_sq_data <- data.frame(x = x_high, y = y_high)

# Calculate the R-squared for the high correlation data
model_high <- lm(y ~ x, data = high_r_sq_data)
r_sq_high <- summary(model_high)$r.squared

# --- Create the Low R-squared Dataset ---

# Set a seed for reproducibility
set.seed(42)

# Generate data with a weak correlation
x_low <- 1:100
y_low <- 0.1 * x_low + rnorm(100, mean = 0, sd = 15)
low_r_sq_data <- data.frame(x = x_low, y = y_low)

# Calculate the R-squared for the low correlation data
model_low <- lm(y ~ x, data = low_r_sq_data)
r_sq_low <- summary(model_low)$r.squared

# --- Create the Plots with ggplot2 ---

# Plot for High R-squared
plot_high_r_sq <- ggplot(high_r_sq_data, aes(x = x, y = y)) +
  geom_point(color = "blue", alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  labs(
    title = "High R-squared",
    subtitle = paste("R² =", round(r_sq_high, 3)),
    x = "Independent Variable",
    y = "Dependent Variable"
  ) +
  theme_minimal()

# Plot for Low R-squared
plot_low_r_sq <- ggplot(low_r_sq_data, aes(x = x, y = y)) +
  geom_point(color = "green", alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkorange") +
  labs(
    title = "Low R-squared",
    subtitle = paste("R² =", round(r_sq_low, 3)),
    x = "Independent Variable",
    y = "Dependent Variable"
  ) +
  theme_minimal()

# --- Arrange the Plots Side-by-Side ---

# Use the patchwork package to combine the plots
plot_high_r_sq + plot_low_r_sq
```

# OLS Classical Assumptions

## Unbiasedness

- A key desirable property of an estimator is **unbiasedness**. 
- The objective of a regression is to say something about the population parameters $\beta$. However, we only have a sample equivalent, $\hat{\beta}$ at our disposal.
  - This estimate goes paired with some uncertainty. 

:::{.callout-note title="Definition: Unbiasedness"}
An OLS estimator is considered unbiased if its expected value, across many hypothetical samples, is equal to the true population parameter it is intended to estimate. Mathematically, for a regression coefficient $\beta$, its OLS estimator $\hat{\beta}$ is unbiased if:

  $$
    E[\hat{\beta}] = \beta
  $$

:::

## Unbiasedness Implication

- This does not imply that an estimate from any single sample will be the true population parameter. 
- Instead, it means that if we were to draw an infinite number of random samples from the population and compute the OLS estimate for each, the average of these estimates would be equal to the true value of $\beta$.
- This property ensures that, on average, the OLS procedure does not systematically overestimate or underestimate the true parameter.

## The SLR Assumptions

- When does unbiasedness hold?
  - For our OLS estimates to have desirable statistical properties, certain assumptions must hold. These are the **SLR Assumptions**.

:::{.callout-note title="SLR Assumptions"}
- Assumption 1: Linearity in Parameters. The population model is $y = \beta_0 + \beta_1 x + u$.
- Assumption 2: Random Sampling. The data $(x_i, y_i)$ are a random sample from the population described by the model.
- Assumption 3: Sample Variation in $x$. The values of $x_i$ in the sample are not all the same. This is the **no perfect collinearity** assumption. 
- Assumption 4: Zero Conditional Mean. $E(u|x) = 0$. The average value of the unobserved factors is unrelated to the value of $x$.
:::

## Unbiasedness of OLS

:::{.callout-note title="Theorem: Unbiasedness of OLS"}

Under assumptions **SLR.1 through SLR.4**, the OLS estimators are **unbiased**.

$$
E(\hat{\beta}_0) = \beta_0 \quad \text{and} \quad E(\hat{\beta}_1) = \beta_1
$$
:::

- **What does this mean?**
  - Unbiasedness is a property of the OLS estimator given the assumptions.
    - If we could draw many, many random samples (with the same $n$) from the population and calculate $\hat{\beta}_1$ for each sample, the *average* of all these estimates would be equal to the true population parameter, $\beta_1$.
  - Our estimate from any single sample might be higher or lower than the true value, but on average, we get it right.
    - This property relies critically on the Zero Conditional Mean assumption (SLR.4). If SLR.4 fails, OLS is biased.

## Violation of Unbiasedness

- Assumption 4, $E(u|x) = 0$, is the most important assumption for establishing **causality**. 
  - It means that the explanatory variable ($x$) must not be correlated with any of the unobserved factors ($u$) that affect the dependent variable ($y$).

- When is unbiasedness violated? 
- An important reason arises when a relevant explanatory variable is excluded from the model, referred to as **Omitted Variable Bias**.  
  - This omission leads to biased and inconsistent estimates for the coefficients of the included variables.

- For OVB to exist, two conditions must be met:

  1.  The omitted variable must be a determinant of the dependent variable (i.e., its true coefficient is not zero).
  2.  The omitted variable must be correlated with at least one of the included independent variables.

- When these conditions hold, the OLS estimator for the included variable's coefficient mistakenly incorporates the effect of the omitted variable, leading to a biased result. 
  - This violates the crucial OLS assumption that the error term is uncorrelated with the regressors.

## Prediction of Direction of Bias

- The direction of the bias is determined by the signs of these two key relationships.

:::{.callout-note title="Omitted Variable Bias Definition"}

Consider the true regression model: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u$, where $x_2$ is the omitted variable. 

Instead, we estimate the simpler model $y = \alpha_0 + \alpha_1 x_1 + v$. The bias in the estimate of $\alpha_1$ can be expressed as:

  $$
    \text{Bias} = E[\hat{\alpha_1}] - \beta_1 = \beta_2 \cdot \delta_1
  $$

Where:

  - $\beta_2$ is the true coefficient of the omitted variable ($X_2$ in the correctly specified model. This represents the partial effect of $X_2$ on Y.
  - $\delta_1$ is the coefficient from an auxiliary regression of the omitted variable ($X_2$) on the included variable ($X_1$): $X_2 = \delta_0 + \delta_1 X_1 + w$. This represents the correlation between $X_1$ and $X_2$. 

:::

## Examples of OVB

- The direction of the bias can be predicted by the product of the signs of $\beta_2$ and $\delta_1$: 

:::{.callout-tip title="Examples: OVB"}
Suppose we estimate $\text{Wage} = \beta_0 + \beta_1 \text{Education} + u$, but there is an omitted variable $X_2$, Innate Ability. The resulting coefficient will be an _oversestimate_ of the true effect: 

  - Sign of $\beta_2 (Effect of $X_2$ on $Y$): The sign is **positive (+)**. It is widely accepted that, holding education constant, individuals with higher innate ability tend to earn higher wages. This is because ability is a component of a worker's overall productivity.
  - Sign of $\delta_1$ (Correlation of $X_2$ with $X_1$): The sign is **positive (+)**. Individuals with higher innate ability are often more likely to attain higher levels of education. They may find schooling less challenging and have greater incentives to pursue further education.
  - The formula for the bias is $\text{Bias}=\beta_2 \cdot \delta_1$. Therefore, the predicted direction is `(+) * (+) = +`.
  
:::

## Examples of OVB (Cont.)

:::{.callout-tip title="Examples: OVB"}

Suppose we estimate the model $\text{Educational_Outcome} = \beta_0 + \beta_1 \text{Class Size} + u$, with the omitted variable $X_2$ being a student's socioeconomic background or level of need. The resulting bias is positive (leading to an underestimation of a negative effect):

- Sign of $\beta_2$ (Effect of $X_2$ on $Y$): The sign is **negative (-)**. Students from disadvantaged socioeconomic backgrounds or with higher needs tend to have, on average, lower educational outcomes. This can be due to fewer resources at home, less parental support, and other related challenges.
- Sign of $\delta_1$ (Correlation of $X_2$ with $X_1$): The sign is **negative (-)**. School districts often purposefully create smaller classes for students who require more attention, such as those from lower-income families, or students with learning disabilities. Therefore, a higher prevalence of disadvantaged students is correlated with smaller class sizes.
- Therefore, the predicted direction is `(-) * (-) = +`.

::: 

## Examples OVB (Cont.)

:::{.callout-tip title="Examples: OVB"}

Suppose we estimate $\text{Profits} = \beta_0 + \beta_1\text{Firm Size} + u$, with the omitted variable $X_2$ being Quality of Management. The resulting bias is _positive_, resulting in overestimation of the effect. 

-  Sign of $\beta_2$ (Effect of $X_2$ on $Y$): The sign is **positive (+)**. High-quality management is a critical factor in a firm's success and is directly and positively related to higher profitability. Better managers make more effective strategic, operational, and financial decisions.
- Sign of $\delta_1$ (Correlation of $X_2$ with $X_1$):The sign is **positive (+)**. Larger firms are often able to attract and retain higher-quality managerial talent. They can offer more competitive compensation packages, provide more extensive career opportunities, and may have more sophisticated recruitment processes. Conversely, better management may also lead to firm growth, increasing its size.
- Therefore, the predicted direction is `(+) * (+) = +`.

:::

    
## Visualization Unbiasedness

- The true values for the intercept are $\beta_0=2$ and the slope $\beta_1=5$ of our hypothetical linear model. 
- We then perform an OLS regression 1,000 times. 
  - In each iteration, it generates a random sample of 100 observations for the variables $x$ and $y$ based on the true model, including a random error term.
  - For each of these samples, we run a linear regression function and store and plot the estimated coefficient for $x$:

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 7
# Set a seed for reproducibility of the random numbers
set.seed(123)

# Define the true parameters of our linear model
# We assume the true relationship between y and x is: y = beta_0 + beta_1*x + u
true_beta_0 <- 2
true_beta_1 <- 5

# Set the number of simulations (i.e., the number of samples to draw)
n_simulations <- 1000

# Set the sample size for each simulation
sample_size <- 100

# Create an empty vector to store the estimated coefficients for beta_1 from each simulation
estimated_beta_1 <- numeric(n_simulations)

# Loop through the number of simulations
for (i in 1:n_simulations) {
  # Generate the independent variable x from a uniform distribution
  x <- runif(sample_size, min = 1, max = 10)

  # Generate the error term u from a normal distribution with mean 0
  u <- rnorm(sample_size, mean = 0, sd = 2)

  # Generate the dependent variable y based on the true model
  y <- true_beta_0 + true_beta_1 * x + u

  # Fit a linear model to the generated sample data
  model <- lm(y ~ x)

  # Extract and store the estimated coefficient for x (beta_1)
  estimated_beta_1[i] <- coef(model)[2]
}

# Now, let's visualize the results
# Create a histogram of the estimated beta_1 coefficients
hist(estimated_beta_1,
     main = "Distribution of OLS Estimates for Beta_1",
     xlab = "Estimated Beta_1",
     col = "lightblue",
     freq = FALSE, # Plot density instead of frequency
     breaks = 30)

# Add a vertical line for the true value of beta_1
abline(v = true_beta_1, col = "red", lwd = 2)

# Add a vertical line for the mean of the estimated beta_1 coefficients
abline(v = mean(estimated_beta_1), col = "blue", lwd = 2, lty = 2)

# Add a legend to the graph
legend("topright",
       legend = c("True Beta_1", "Mean of Estimates"),
       col = c("red", "blue"),
       lwd = 2,
       lty = c(1, 2))

# Print the mean of the estimated coefficients to show it's close to the true value
# print(paste("True Beta_1:", true_beta_1))
# print(paste("Mean of Estimated Beta_1:", round(mean(estimated_beta_1), 4)))
```
    
    
## Variance of OLS Estimators

- We also want our estimators to be precise, meaning they don't vary too much from sample to sample. This is measured by their sampling variance.

:::{.callout-note title="Theorem: Variance of the OLS Estimator"}

Under assumptions **SLR.1 through SLR.5** (all four SLR assumptions plus _homoskedasticity_), the estimated variance of the OLS slope estimator is:

$$
\widehat{Var}(\hat{\beta}_1) = \frac{\hat{\sigma}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\hat{\sigma}^2}{SST_x}
$$

:::

## Variance of OLS Estimators (Cont.)

- Let's forget regression for a second. 
  - How much does a simple sample mean ($\bar{x}$) wobble?
  - Its variance (under random sampling) is famously simple: $Var(\bar{x}) = \frac{\sigma^2}{n}$
  
- This formula is driven by two intuitive ideas:
  - $\sigma^2$ is the variance of the underlying population. It's the inherent "noisiness" or "spread" of the data points themselves.
  - $n$ is your sample size. It's how much data you have to "anchor" your estimate.
  
- Looking at $\widehat{Var}(\hat{\beta}_1) = \frac{\hat{\sigma}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\hat{\sigma}^2}{SST_x}$ tells us the exact same.. 

## Determinants of the Variance

- What determines the precision of our estimate?
  - **The error variance, $\sigma^2$**: More "noise" in the relationship (larger $\sigma^2$) leads to a larger variance for $\hat{\beta}_1$.
  - **The total sample variation in $X$, $SST_x$**: More variation in our explanatory variable ($x$) leads to a *smaller* variance for $\hat{\beta}_1$. We learn more about the slope when our $x$ values are more spread out.
  - **The sample size, $n$**: A larger sample size generally increases $SST_x$, which *decreases* the variance of $\hat{\beta}_1$.
  
## Variance Illustration (Dependence on $\hat{\sigma}$)

- We plot the distribution of the 1,000 estimated coefficients, which is centered around the true value of 5 (the solid red line).
  - In the left panel below, we have a *low-variance estimator* ($\sigma=1$)
    - The histogram on the right is much narrower and more peaked. The estimates are tightly clustered around the true value. 
  - In the right panel below, we have a *high-variance estimator* ($\sigma=8$)
    - The histogram on the right is visibly wider and flatter. This indicates a larger spread in the estimated coefficients. Any single estimate from a sample of this size could be quite far from the true value. 
    
    
```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 15
# --- Model & Simulation Parameters ---

# Define the true parameters of our linear model: y = beta_0 + beta_1*x + u
true_beta_0 <- 2
true_beta_1 <- 5

# Set the number of simulations
n_simulations <- 1000

# Set a constant sample size for both scenarios
sample_size <- 100

# Set the standard deviation (sigma) of the error term for the two scenarios
low_sigma <- 1   # For the low-variance estimator
high_sigma <- 8  # For the high-variance estimator

# --- Simulation Function ---

# Create a function to run the simulation for a given error variance (sigma)
run_simulation <- function(sample_size, sigma, n_sims, beta_0, beta_1) {
  
  # Create an empty vector to store the estimated coefficients for beta_1
  estimated_beta_1 <- numeric(n_sims)

  # Loop through the number of simulations
  for (i in 1:n_sims) {
    # Generate the independent variable x
    x <- runif(sample_size, min = 1, max = 10)

    # Generate the error term u with the specified sigma
    u <- rnorm(sample_size, mean = 0, sd = sigma)

    # Generate the dependent variable y based on the true model
    y <- beta_0 + beta_1 * x + u

    # Fit a linear model to the generated sample data
    model <- lm(y ~ x)

    # Store the estimated coefficient for beta_1
    estimated_beta_1[i] <- coef(model)[2]
  }
  
  return(estimated_beta_1)
}

# --- Run Both Simulations ---

# Run simulation for the low-variance case (small sigma)
low_variance_estimates <- run_simulation(sample_size, low_sigma, n_simulations, true_beta_0, true_beta_1)

# Run simulation for the high-variance case (large sigma)
high_variance_estimates <- run_simulation(sample_size, high_sigma, n_simulations, true_beta_0, true_beta_1)

# --- Plotting the Results ---

# Set up the plotting area to have two subplots side-by-side
par(mfrow = c(1, 2))

# --- Subplot 1: Low-Variance Estimator ---
hist(low_variance_estimates,
     main = paste("Low-Variance Estimator (sigma = ", low_sigma, ")", sep=""),
     xlab = "Estimated Beta_1",
     col = "lightblue",
     freq = FALSE,
     breaks = 30,
     xlim = c(3.5, 6.5)) # Set common x-axis limits for comparison

# Add a vertical line for the true value of beta_1
abline(v = true_beta_1, col = "red", lwd = 2)

# Add a vertical line for the mean of the estimated beta_1 coefficients
abline(v = mean(low_variance_estimates), col = "blue", lwd = 2, lty = 2)

# Add a legend
legend("topright",
       legend = c("True Beta_1", "Mean of Estimates"),
       col = c("red", "blue"),
       lwd = 2,
       lty = c(1, 2),
       cex = 0.7)

# --- Subplot 2: High-Variance Estimator ---
hist(high_variance_estimates,
     main = paste("High-Variance Estimator (sigma = ", high_sigma, ")", sep=""),
     xlab = "Estimated Beta_1",
     col = "lightcoral",
     freq = FALSE,
     breaks = 30,
     xlim = c(3.5, 6.5)) # Set common x-axis limits for comparison

# Add a vertical line for the true value of beta_1
abline(v = true_beta_1, col = "red", lwd = 2)

# Add a vertical line for the mean of the estimated beta_1 coefficients
abline(v = mean(high_variance_estimates), col = "blue", lwd = 2, lty = 2)

# Add a legend
legend("topright",
       legend = c("True Beta_1", "Mean of Estimates"),
       col = c("red", "blue"),
       lwd = 2,
       lty = c(1, 2),
       cex = 0.7)

# Reset the plotting layout to the default (1 plot)
par(mfrow = c(1, 1))

```


## Variance Illustration (Dependence on $N$)

- We plot the distribution of the 1,000 estimated coefficients, which is centered around the true value of 5 (the solid red line).
  - In the left panel below, we have a *high-variance estimator* (n=30)
    - The histogram on the left is visibly wider and flatter. This indicates a larger spread in the estimated coefficients. Any single estimate from a sample of this size could be quite far from the true value. 
  - In the right panel below, we have a *low-variance estimator* (n=500)
    - The histogram on the right is much narrower and more peaked. The estimates are tightly clustered around the true value. 


```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 15

# Set a seed for reproducibility
set.seed(456)

# --- Parameters for Simulation ---

# Define the true parameters of our linear model: y = beta_0 + beta_1*x + u
true_beta_0 <- 2
true_beta_1 <- 5

# Set the number of simulations
n_simulations <- 1000

# Set sample sizes for the two scenarios
small_sample_size <- 30   # For the high-variance estimator
large_sample_size <- 500  # For the low-variance estimator

# --- Simulation Function ---

# Create a function to run the simulation for a given sample size
run_simulation <- function(sample_size, n_sims, beta_0, beta_1) {
  
  # Create an empty vector to store the estimated coefficients for beta_1
  estimated_beta_1 <- numeric(n_sims)

  # Loop through the number of simulations
  for (i in 1:n_sims) {
    # Generate the independent variable x
    x <- runif(sample_size, min = 1, max = 10)

    # Generate the error term u
    u <- rnorm(sample_size, mean = 0, sd = 2)

    # Generate the dependent variable y based on the true model
    y <- beta_0 + beta_1 * x + u

    # Fit a linear model to the generated sample data
    model <- lm(y ~ x)

    # Store the estimated coefficient for beta_1
    estimated_beta_1[i] <- coef(model)[2]
  }
  
  return(estimated_beta_1)
}

# --- Run Both Simulations ---

# Run simulation for the high-variance case (small sample size)
high_variance_estimates <- run_simulation(small_sample_size, n_simulations, true_beta_0, true_beta_1)

# Run simulation for the low-variance case (large sample size)
low_variance_estimates <- run_simulation(large_sample_size, n_simulations, true_beta_0, true_beta_1)

# --- Plotting the Results ---

# Set up the plotting area to have two subplots side-by-side. [2, 4]
par(mfrow = c(1, 2))

# --- Subplot 1: High-Variance Estimator ---
hist(high_variance_estimates,
     main = "High-Variance Estimator (n=30)",
     xlab = "Estimated Beta_1",
     col = "lightcoral",
     freq = FALSE,
     breaks = 30,
     xlim = c(4, 6)) # Set common x-axis limits for comparison

# Add a vertical line for the true value of beta_1
abline(v = true_beta_1, col = "red", lwd = 2)

# Add a vertical line for the mean of the estimated beta_1 coefficients
abline(v = mean(high_variance_estimates), col = "blue", lwd = 2, lty = 2)

# Add a legend
legend("topright",
       legend = c("True Beta_1", "Mean of Estimates"),
       col = c("red", "blue"),
       lwd = 2,
       lty = c(1, 2),
       cex = 0.7)

# --- Subplot 2: Low-Variance Estimator ---
hist(low_variance_estimates,
     main = "Low-Variance Estimator (n=500)",
     xlab = "Estimated Beta_1",
     col = "lightblue",
     freq = FALSE,
     breaks = 30,
     xlim = c(4, 6)) # Set common x-axis limits for comparison

# Add a vertical line for the true value of beta_1
abline(v = true_beta_1, col = "red", lwd = 2)

# Add a vertical line for the mean of the estimated beta_1 coefficients
abline(v = mean(low_variance_estimates), col = "blue", lwd = 2, lty = 2)

# Add a legend
legend("topright",
       legend = c("True Beta_1", "Mean of Estimates"),
       col = c("red", "blue"),
       lwd = 2,
       lty = c(1, 2),
       cex = 0.7)

# Reset the plotting layout to the default (1 plot)
par(mfrow = c(1, 1))
```

## Variance Illustration (Dependence on $N$)

  
- This demonstrates that with a larger sample size, the OLS estimator is more efficient
- We can be more confident that any single estimate is close to the true population parameter
- The mean of the estimates in both scenarios (the dashed blue line) is very close to the true value, confirming that both estimators are unbiased.


## Determining the Variance in Practice

- In theory, to know the true variance of the OLS estimator, we need $\sigma^2$ (the true variance of the errors) for our formula.
  - But we can never know the true errors, because we don't know the true line!
  - All we have are the residuals ($e_i$) from our estimated line: $e_i = y_i - \hat{y}_i$

- Solution: We use the residuals to estimate the error variance. We call this estimate $s^2$ or $\hat{\sigma}^2$, and its square root is called the **standard error** of a regression coefficient. 
- On the basis of this estimate, we _estimate_ the sample variance _for our estimate $\hat{\beta}$_. Hence our terminology of $\widehat{Var}(\hat{\beta})$. 

## The Standard Error

- Our starting point is the computation of the _standard error of a regression_ (SER). 
- The **SER** is an estimator of the standard deviation of the population error term, $\sigma$. It measures the typical size of a residual (the model's "average mistake").

:::{.callout-note title="Definition: Standard Error of a Regression"}
$$
\hat{\sigma} = SER = \sqrt{\frac{SSR}{n-2}} = \sqrt{\frac{\sum e_i^2}{n-2}}
$$
:::

- We divide by $n-2$ (degrees of freedom) because we had to estimate two parameters ($\beta_0, \beta_1$) to get the residuals.
- SER is measured in the same units as $y$. A smaller SER is better.

## From Standard Error to Inference

- Now we have all the pieces:
  - Start with the true (but unusable) variance formula $Var(\hat{\beta}_1) = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}$
  - Plug in our estimate $\hat{\sigma}$ for the unknown $\sigma$ (the SER). 
  - This gives us the estimated variance: $\hat{\sigma}^2 / \sum (X_i - \bar{X})^2$
  - Take the square root to get it back to the original units of $\beta_1$:
  - This is the Standard Error of the $\beta_1$ coefficient.

:::{.callout-note title="Definition: Standard Error of a Coefficient"}
$$
SE(\hat{\beta}) = \hat{\sigma} / \sqrt{\sum_{i=1}^n (X_i - \bar{X})^2}
$$
:::

## Hypotheses Testing

- To test a hypothesis about a single coefficient (e.g., $H_0: \beta = 0$), we want to see how many standard deviations our estimate $\hat{\beta}$ is from the hypothesized value.
  $$
    \text{Test Stat} = (\text{Our Estimate of } \hat{\beta} - \text{Hypothesized Value}) / \text{Standard Error}(\hat{\beta})
  $$
  
- If we knew the true population standard error $\sigma$, this statistic would follow a perfect Normal distribution.
- We don't know the population standard error, $\sigma$.
- Solution: We replace $\sigma$ with its sample estimate, $\hat{\sigma} = \sqrt{\frac{SSR}{n-k-1}}$. 
  - Because we had to *estimate* $\sigma$, we introduce extra sampling variability into our statistic.
  - This means that our test statistic will be $t$-distributed instead of normally distributed. 

## Why a t-statistic?

- The ratio of our estimate to its standard error is no longer normally distributed. It follows a **t-distribution**.

:::{.callout-note title="Definition: Distribution of t-value under $H_0$"}

$$
t = \frac{\hat{\beta} - \beta}{se(\hat{\beta}_j)} \sim t_{(n-2)}
$$

:::

- The **t-distribution** looks very similar to the normal distribution but has "fatter tails," reflecting the added uncertainty from estimating $\sigma^2$.
  - It is characterized by its **degrees of freedom (df)**, which for Simple Linear Regression is $df = n - 2$.
  - As the sample size ($n$) gets large, the t-distribution converges to the standard normal distribution.

## Visualization

:::{.callout-tip title="Example: $t$-distribution vs. Normal Distribution"}

```{r}
#| fig-align: 'center'
#| fig-width: 8
#| fig-height: 4

# Set the range for x-values
x <- seq(-4, 4, length.out = 1000)

# Create data frame for normal distribution
normal_df <- data.frame(
  x = x,
  y = dnorm(x),
  df = NA,  # Add df column to match t_df structure
  Distribution = "Standard Normal"
)

# Create data frame for t-distributions with different degrees of freedom
t_df <- data.frame(
  x = rep(x, 3),
  y = c(dt(x, df = 1), dt(x, df = 5), dt(x, df = 30)),
  df = rep(c(1, 5, 30), each = length(x)),
  Distribution = rep(c("t (df=1)", "t (df=5)", "t (df=30)"), each = length(x))
)

# Now the columns match and rbind will work
combined_df <- rbind(normal_df, t_df)

# Create the plots
plot_normal <- ggplot(normal_df, aes(x, y)) +
  geom_line(color = "blue", linewidth = 1) +
  ggtitle("Standard Normal Distribution") +
  xlab("x") + ylab("Density") +
  theme_minimal() +
  ylim(0, 0.45)

plot_t <- ggplot(t_df, aes(x, y, color = Distribution)) +
  geom_line(linewidth = 1) +
  ggtitle("t-Distributions with Various Degrees of Freedom") +
  xlab("x") + ylab("Density") +
  scale_color_manual(values = c("red", "green", "purple")) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  ylim(0, 0.45)

# Arrange plots side by side
grid.arrange(plot_normal, plot_t, ncol = 2)
```

:::

## Example: A $t$-test in Simple Linear Regression

:::{.callout-tip title="Example: $t$-test in Linear Regression"}
Consider the following regression output:

```{r}
#| code-fold: true
#| echo: true
#| collapse: true
summary(slr_model)
```

From this, we can see that the regression standard error (SER), $\hat{\sigma} = 3.4$. We can also see that the SE on the `educ` coefficient is 0.19. We can relate the two by dividing SER by $\sqrt{\sum (X_i - \bar{X})^2}$: 

```{r}
#| code-fold: true
#| echo: true
#| collapse: true

ser <- sqrt(sum(slr_model$residuals^2/98))
cat("Standard error regression:", ser)

se_beta_educ <- ser / sqrt(sum((dat$educ - mean(dat$educ))^2))
cat("Standard error Beta_educ:", se_beta_educ)
```

We can therefore manually calculate the $t$-statistic testing $H_0: \beta=0$ as:

```{r}
#| code-fold: true
#| echo: true
#| collapse: true

t_stat <- (slr_model$coefficients['educ'] - 0) / se_beta_educ
t_stat
```

Finally we can even calculate the two-sided $p$-value of observing a test statistic this extreme under the null hypothesis:

```{r}
#| code-fold: true
#| echo: true
#| collapse: true

p_val <- (1-pt(t_stat, 98)) + pt(-t_stat, 98)
cat("The p-value is:", p_val)
```

:::

## What did we do?

- **The Linear Model's Purpose:**
  - Econometrics uses the linear regression model to estimate the relationship between a dependent variable (e.g., wage) and one or more explanatory variables (e.g., education). The goal is to estimate an unknown "population" relationship using a "sample" of data.

- **The OLS Method**:
  - The model's coefficients (slope and intercept) are estimated using the Ordinary Least Squares (OLS) method. This technique finds the best-fitting line by minimizing the sum of the squared differences (residuals) between the actual data points and the predicted values on the line.

- **Interpretation of Coefficients**: 
  - The meaning of a coefficient depends on the model's structure. While a basic model shows unit changes, using logarithms (log-level, level-log, log-log) allows for interpreting relationships in terms of percentage changes or elasticities, which is common in economics.

# The End




