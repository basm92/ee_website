---
title: "Empirical Economics"
subtitle: "Lecture 3: Time Series and Prediction"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 3 - Time Series and Prediction'
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Introduction

## Course Overview

- Linear Model I
- Linear Model II
- Time Series and Prediction
- Panel Data I
- Panel Data II
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Instrumental Variables

## What do we do today?

- First look at time series data
- Discuss statistics measuring the persistence of univariate time series (autocovariance, autocorrelation)
- Discuss various univariate processes
- Switch to bivariate time series and consider the danger of spurious regression
- Consider more advanced time series models
- Predictinon in all aforementioned models

- **Material**: Wooldridge Chapters 10 and 11


## What is Time Series Data?

:::{.callout-note title="Definition: Time Series Data"}
Time series data consists of observations of a variable or several variables over time.
:::

- **Key Feature:** The data is ordered chronologically.

:::{.callout-tip title="Examples: Time Series Data"}
- Gross Domestic Product (GDP) measured quarterly
- Monthly inflation rates
- Daily stock prices
- Annual government budget deficits
:::

## Characteristics of Time Series Data

- **Temporal Ordering:** Unlike cross-sectional data, the order of observations in time series data matters. The past can affect the future, but the future cannot affect the past.
- **Serial Correlation (or Autocorrelation):** Observations in a time series are often correlated with their own past values. For example, a high GDP in one quarter may suggest a high GDP in the next quarter.
- **Seasonality:** Many time series exhibit regular patterns at certain times of the year (e.g., retail sales are higher in the fourth quarter).
- **Trends:** A time series may have a long-term upward or downward movement.

## Objectives of Time Series Analysis

- There are various objectives of time series econometrics:

1. **Forecasting:** Predicting future values of a variable is a primary objective. For instance, forecasting inflation is crucial for central banks.
2. **Policy Analysis:** Economists use time series models to assess the impact of policy changes. For example, what is the effect of an interest rate hike on unemployment?
3. **Understanding Dynamic Economic Relationships:** Time series analysis helps us understand how economic variables interact over time.

## Notation and Basic Concepts

- $Y_t$:  The value of the variable Y at time period t.
- $Y_{t-1}$: The value of Y in the previous period (the first lag).
- $Y_{t-s}$: The value of Y s periods ago (the s-th lag).
- $\Delta Y_t = Y_t - Y_{t-1}$: The first difference of $Y$, which represents the change in $Y$ from the previous period to the current period.

# Introduction to Dynamic Processes

## A Time Series as a Stochastic Process

:::{.callout-note title="Definition: Time Series (Mathematical)"}
A time series is a set of random variables indexed by time, denoted as $\{Y_1, Y_2, \dots, Y_T\}$ or simply $\{Y_t\}$. 
:::

- Stochastic vs. Deterministic: We treat a time series as a stochastic process because we don't know the future values with certainty. We can only talk about their probability distributions.

- One Realization: The actual data we have (e.g., GDP from 1950-2024) is just one of many possible paths the process could have taken. 
- Our goal is to infer the properties of the underlying process from this single realization.

## Basic Statistical Properties

- Just like any random variable, each point in a time series has statistical properties.

  - Mean: The expected value of the process at time t: $E(Y_t) = \mu_t$
  - Variance: The variance of the process at time t: $Var(Y_t) = E[(Y_t - \mu_t)^2] = \sigma_t^2$

- Crucially, in general, the mean and variance could be different at each point in time.


## Autocovariance 

- Covariance: 
  - Recall that covariance measures how two variables move together.

- Autocovariance
  - Autocovariance is the covariance of a time series with its own past values. It measures the linear dependence between different points in the series.
  
:::{.callout-note title="Definition: Autocovariance"}
The $k$-th Order Autocovariance ($\gamma_k$):

  $$
    \gamma(t, t-k) = Cov(Y_t, Y_{t-k}) = E[(Y_t - \mu_t)(Y_{t-k} - \mu_{t-k})]
  $$
:::

- This measures the covariance between the series at time t and time t-k.

## Stationarity: A Key Simplifying Assumption

- For analysis to be tractable, we often assume the series is covariance stationary (or weakly stationary).

- This means the three aforementioned statistical properties do not change over time. Three conditions must hold:

:::{.callout-note title="Definition: Stationarity"}
- Constant Mean: $E(Y_t) = \mu$ for all t. The series fluctuates around a constant level.
- Constant Variance: $Var(Y_t) = \sigma^2$ for all t. The volatility of the series is constant.
- Constant Autocovariance: $Cov(Y_t, Y_{t-k}) = \gamma_k$ for all t. The covariance between two points depends only on the lag $k$ (how far apart they are), not on their position in time.
:::

## Autocorrelation Function (ACF)

- The value of the autocovariance ($\gamma_k$) depends on the units of the variable $Y$. This makes it hard to compare across different series.

- Autocorrelation: standardize the autocovariance to get the autocorrelation, which is unit-free.

:::{.callout-note title="Definition: Autocorrelation"}

The $k$-th Order Autocorrelation ($\rho_k$): 

  $$
    \rho_k = \frac{ Cov(Y_t, Y_{t-k}) }{\sqrt{Var(Y_t) Var(Y_{t-k})}}
  $$

If the series is stationary, this simplifies to:

  $$
    \rho_k = \frac{\gamma_k}{\gamma_0}
  $$
  
where $\gamma_0$ is the variance, $Var(Y_t)$. 
  
:::

## Interpretation of the Autocorrelation Function (ACF)

- The ACF, denoted by $\rho_k$, measures the "memory" or persistence of a time series.
  - It is a value between -1 and +1.
  - $\rho_1$: The correlation between $Y_t$ and $Y_{t-1}$("today" and "yesterday").
  - $\rho_k$: The correlation between $Y_t$ and $Y_{t-k}$
  - A high $\rho_k$ suggests that a shock in one period will have a persistent effect k periods later.

- For a stationary series, we expect $\rho_k \rightarrow 0$ as $k$ gets larger.

## The Correlogram (ACF Plot)

- A correlogram is a bar chart that plots the sample autocorrelations ($r_k$) for different lags ($k = 1, 2, 3, \dots$).
  - It provides a visual summary of the persistence in a time series. We can see how quickly the correlations die out.
  - Correlograms usually include confidence bands. Autocorrelations that fall outside these bands are considered statistically different from zero.

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 6

# Number of lags for the correlogram
k <- 9
n <- 100

# Create a sequence of lags from 1 to k
lags <- 1:k

# Simulate the autocorrelation values. 
# A common pattern is an exponential decay, typical of an AR(1) process.
# Let's simulate an ACF from a process with phi = 0.8.
# The ACF for an AR(1) at lag k is phi^k.
acf_values <- 0.8^lags

# Create a data frame, which is the standard input for ggplot
acf_df <- data.frame(
  Lag = lags,
  ACF = acf_values
)

#Confidence Bounds
# This tests the null hypothesis that the autocorrelation at a given lag is zero.
conf_limit <- 1.96 / sqrt(n)

# Create the plot object
correlogram_plot <- ggplot(data = acf_df, aes(x = Lag, y = ACF)) +
  
  # Add the horizontal lines for the significance bounds (in blue)
  geom_hline(yintercept = conf_limit, linetype = "dashed", color = "blue") +
  geom_hline(yintercept = -conf_limit, linetype = "dashed", color = "blue") +
  
  # Add the vertical lines (segments) from y=0 to the ACF value
  # This is the classic look of a correlogram
  geom_segment(aes(xend = Lag, yend = 0), linewidth = 1) +
  
  # Add points at the top of the segments (optional but common)
  geom_point(color = "black", size = 3) +
  
  # Add a solid line at y=0 for reference
  geom_hline(yintercept = 0, color = "black") +

  # Set the x-axis to only show integer lags
  scale_x_continuous(breaks = 1:k) +
  
  # Set the y-axis limits
  coord_cartesian(ylim = c(-1, 1)) +
  
  # Add informative labels and a title
  labs(
    title = "Simulated Correlogram (ACF Plot)",
    subtitle = paste("k =", k, "lags, assumed n =", n),
    x = "Lag",
    y = "Autocorrelation"
  ) +
  
  # Use a clean theme
  theme_minimal()

# Print the plot to the screen
print(correlogram_plot)
```


# Modeling a Single Time Series

## Why Model a Single Series?

- Before we ask how X affects Y, we must first understand the behavior of Y itself.
- **Univariate models** describe the dynamic properties of a single time series using its own past.
- Main uses:
    1.  **Understanding Persistence:** How long do shocks to the economy last?
    2.  **Forecasting:** Using the past of a series to predict its future.

## The White Noise Process

- The Simplest Time Series: A process called "white noise" is the building block for more complex models. We often denote it as $u_t$

:::{.callout-note title="Definition: White Noise Process"}
A white noise is a time series such that:

- $E(u_t) = 0$ (Zero mean)
- $Var(u_t) = \sigma^2$ (Constant variance)
- $Cov(u_t, u_s) = 0$ for all $t\neq s$ (No autocorrelation)
:::

- Interpretation: A white noise process is completely random and unpredictable. Its ACF will be zero for all lags $k > 0$. It is the ideal error term in a time series regression.

## The Autoregressive (AR) Model: AR(1)

- The simplest dynamic model is the first-order autoregressive, or AR(1), model:

$$
  Y_t = α + \rho Y_{t-1} + u_t
$$

- $Y_t$: The value of Y at time t.
- $Y_{t-1}$: The value of Y in the previous period.
- $\rho$: The autoregressive coefficient, which measures the persistence of the series.
- $u_t$: A white noise error term (uncorrelated with past values).

## Visualization AR(1) Process

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import polars as pl
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

T = 100      # Number of time periods to simulate
rho = 0.9    # Autoregressive parameter (must be < 1 for stationarity)
c = 2        # A constant/intercept term
sigma = 1.0  # Standard deviation of the white noise error term

# Generate all the random shocks at once using NumPy
np.random.seed(42) # for reproducibility
epsilon = np.random.normal(loc=0, scale=sigma, size=T)

# Initialize a Python list to store our time series values
y_values = [0] * T

# Set the initial value. A good choice is the unconditional mean of the process
# E[Y] = c / (1 - rho), to start the series near its long-run equilibrium.
y_values[0] = c / (1 - rho)

# Loop to generate the AR(1) series recursively
for t in range(1, T):
    # Apply the AR(1) formula
    y_values[t] = c + rho * y_values[t-1] + epsilon[t]

# Create a DataFrame with a time index and the simulated series
df = pl.DataFrame({
    'time': range(T),
    'ar_process': y_values
})

# Set the style and size for the plot
sns.set_theme(style="whitegrid")
plt.figure(figsize=(14, 7))

# Create the line plot
lineplot = sns.lineplot(
    data=df,
    x='time',
    y='ar_process',
    label=f'AR(1) Process (ρ={rho})'
)

# Add a horizontal line for the theoretical mean of the process
mean_value = c / (1 - rho)
plt.axhline(
    mean_value,
    color='red',
    linestyle='--',
    label=f'Theoretical Mean: {mean_value:.2f}'
)

# Add titles and labels for clarity
plt.title('Simulation of a Stationary AR(1) Process', fontsize=16)
plt.xlabel('Time Period', fontsize=12)
plt.ylabel('Value', fontsize=12)
plt.legend()

# Show the plot
plt.show()
```


## Stationarity of an AR(1) Process

- For the AR(1) model $Y_t = \alpha + \rho Y_{t-1} + u_t$:
  - If $|\rho| < 1$:, the series is **stationary**. Any shock ($u_t$) will eventually die out.
  - If $|\rho|=1$, the series is **non-stationary** and is called a **random walk**. Shocks have a permanent effect.
  - If $|\rho|>1$, the series is **explosive** (this is rare in economics).

- **Speed of Adjustment:** A value of $\rho$ close to 0 suggests that the effect of a past value dies out quickly.
- **Persistence:** A value of $\rho$ close to 1 indicates that a shock to the system will persist for a long time. The series will return to its mean slowly.

## The Random Walk Model

- A special case of the AR(1) model where $\rho=1$:

  $$
    Y_t = Y_{t-1} + u_t
  $$

- The best forecast for tomorrow's value is today's value.
- **Permanent Shocks:** The impact of a shock $u_t$ is permanent; it is carried forward in all future periods.
  - **Examples:** The prices of financial assets are often modeled as random walks.

## Visualization Random Walk

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import polars as pl
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

T = 500      # Number of time periods to simulate
rho = 1    # Autoregressive parameter (must be < 1 for stationarity)
c = 2        # A constant/intercept term
sigma = 5.0  # Standard deviation of the white noise error term

# Generate all the random shocks at once using NumPy
np.random.seed(42) # for reproducibility
epsilon = np.random.normal(loc=0, scale=sigma, size=T)

# Initialize a Python list to store our time series values
y_values = [0] * T

# Set the initial value. A good choice is the unconditional mean of the process
# E[Y] = c / (1 - rho), to start the series near its long-run equilibrium.
y_values[0] = 0

# Loop to generate the AR(1) series recursively
for t in range(1, T):
    # Apply the AR(1) formula
    y_values[t] = c + rho * y_values[t-1] + epsilon[t]

# Create a DataFrame with a time index and the simulated series
df = pl.DataFrame({
    'time': range(T),
    'ar_process': y_values
}, strict=False)

# Set the style and size for the plot
sns.set_theme(style="whitegrid")
plt.figure(figsize=(14, 7))

# Create the line plot
lineplot = sns.lineplot(
    data=df,
    x='time',
    y='ar_process',
    label='Random Walk'
)

# Add titles and labels for clarity
plt.title('Simulation of a Random Walk', fontsize=16)
plt.xlabel('Time Period', fontsize=12)
plt.ylabel('Value', fontsize=12)
plt.legend()

# Show the plot
plt.show()
```

## Higher-Order AR(p) Models

- We can include more lags of the dependent variable. An $AR(p)$ model includes $p$ lags:

$$
Y_t = \alpha + \rho_1 Y_{t-1} + \rho_2 Y_{t-2} + \dots + \rho_p Y_{t-p} + u_t
$$

- **Choosing p:** The number of lags (p) can be determined using information criteria like the Akaike Information Criterion (AIC) or the Schwarz Information Criterion (SIC).

## The Moving Average Model

- In the MA model, the current value is a function of the current and past **random shocks**.

  $$
    Y_t = \mu + u_t + \theta u_{t-1}
  $$

- $\mu$: The mean of the series.
- $\theta$: The moving average coefficient.
- The MA(1) process has a **finite memory**. A shock $u_{t-1}$ affects $Y_{t-1}$ and $Y_t$, but has no direct effect on $Y_{t+1}$. 

## Estimation and Interpretation of MA Coefficients ($\theta$)

- As you may have noticed, there are no independent variables in an MA model. 
  - Usually, what we do is assume $u_0=0$, take a guess for $\mu$ and $\theta$, and solve for $u_t$: $u_t = Y_t - \mu - \theta u_{t-1}$
  - If the true shocks came from a Normal distribution $N(0, \sigma^2_u)$, how likely was it to get this specific sequence of inferred shocks? The joint probability of observing this sequence is called the Likelihood.
  - We try to find the most likely $\mu$ and $\theta$ that could have generated the actual data.[^1]
  
- Unlike an AR(1) model, a shock in an MA(1) model only persists for one period. $Y_t$ is affected by $u_t$ and $u_{t-1}$, but $Y_{t+1}$ will not be affected by $u_t$.
- The coefficient $\theta$ determines the extent to which a past shock affects the current observation.

[^1]: More about this in Lecture 6. 

## The Autoregressive Moving Average (ARMA) Model

- We can combine the features of autoregressive and moving average models to create an ARMA(p,q) model.
- For example, an ARMA(1,1) Model: $Y_t = \alpha + \rho Y_{t-1} + u_t + \theta u_{t-1}$
- ARMA models are usually very flexible and can capture a wide range of time series dynamics. This is how they are implemented in Python/R/Stata:

:::{.panel-tabset}

### R

```{r}
#| echo: true
#| code-fold: true
#| collapse: true

# Generate sample data
set.seed(123)
data <- rnorm(100)

# Fit ARMA(1,1) model
arma_model <- arima(data, order = c(1, 0, 1))  # (p, d, q) where d=0 for ARMA

# Print summary
summary(arma_model)
```

### Python

```{python}
#| echo: true
#| code-fold: true
#| collapse: true

import numpy as np
import statsmodels.api as sm

# Generate sample data
np.random.seed(123)
data = np.random.randn(100)

# Fit ARMA(1,1) model
arma_model = sm.tsa.ARIMA(data, order=(1,0,1))
arma_results = arma_model.fit()

# Print summary
print(arma_results.summary())
```

### Stata

```{stata}
#| eval: false
#| code-fold: true

* Generate sample data  
set obs 100  
set seed 123  
gen y = rnormal()  

* Estimate ARMA(1,1)  
arima y, ar(1) ma(1)  

* Display results  
estimates table  
```


:::

# Relationship Between Time Series

## Linear Model in Time Series

- The starting point for a relationship between time series is often the linear model. 

- The model form is identical to its cross-sectional counterpart:

  $$
    Y_t = \alpha + \beta X_t + \epsilon_t
  $$

- This model posits a contemporaneous relationship between X and Y.
- Question: Can we estimate this with OLS and trust the results?
- For OLS to be a good estimator, the standard assumptions must hold. The most problematic one for time series data is the assumption of finite variance:
  
  $$
    \text{Var}(X_t) = \sigma^2
  $$
    
## When does OLS work?

- If $X$ is stationary, e.g. $X \sim AR(1)$ with $|\rho|<1$, this is a valid approach. 

:::{.callout-tip title="Example: Finite Expectation and Variance of $X$ when $X \sim AR(1)$"}
Let $X_t = \rho X_{t-1} + u_t$. Using the definition for $X_{t-1}$, we find that $X_t = \rho (\rho X_{t-2} + u_t$). 

After recursively substituting, past values of $X_t$, we can express $X_t$ as:

$$
X_t = \rho^k X_{t-k} + \sum_{j=0}^{k-1} u_{t-j}
$$
As $k \rightarrow \infty$, we end up with $X_t = \sum_{j=0}^{\infty} \rho^j u_j$. 

Taking the expected value of this, we immediately see that $E[X_t]=0$.

:::

## When does OLS work? (Cont.)

- As for the variance:

:::{style="font-size: 0.7em;"}

:::{.callout-tip title="Example: Finite Expectation and Variance of $X$ when $X \sim AR(1)$"}

  $$
    \begin{align}
    \text{Var}(X_t) &= \text{Var}(\sum_{j=0}^\infty \rho^j u_{t-j}) \\
    &= \sum_{j=0}^\infty \text{Var}(\rho^j u_{t-j}) \quad \text{ Uncorrelated $u_t$} \\
    &= \sum_{j=0}^\infty \rho^{2j} \text{Var}(u_{t-j}) \quad \text{Constant term in Var} \\
    &= \sigma^2_u \sum_{j=0}^\infty \rho^{2j} \quad \quad \text{Constant variance of $u_t$} \\
    \end{align}
  $$

Since $\sum_{j=0}^\infty \rho^{2j}=\frac{1}{1-\rho^2}$ if $|\rho|<1$, the variance is equal to $\frac{\sigma^2_u}{1-\rho^2}$.  

:::

:::

## The Problem of Spurious Regression

- OLS might also work in other cases where $X_t$ is stationary, or when $X_t$ and $Y_t$ are cointegrated.[^2]
- However, in most situations, OLS may give a **deceptive relationship:**
  - A spurious regression occurs when we find a statistically significant relationship between two time series variables that are actually unrelated.
- This often happens when both variables have a strong trend (are non-stationary), and especially, when $X$ is non-stationary. 
  - The trend can be deterministic (predictable) or stochastic (random).

[^2]: This means roughly that $X_t$ and $Y_t$ follow a common trend, but it is outside the scope of this lecture. 

## Spurious Regression is Non-Stationarity

- Let's look at the most common non-stationary process: the random walk.

  $$
    X_t = X_{t-1} + u_t
  $$

:::{.callout-note title="Theorem: Variance of a Random Walk"}
Suppose $X_0=0$. Let us find the variance of $X_t$. 

We know that $X_1 = u_i$. $X_2 = X_1 + u_2 = u_1 + u_2$, etc. 

Hence, $X_t = \sum u_i$, and $Var(X_t) = Var(u_1 + u_2 + \dots + u_t)$. 

Since the $u_i$ are uncorrelated (white noise), the variance of the sum is the sum of the variances:
  $$
    \begin{align}
    Var(X_t) &= \sigma^2_u + \dots + \sigma^2_u \text{ ( t times)} \\
            &= t \times \sigma^2_u
    \end{align}
  $$

The variance depends on time $t$ and grows without bound. This is a clear violation of stationarity.

:::

## Failure of Reliable Estimation

- Imagine two independent and unrelated random walks, $Y_t = Y_{t-1} + u_t$ and $X_t = X_{t-1} + v_t$, where $u_t$ and $v_t$ are independent white noise processes.

- By construction, the true $\beta$ in a regression of $Y_t$ on $X_t$ is zero.

- When we run $Y_t= \alpha + \beta X_t + u_t$,  we know $Var(Y_t)=Var(X_t)=t\times \sigma^2_u$. 
  - $\hat{\beta}=\frac{\text{Cov}(X,Y)}{\text{Var}(X)}$. 

- Since both $Y_t$ and $X_t$ are non-stationary, both the numerator and the denominator converge to a random variable, not to a particular number. 
  - Hence, the OLS coefficient is **inconsistent**. 


## Consequences of Spurious Regression

- Since the OLS estimator converges to a random variable with variance tending to infinity, if you run a regression of $Y_t$ on $X_t$, where both are trending but unrelated:
  - You will likely find a **high R-squared**.
  - The **t-statistics** for the estimated coefficients will likely be **significant**.
  - You might conclude that X causes Y, but this conclusion would be **meaningless and misleading**.

## Detecting Spurious Regression

- **Rejection of the BG Test:** A key indicator of spurious regression is a rejection of the Breusch-Godfrey test.

:::{.callout-note title="Definition: Breusch-Godfrey Test"}
The BG test assesses whether the error term, $\epsilon_t$, follows an AR(p) process. You, the researcher, choose the number of lags (p) to test for. The assumed model for the error is:
$$\epsilon_t = \rho_1 \epsilon_{t-1} + \rho_2 \epsilon_{t-2} + \dots + \rho_p \epsilon_{t-p} + v_t,$$

where $v_t$ is a white noise term. $H_0: \rho_1, \dots, \rho_p=0$, and $H_A$ is that at least one $\rho$ coefficient is non-zero: there is serial correlation up to some order $p$. 
:::

## Breusch-Godfrey Test

:::{.callout-note title="Breusch-Godfrey Test: Procedure"}
The BG test is performed in three simple steps:

- Run the Main Regression: First, estimate your original model using OLS, regardless of what it is. Obtain the series of residuals,
- Run an Auxiliary Regression. Run a new regression where the residual $e_t$ is the dependent variable. The independent variables are:
  - All the original regressors from the main model. 
  - The lagged residuals up to order $p$: $e_{t-1}, \dots, e_{t-p}$. 
- Obtain the R-squared: Calculate the R-squared from this auxiliary regression. 

---

- The Breusch-Godfrey test uses a Lagrange Multiplier (LM) statistic: $LM = T \times R^2_{aux}$, where $T$ is the number of observations used in the auxiliary regression and $R^2_{aux}$ is the $R^2$ from the auxiliary regression in Step 2.

- Under the null hypothesis of no serial correlation, the LM statistic follows a $\chi^2$ distribution with p degrees of freedom (where p is the number of lagged residuals included).

- If $LM > \chi^2_{crit}$, you reject the null hypothesis. You conclude there is significant evidence of serial correlation.
  
:::

## Example BG Test

- We demonstrate the BG test using autocorrelated errors: $Y_t = \alpha + \beta X_t + u_t$ where $u_t = \rho u_{t-1} + e_t$, with $e_t$ being white noise. 

:::{.panel-tabset .hey}

### R

```{r}
#| echo: true
#| code-fold: true
#| collapse: true

library(lmtest)
# Step 1: Simulate Data with Serially Correlated Errors

# For reproducibility of the random numbers
set.seed(42) 

# Define simulation parameters
n <- 200                 # Sample size
alpha <- 5               # True intercept
beta <- 2                # True slope for X

# *** Key step: Define the autocorrelation structure for the errors ***
# We will create an error term u_t = rho * u_{t-1} + e_t
# where e_t is white noise.
rho <- 0.8               # High positive autocorrelation coefficient
sigma_e <- 1.5           # Standard deviation of the white noise component

# Generate the independent variable X
X <- rnorm(n, mean = 20, sd = 10)

# Generate the serially correlated error term 'u'
u <- numeric(n)
e <- rnorm(n, mean = 0, sd = sigma_e) # White noise component

# The first error term has no preceding term
u[1] <- e[1] 

# Generate the rest of the errors using the AR(1) formula
for (t in 2:n) {
  u[t] <- rho * u[t-1] + e[t]
}

# Generate the dependent variable Y using the regression equation
# Y_t = alpha + beta*X_t + u_t
Y <- alpha + beta * X + u

# Combine into a data frame for easy use
sim_data <- data.frame(Y = Y, X = X)

# Step 2: Estimate the OLS Regression Model

# We estimate the model Y ~ X, pretending we don't know about the
# autocorrelation in the errors.
ols_model <- lm(Y ~ X, data = sim_data)

# Print the summary. The coefficients will likely be close to the true
# values, but the standard errors and p-values are unreliable.
print(summary(ols_model))


#--------------------------------------------------------------------
# Step 3: Conduct the Breusch-Godfrey Test
#--------------------------------------------------------------------

# The bgtest() function from the 'lmtest' package performs the test.
# We need to specify the model and the order of autocorrelation to test for.
# Since we created an AR(1) process, we will test for order 1.
bg_test_order1 <- bgtest(ols_model, order = 1)
print(bg_test_order1)

# You can also test for higher orders. For example, testing up to order 3.
# This would test the joint null hypothesis that the coefficients on the first
# three lags of the residuals are all zero in the auxiliary regression.
bg_test_order3 <- bgtest(ols_model, order = 3)
print(bg_test_order3)
```

### Python

```{python}
#| echo: true
#| collapse: true
#| code-fold: true

import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.diagnostic import acorr_breusch_godfrey

# Step 1: Simulate Data with Serially Correlated Errors

# For reproducibility of the random numbers
np.random.seed(42)

# Define simulation parameters
n = 200                 # Sample size
alpha = 5               # True intercept
beta = 2                # True slope for X

# *** Key step: Define the autocorrelation structure for the errors ***
# We will create an error term u_t = rho * u_{t-1} + e_t
# where e_t is white noise.
rho = 0.8               # High positive autocorrelation coefficient
sigma_e = 1.5           # Standard deviation of the white noise component

# Generate the independent variable X
# In numpy, mean is 'loc' and standard deviation is 'scale'
X = np.random.normal(loc=20, scale=10, size=n)

# Generate the serially correlated error term 'u'
# Initialize arrays for the white noise and the final error term
e = np.random.normal(loc=0, scale=sigma_e, size=n)
u = np.zeros(n)

# The first error term has no preceding term (Python uses 0-based indexing)
u[0] = e[0]

# Generate the rest of the errors using the AR(1) formula
for t in range(1, n):
  u[t] = rho * u[t-1] + e[t]

# Generate the dependent variable Y using the regression equation
# Y_t = alpha + beta*X_t + u_t
Y = alpha + beta * X + u

# Combine into a data frame for easy use
sim_data = pd.DataFrame({'Y': Y, 'X': X})

# Step 2: Estimate the OLS Regression Model

# We estimate the model Y ~ X, pretending we don't know about the
# autocorrelation in the errors.
# The formula syntax in statsmodels is identical to R's.
# We must call .fit() to perform the estimation.
ols_model = smf.ols('Y ~ X', data=sim_data).fit()

# Print the summary. The coefficients will likely be close to the true
# values, but the standard errors and p-values are unreliable.
print(ols_model.summary())

# Step 3: Conduct the Breusch-Godfrey Test

# The acorr_breusch_godfrey() function performs the test.
# It takes the fitted model results and the order of autocorrelation (nlags).
# It returns (lm_statistic, lm_pvalue, f_statistic, f_pvalue)

# Test for order 1, as we created an AR(1) process.
bg_test_order1 = acorr_breusch_godfrey(ols_model, nlags=1)

print("\n" + "="*50)
print("Breusch-Godfrey Test (Order 1)")
print(f"  LM Statistic: {bg_test_order1[0]:.4f}")
print(f"  p-value (LM): {bg_test_order1[1]:.4f}")
# A very low p-value indicates rejection of the null hypothesis (no serial correlation).

# You can also test for higher orders. For example, testing up to order 3.
# This tests the joint null hypothesis that the coefficients on the first
# three lags of the residuals are all zero in the auxiliary regression.
bg_test_order3 = acorr_breusch_godfrey(ols_model, nlags=3)

print("\n" + "="*50)
print("Breusch-Godfrey Test (Order 3)")
print(f"  LM Statistic: {bg_test_order3[0]:.4f}")
print(f"  p-value (LM): {bg_test_order3[1]:.4f}")
print("="*50)
```

### Stata

```{stata}
#| eval: false
#| echo: true
#| code-fold: true
#| collapse: true
* Housekeeping: Clear memory and stop the output from pausing
clear all
set more off

* Step 1: Simulate Data with Serially Correlated Errors
set seed 42

* Define simulation parameters using local macros
local n = 200                 // Sample size
local alpha = 5               // True intercept
local beta = 2                // True slope for X
local rho = 0.8               // High positive autocorrelation coefficient
local sigma_e = 1.5           // Standard deviation of the white noise component

* Set the number of observations for the dataset
set obs `n'

* Generate an index variable 't' to declare this as time-series data
gen t = _n
tsset t

* Generate the independent variable X
gen X = rnormal(20, 10)

* Generate the serially correlated error term 'u'
* First, create the white noise component 'e'
gen e = rnormal(0, `sigma_e')

* Now, generate 'u' using the recursive AR(1) formula
* u_t = rho * u_{t-1} + e_t
* We initialize u as missing, set the first value, then generate the rest
gen u = .
replace u = e in 1
replace u = `rho' * L.u + e in 2/l // L. is the lag operator, 2/l means "from obs 2 to the last"

* Generate the dependent variable Y using the regression equation
* Y_t = alpha + beta*X_t + u_t
gen Y = `alpha' + `beta'*X + u


* Step 2: Estimate the OLS Regression Model

* We estimate the model Y ~ X, pretending we don't know about the
* autocorrelation in the errors. The output is printed automatically.
regress Y X

* Step 3: Conduct the Breusch-Godfrey Test

* The `estat bgodfrey` command is a post-estimation command that must be
* run immediately after the regression.

* Test for order 1, as we created an AR(1) process.
estat bgodfrey, lags(1)

* You can also test for higher orders. For example, testing up to order 3.
* This tests the joint null hypothesis that the coefficients on the first
* three lags of the residuals are all zero in the auxiliary regression.
estat bgodfrey, lags(3)
```

:::

## Avoiding Spurious Regression

- If the static model $Y_t = \alpha + \beta X_t + \epsilon_t$ is so dangerous, what is the alternative?

- **Differencing the Data:** If two variables are non-stationary, their first differences ($\Delta Y_t$ and $\Delta X_t$) may be stationary.
- **Dynamic models**: We need models that explicitly account for dynamics—the idea that a change in X can affect Y both today and in the future. 
  - This makes it more likely that the error term is white noise. 

## Distributed Lag (DL) Models

- Distributed lag models are used when we want to model how a change in an explanatory variable (X) affects the dependent variable (Y) over time, and we want to focus on potential **delayed effects**. 
  - **Example:** How does a change in government spending (X) affect GDP (Y) in the current quarter and in future quarters?

:::{.callout-note title="Definition: Distributed Lag Model"}
A finite distributed lag model of order $q$ is:

$$Y_t = \alpha + \beta_0 X_t + \beta_1 X_{t-1} + \dots + \beta_q X_{t-q} + u_t$$
where:

$\beta_0$: The **impact multiplier** - the immediate effect of a one-unit change in $X_t$ on $Y_t$.

$\beta_s (s > 0)$: The **dynamic multipliers** - the effect of a one-unit change in $X_{t-s}$ on $Y_t$.

:::

## Interpreting the DL Coefficients

$$Y_t = \alpha + \beta_0 X_t + \beta_1 X_{t-1} + \dots + \beta_q X_{t-q} + u_t$$

- **Short-Run Multiplier:** The total effect after a certain number of periods (e.g., $\beta_0 + \beta_1$).
- **Long-Run (or Total) Multiplier:** The total effect of a sustained one-unit change in X on Y. It is the sum of all the $\beta$ coefficients: $\sum \beta_S$.

## The Autoregressive Distributed Lag (ARDL) Model

- ARDL models are a class of models that include lags of both the dependent variable and the explanatory variable(s).

:::{.callout-note title="Definition: ARDL Model"}

An ARDL(p,q) model is defined as follows:

$$Y_t = \alpha + \sum(ρ_j \cdot Y_{t-j}) \text{ [from j=1 to p]} + \sum(\beta_j \cdot X_{t-m}) \text{ [from m=0 to q]} + u_t$$
:::

- It includes both autoregressive (lags of the DV) and distributed lag (lags of the IV) terms. 
- ARDL models are very flexible and can be used to estimate both short-run and long-run effects.

## Interpretation ARDL Coefficients

- The individual coefficients represent direct, short-term impacts before feedback effects occur.

- **Impact Multiplier ($\beta_0$)**: The immediate effect of a change in $X_t$ on $Y_t$.
  - **Derivation**: $\frac{\partial Y_t}{\partial X_t} = \beta_0$

- **Delay Multipliers ($\beta_j$)**: The effect of past changes in $X$ on current $Y$.
  - **Interpretation**: A one-unit increase in $X_{t-j}$ is associated with a change of **$\beta_j$** units in today's $Y_t$.

- **Autoregressive Coefficients ($\phi_i$)**: Measure the persistence or inertia in the process.
  - **Interpretation**: **$\phi_i$** captures the proportion of $Y_{t-i}$ that is carried over to $Y_t$. It describes how quickly the variable returns to its equilibrium after a shock.


## Long-Run Multiplier (LRM) Derivation & Interpretation

- The LRM shows the total effect on Y after a permanent change in X has fully worked through the system.

- **Derivation (via Equilibrium Condition)**:
  - In a long-run steady state, we assume variables are constant:
    - $Y_t = Y_{t-1} = \dots = Y_{eq}$
    - $X_t = X_{t-1} = \dots = X_{eq}$

  - Substitute this into the ARDL equation (assuming $E[u_t]=0$):
    - $Y_{eq} = c + \sum \phi_i Y_{eq} + \sum \beta_j X_{eq}$

  - Solve for $Y_{eq}$:
    - $Y_{eq} (1 - \sum \phi_i) = c + (\sum \beta_j) X_{eq}$
    - $Y_{eq} = \frac{c}{1 - \sum \phi_i} + \left( \frac{\sum \beta_j}{1 - \sum \phi_i} \right) X_{eq}$
    
## Long-Run Multiplier (LRM) Derivation & Interpretation (Cont.)

- **The Long-Run Multiplier ($\theta$)**:
  - The coefficient on $X_{eq}$ in the long-run equation is the LRM:
    $$ \theta = \text{LRM} = \frac{\text{Total effect of X}}{\text{Persistence of Y}} = \frac{\sum_{j=0}^{q} \beta_j}{1 - \sum_{i=1}^{p} \phi_i} $$

- **Interpretation**: A **permanent** one-unit increase in $X$ is associated with a total **long-run change of $\theta$ units** in $Y$, after all dynamic adjustments are complete.

- The ARDL model powerfully separates the **immediate impacts** ($\beta_j$) from the **total equilibrium effect** ($\theta$), which accounts for the feedback from Y's own past values.

## Estimating ARDL Models

- In R, the `ARDL` package is the standard for estimating these models. 
- Stata has a powerful built-in `ardl` command (in newer versions) or a widely used user-written one (`ssc install ardl`). It automates lag selection and estimation.
- In Python, the `statsmodels` library has incorporated a dedicated `ARDL` class, which makes estimation much more straightforward than the previous manual method of creating lagged variables.

:::{.panel-tabset .hey}

### R

```{r}
#| echo: true
#| collapse: true
#| code-fold: true

library(ARDL)
library(urca) # Contains the example 'denmark' dataset

# Load the data
data(denmark)
# The variables are LRM, LRY, IBO, IDE (inflation)

# Find the optimal lag order automatically
# We model LRM as a function of LRY and IBO, with a max lag of 4 for each
auto_model <- auto_ardl(LRM ~ LRY + IBO, data = denmark, max_order = 4)

# Print the selected orders
print(auto_model$best_order)

# Estimate the selected ARDL model
# Let's say auto_ardl selected ARDL(3, 1, 4)
ardl_model <- ardl(LRM ~ LRY + IBO, data = denmark, order = c(3, 1, 4))

# Get a detailed summary of the model
# This output neatly separates short-run and long-run coefficients
print(summary(ardl_model))
```

### Python

```{python}
#| collapse: true
#| echo: true
#| code-fold: true

import pandas as pd
import statsmodels.api as sm
from statsmodels.tsa.api import ARDL, ardl_select_order

# Load the same 'lutkepohl2' dataset from statsmodels datasets
data = r.denmark
data.index = pd.to_datetime(data['ENTRY'], format='%Y:%m').dt.to_period('Q')
# Select the variables of interest
data_subset = data[['LRM', 'LRY', 'IBO']]

# Assign endogenous (Y) and exogenous (X) variables
endog = data_subset['LRM']
exog = data_subset[['LRY', 'IBO']]

# Find the optimal lag order
# maxlag is for the AR (dependent) part, maxorder is for the DL (independent) part
# aic is the information criterion to use for selection
sel_order = ardl_select_order(
    endog, maxlag=4, exog=exog, maxorder=4, ic='aic', trend='c'
)

# Instantiate the ARDL model with the selected order
# sel_order.ardl_order gives the optimal (p, q1, q2, ...)
ardl_result = sel_order.model.fit()

# Print the summary of the results
# This shows the short-run coefficients
print(ardl_result.summary())

params = ardl_result.params
denominator = 1 - params.filter(like='LRM.L').sum()

# 3. Calculate the sum of coefficients for each independent variable and divide
long_run_effects = pd.Series({
    'LRY': params.filter(like='LRY.L').sum() / denominator,
    'IBO': params.filter(like='IBO.L').sum() / denominator,
    'const': params['const'] / denominator  # Optional: long-run intercept
})

print(long_run_effects)
```


### Stata

```{stata}
#| eval: false
#| echo: true
#| code-fold: true
#| collapse: true

* Load the data (same as the 'denmark' dataset in R)
webuse lutkepohl2, clear

* Declare the data as time series
tsset qtr

* Estimate the ARDL model, letting Stata select the best lags up to a max of 4.
* The dependent variable is 'ln_rm' (log real money).
* The independent variables are 'ln_inc' (log income) and 'ln_consump' (log consumption).
* We'll use the variables from the Stata dataset.
ardl ln_rm ln_inc, maxlags(4)

* After estimation, Stata shows the optimal lags and the short-run results.

* To see the long-run coefficients, use the post-estimation command `estat ec`.
estat ec
```

:::

# Prediction

## What is Forecasting?

:::{.callout-note title="Definition: Forecasting"}
To predict future values of a time series based on its own past and potentially other related variables.
:::

- We use the information available up to time $t$ to form the best possible guess about the value of the series at a future time $t+h$.
  -  **Information Set ($I_t$):** All known values of the series (and other relevant variables) up to and including time $t$.
    $I_t = \{Y_t, Y_{t-1}, ...\}$
  - **Forecast Horizon ($h$):** How many steps into the future we are predicting.
    - $h=1$: One-step-ahead forecast.
    - $h>1$: Multi-step-ahead forecast.

- We want to find the *conditional expectation* of the future value, given our current information.

  $$
    \text{Forecast} = E[Y_{t+h} | I_t]
  $$



## Evaluating Forecasts

- Before we make forecasts, we need to know how to measure their accuracy.
- We do this by comparing the actual, observed values ($Y_t$) with our forecasted values ($\hat{Y}_t$).

:::{.callout-note title="Definition: Forecast Error"}

The forecast error is the difference between an actual observation and a forecast. 

  $$
    e_t = Y_t - \hat{Y}_t
  $$
:::

## Common Evaluation Metrics:

- **Mean Absolute Error (MAE):** $\text{MAE} = \frac{1}{n} \sum_{t=1}^{n} |Y_t - \hat{Y}_t|$
  - **Interpretation:** The average absolute size of the errors. Easy to understand and robust to outliers.

- **Mean Squared Error (MSE):** $\text{MSE} = \frac{1}{n} \sum_{t=1}^{n} (Y_t - \hat{Y}_t)^2$
  - **Interpretation:** Penalizes large errors much more heavily than small ones due to the squaring. Widely used in statistics.

- **Root Mean Squared Error (RMSE):** $\text{RMSE} = \sqrt{\text{MSE}}$
  - **Interpretation:** Same units as the original data, making it more interpretable than MSE.


## Relative and Percentage Errors

- Sometimes the scale of the data matters. An error of 10 is huge for a series with an average of 5, but tiny for a series with an average of 5,000.

- **Scale-Free Metric:**
  - **Mean Absolute Percentage Error (MAPE):** $\text{MAPE} = \left( \frac{1}{n} \sum_{t=1}^{n} \left| \frac{Y_t - \hat{Y}_t}{Y_t} \right| \right) \times 100\%$
    - **Interpretation:** The average percentage error. Very intuitive ("we were off by about 5% on average").
    - Can be problematic if actual values ($Y_t$) are close to or equal to zero.

- **In-Sample vs. Out-of-Sample Evaluation:**
  -  **In-Sample:** Evaluating the model on the same data used for training. Tells you about model *fit*.
  -  **Out-of-Sample:** Splitting your data (e.g., 80% train, 20% test). You train the model on the first part and evaluate its forecasting performance on the part it has never seen. 
    - **This is the true test of a forecast model!**


## Forecasting with AR Models

- Let's start predicting with a simple AR(p) model.

  $$
    Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \varepsilon_t
  $$

- Produce forecasts by **iterative substitution**: 

  1.  **One-Step-Ahead Forecast ($h=1$):**
      *   We want $E[Y_{t+1} | I_t]$. The future error term has an expectation of zero, $E[\varepsilon_{t+1}|I_t] = 0$.
      *   We simply plug in all the *known* values of Y up to time $t$:
      $$
      \hat{Y}_{t+1|t} = c + \phi_1 Y_t + \phi_2 Y_{t-1} + \dots + \phi_p Y_{t-p+1}
      $$

  2.  **Two-Step-Ahead Forecast ($h=2$):**
      *   We want $E[Y_{t+2} | I_t]$. The formula is $Y_{t+2} = c + \phi_1 Y_{t+1} + \dots$
      *   The problem: We don't know $Y_{t+1}$!
      *   **Solution:** We substitute our forecast for it, $\hat{Y}_{t+1|t}$.
      $$
      \hat{Y}_{t+2|t} = c + \phi_1 \mathbf{\hat{Y}_{t+1|t}} + \phi_2 Y_t + \dots + \phi_p Y_{t-p+2}
      $$

- For stationary AR processes, the long-run forecast (as $h \to \infty$) will converge to the unconditional mean of the process: $E[Y] = \frac{c}{1 - \sum \phi_i}$.

## Forecasting with MA Models

- Now for an MA(q) model. The "memory" of these models is very different.

  $$
    Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}
  $$

  1.  **One-Step-Ahead Forecast ($h=1$):**
      *   We want $E[Y_{t+1} | I_t]$. The equation is $Y_{t+1} = \mu + \varepsilon_{t+1} + \theta_1 \varepsilon_t + \dots$
      *   The only future term is $\varepsilon_{t+1}$, whose expectation is 0. We can estimate the past error terms from the data (using residuals).
      $$
      \hat{Y}_{t+1|t} = \mu + \theta_1 \varepsilon_t + \dots + \theta_q \varepsilon_{t-q+1}
      $$

  2.  **Two-Step-Ahead Forecast ($h=2$):**
      *   $Y_{t+2} = \mu + \varepsilon_{t+2} + \theta_1 \varepsilon_{t+1} + \dots$
      *   Both $\varepsilon_{t+2}$ and $\varepsilon_{t+1}$ are unknown future shocks. Their conditional expectation is 0.
      $$
      \hat{Y}_{t+2|t} = \mu + \theta_2 \varepsilon_t + \dots + \theta_q \varepsilon_{t-q+2}
      $$

- For any forecast horizon $h$ greater than the order $q$ ($h > q$), all the error terms in the equation will be in the future.
  - $\mathbf{\hat{Y}_{t+h|t} = \mu}$ for all $h > q$.
  - The forecast quickly reverts to the process mean. MA models have a "short memory."

## Forecasting with ARMA(p,q) Models

- An ARMA model combines the features of both AR and MA processes.

  $$
    Y_t = c + \phi_1 Y_{t-1} + \dots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}
  $$

- **Forecasting combines the two previous methods:**
  - For the AR part, we use an iterative process, plugging in past *forecasts* for unknown future values of $Y$.
  - For the MA part, the influence of past shocks ($\varepsilon_t$) fades and disappears after $q$ periods.

:::{.callout-tip title="Example: ARMA(1,1) Forecast"}
- $Y_{t+1} = c + \phi_1 Y_t + \varepsilon_{t+1} + \theta_1 \varepsilon_t$
  - Forecast $\hat{Y}_{t+1|t} = c + \phi_1 Y_t + \theta_1 \varepsilon_t$

- $Y_{t+2} = c + \phi_1 Y_{t+1} + \varepsilon_{t+2} + \theta_1 \varepsilon_{t+1}$
  - Forecast: $\hat{Y}_{t+2|t} = c + \phi_1 \mathbf{\hat{Y}_{t+1|t}} \quad (\text{The MA term vanishes as } E[\varepsilon_{t+1}|I_t]=0)$
  
The AR component gives the forecast a persistent, decaying memory, while the MA component captures the effect of short-term, finite shocks.

:::

## Forecasting with ARDL(p,q) Models

- Why limit ourselves to just the past of Y? Other variables might help predict it.

  $$
    Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \sum_{j=0}^{q} \beta_j X_{t-j} + \varepsilon_t
  $$

- The process is similar to a standard AR forecast, but with an added component.
  - **One-Step-Ahead Forecast ($h=1$):**
    $$
      \hat{Y}_{t+1|t} = c + \sum_{i=1}^{p} \phi_i Y_{t-i+1} + \sum_{j=0}^{q} \beta_j X_{t-j+1}
    $$
  - Simply plug in all known values for past $Y$ and past $X$.

## Challenge of Multivariate Forecasting

- To forecast $Y_{t+1}$, the model requires the value of $X_{t+1}$ if $\beta_0 \neq 0$!
- To forecast $Y_{t+h}$, you need values for $X_{t+h}, X_{t+h-1}, \dots, X_{t+1}$.

- **Solutions:**
  1.  **Build a separate forecast model for X.** (e.g., use an ARIMA model to forecast future temperatures).
  2.  **Create scenarios for X.** (e.g., "What will sales be if we assume a 10% increase in our advertising budget ($X$)?").
  3.  **Only use lagged X's.** If $\beta_0 = 0$, then you only need $X$ values up to time $t$ to predict $Y_{t+1}$, which are known.

## Takeaways

- **The Foundation is Conditional Expectation:** all standard forecasting methods are an attempt to calculate $E[Y_{t+h} | I_t]$.

- Test your forecasts on out-of-sample data using metrics like MAE, RMSE, or MAPE to understand their real-world performance.

- **Model Structure dictates forecast behavior:**
  - **AR(p):** Forecasts are iterative and have long memory. They converge slowly to the mean.
  - **MA(q):** Forecasts have short memory. They revert to the mean after $q$ periods.
  - **ARMA(p,q):** A hybrid approach capturing both long-term persistence and short-term shocks.
  - **ARDL(p,q):** Incorporates external information but introduces the challenge of needing to forecast your predictors ($X$ variables).

## Other Topics in Time Series Econometrics

- This lecture has been an introduction. More advanced topics include:

  - **Testing for Stationarity:** Formal tests (like the Dickey-Fuller test) to determine if a series has a unit root.
  - **Cointegration:** A method for analyzing the long-run relationships between non-stationary variables (the proper way to handle trending variables that are truly related).
  - **Volatility Modeling (ARCH/GARCH):** Modeling the changing variance of a time series, which is crucial in finance.


# Summary

## What did we do?

- **Univariate Time Series**: We looked at ways to analyze a univariate time series, in particular, at:
  - **Autoregressive models**: Explain a variable using its own past values.
  -  **Moving Average models**: Explain a variable using past random shocks.

- **The Linear Model**: We focused on the possibility of using OLS on bivariate time series data. 
  - We saw some cases (where $X_t$ is stationary) in which it is allowed to estimate $Y_t = \alpha + \beta X_t + u_t$
  - If we have **stationary** errors, this might be a sensible reason that this assumption is justified. 
  - However, if not, we run into the problem of **spurious regression:** a misleading relationship between two non-stationary time series.


## What did we do? (Cont.)

- **Distributed Lag (DL) Models:**
  - To model a more realistic dynamic structure, we focused on models that investigate the dynamic impact of an explanatory variable over time, allowing us to distinguish between short-run and long-run effects. These models make it more likely that the error term is white noise. 

- **Autoregressive Distributed Lag Models**:
  - A generalization of distributed lag models, also integrating autoregressive components. A model like this has the ability to separate feedback effects from long-run impacts of $X$. 
  
- **Forecasting**: 
  - We saw all of these models in a forecasting setting. We discussed conditional expectation as the main vehicle for forecasting, talked about particular processes and discussed some of the evaluation metrics. 
  
# The End