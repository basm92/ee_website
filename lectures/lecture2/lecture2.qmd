---
title: "Empirical Economics"
subtitle: "Lecture 2: The Linear Model II"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 2 - The Linear Model II'
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")

library(ggplot2); library(modelsummary); library(tidyverse); library(ggpp)
# Generate some plausible data for wage and education
set.seed(123)
n <- 100
educ <- round(rnorm(n, 13, 2))
educ[educ < 8] <- 8
educ[educ > 20] <- 20
# u is the error term, correlated with nothing but adds noise
u <- rnorm(n, 0, 3.5)
# wage = beta0 + beta1*educ + u
wage <- 1.5 + 1.2 * educ + u
wage[wage < 2] <- 2
dat <- data.frame(wage = wage, educ = educ)

# Add experience as another variable
exper <- round(rnorm(n, 15, 5))
exper[exper < 0] <- 0
# wage_mlr = beta0 + beta1*educ + beta2*exper + u
wage_mlr <- 1.5 + 1.1 * educ + 0.2 * exper + rnorm(n, 0, 3)
dat_mlr <- data.frame(wage = wage_mlr, educ = educ, exper = exper)
slr_model <- lm(wage ~ educ, data = dat)
```


# Outline

## Course Overview

- Linear Model I
- Linear Model II
- Time Series and Prediction
- Panel Data I
- Panel Data II
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Instrumental Variables

## What do we do today?

- First two lectures devoted to the linear model.

- Prequisite knowledge:
  - How do we model the processes that might have generated our data?
  - How do we summarize and describe data, and try to uncover what process may have generated it?
  - Probability and statistics

- This lecture:
  - Multiple linear regression, hypotheses tests, F-tests, Interactions, Understanding Statistical Output
  
- **Material:** Wooldridge Chapter 3


# Multiple Regression

## Introduction to Multiple Linear Regression

- Simple Linear Regression is often inadequate because we can't control for other factors that might be important. This leads to omitted variable bias.

- The solution is to include those other factors in the model. 

:::{.callout-note title="Definition: Multiple Linear Regression Model"}

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k + u
$$

Now we have $k$ explanatory variables.

- $\beta_j$ is the effect of a one-unit change in $x_j$ on $y$, **holding all other explanatory variables ($x_1, ..., x_{j-1}, x_{j+1}, ... x_k$) constant.**
- This is the concept of *ceteris paribus* (all else equal). MLR allows us to isolate the effect of one variable while mathematically controlling for the others.
:::

## OLS Estimation in MLR

- The principle is the same: we choose $\hat{\beta}_0, \hat{\beta}_1, ..., \hat{\beta}_k$ to minimize the Sum of Squared Residuals (SSR). 

- The formulas are complex (usually done with matrix algebra) but are easily handled by software:

  - `lm(y ~ x1 + x2, data=df)` in R
  - `reg y x1 x2` in Stata
  - `pf.feols("y ~ x1 + x2", data=df)` after `import pyfixest as pf` in Python.^[Some examples use the `statsmodels` package in Python, which is the standard way. However, the `pyfixest` package is easier and more straightforward.] 
  
## Interpretation of MLR

- In multiple linear regression, our primary goal is often to estimate the causal effect of a specific variable of interest (say $X_1$) on an outcome ($Y$). The challenge is that in the real world, many factors are changing at once.

- The purpose of control variables ($X_2, X_3, \dots$) is to statistically "hold constant" other relevant factors that could be confounding our results.

- The validity of your estimated causal effect hinges on choosing the right controls. Including the wrong ones can be worse than including none at all.

## Good vs. Bad Controls

- A good control variable is a _confounder_. A confounder is a pre-treatment variable that is correlated with both your variable of interest ($X_1$) and your outcome ($Y$).
- Failing to control for a confounder leads to omitted variable bias (OVB). The coefficient $\beta_1$ will incorrectly absorb the effect of the missing variable.

:::{.callout-tip title="Example: Good Control Variable"}

In estimating the effect of education ($X_1$) on wages ($Y$), innate ability is a classic confounder. Ability is likely correlated with how much education someone pursues and their potential earnings. 

Controlling for a proxy of ability (like an IQ test score, $X_2$) helps to prevent the estimated return to education from being biased upwards.

:::

## Bad Controls

- Bad controls are variables that, when included in a regression, actually induce bias or obscure the true causal relationship. There are two kinds of them:
  - An intermediate variable that lies on the causal pathway between your variable of interest and the outcome.
    - Controlling for a mediator closes off one of the channels through which $X_1$ affects $Y$. You are essentially holding constant a part of the effect you want to measure.
  - A collider variable that is causally influenced by both the variable of interest ($X_1$) and the outcome ($Y$).
    - Controlling for a collider can induce a spurious statistical association between $X_1$ and $Y$, even if none exists.

:::{style="font-size: 0.7em;"}

:::{.callout-tip title="Example: Bad Control"}

Suppose we study the relationship between having a MSc degree ($X_1$) and having a start-up idea ($Y$) among people who receive research grants ($Z$). Getting a grant ($Z$) may be caused by both having a MSc and having a good idea. 

If we only look at people who received grants (i.e., control for $Z$), we might find a spurious negative correlation. Within the grant-recipient pool, someone with a MSc but a weak idea could get the grant, as could someone without a MSc but a brilliant idea. This creates an artificial trade-off in our selected sample.

:::

:::

## Variance of OLS Estimators in MLR

:::{.callout-note title="Definition: (Estimated) Variance of the OLS Estimator (Multivariate)"}

$$
\widehat{Var}(\hat{\beta}_j) = \frac{\hat{\sigma}^2}{SST_j (1 - R_j^2)}
$$

- $SST_j$ is the total variation in $x_j$: $\sum_{i=1}^N (x_{ij} - \bar{x}_j)^2$
- $R_j^2$ is the R-squared from a regression of $x_j$ on all other explanatory variables in the model.

:::

- The variance of a coefficient $\hat{\beta}_j$ now depends on **multicollinearity** -- how correlated $x_j$ is with the *other* explanatory variables.
  - If $x_j$ is highly correlated with other $x$'s, $R_j^2$ will be close to 1, making the denominator small and $Var(\hat{\beta}_j)$ very large. This is **imperfect multicollinearity**.
  - The **no perfect collinearity** assumption for MLR means that no $x_j$ can be a perfect linear combination of the others (i.e., $R_j^2 \neq 1$).

## Hypothesis Testing in Multiple Regression

- Once we've estimated our model, $\widehat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \dots + \hat{\beta}_k x_k$, we need to ask: **Is the relationship we found statistically significant?**
  - Our estimates $\hat{\beta}_0$ and $\hat{\beta}_j$ are based on a *sample* of data. They are subject to sampling variability.
  - It's possible that the true relationship in the *population* is zero ($\beta_1 = 0$), and we just found a non-zero $\hat{\beta}_1$ by random chance.

- In addition to the $t$-test, introduced with the Simple Linear Regression model, we also have the $F$-test:
  - This tests the *joint* significance of multiple coefficients or the model as a whole.

## The $t$-Test: Significance of a Single Coefficient

- The $t$-test is our tool for testing a hypothesis about a single coefficient.
  - The most common hypothesis is that a variable has no effect on the dependent variable.
- Hypotheses for a single coefficient $\beta_j$:
  - **Null Hypothesis ($H_0$)**: The variable has no effect. $H_0: \beta_j = 0$
  - **Alternative Hypothesis ($H_A$)**: The variable does have an effect. $H_A: \beta_j \neq 0$
  - We calculate the t-statistic, which measures how many standard errors our estimated coefficient is away from the hypothesized value (zero).

:::{.callout-note title="The $t$-statistic (Multivariate)"}
$$
t = \frac{\text{Estimate} - \text{Hypothesized Value}}{\text{Standard Error}} = \frac{\hat{\beta}_j - 0}{se(\hat{\beta}_j)}
$$

where the $se(\hat{\beta}_j)$ is the square root of $Var(\hat{\beta}_j)$ as presented earlier. 

:::

## The F-Test: Testing Joint Significance

- The F-test is used to test hypotheses about **multiple** coefficients at the same time.
  - Its most common use is to test the **overall significance** of the regression model.

:::{.callout-note title="F Test for Overall Significance"}
This tests whether *any* of our independent variables have an effect on the dependent variable.

Model: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + u$

**Null Hypothesis ($H_0$)**: *None* of the independent variables have an effect on $y$. The model has no explanatory power. $H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0$

**Alternative Hypothesis ($H_A$)**: *At least one* of the coefficients is not zero. The model has some explanatory power. $H_A: \text{At least one } \beta_j \neq 0 \text{ for } j=1, \dots, k$

:::

## F-statistic

- In general, the F-test serves to compare a "restricted model", where _some_ of the $\beta$ coefficients are zero under a null hypothesis, against an "unrestricted" model where coefficients are allowed to vary. 
- A **large F-statistic** suggests that the unrestricted model explains significantly more variation in $y$ than the restricted model. 
- Like the t-test, we typically look at the **p-value** for the F-statistic. If $p < 0.05$, we reject the null and conclude our model is jointly significant.

## F-statistic Procedure

:::{.callout-note title="F Statistic: Definition (General)"}
The F statistic is defined as:

$$
F = \frac{(SSR_{restricted} - SSR_{unrestricted}) / q}{SSR_{unrestricted} / (n - k - 1)}
$$

**Where:**

*   $SSR_{restricted}$ = Sum of Squared Residuals from the restricted model (the model with fewer predictors, where the null hypothesis is assumed to be true).
*   $SSR_{unrestricted}$ = Sum of Squared Residuals from the unrestricted model (the model with more predictors).
*   $q$ = The number of restrictions being tested. This is essentially the number of parameters that are set to zero in the restricted model. You can often simply count the number of equals signs in your null hypothesis to determine the value of q. For instance, if your null hypothesis is $H_0: \beta_1 = \beta_2 = 0$, then $q=2$ because you are imposing two restrictions.
*   $n$ = The number of observations in your sample.
*   $k$ = The number of parameters in the unrestricted model (including the intercept).

:::

## F Distribution: Visualization

- The $F$ distribution comes with three parameters, $n$, $k$ and $q$ as defined in the previous slide. 

:::{.callout-tip title="Example: F Distribution"}

```{r}
#| fig-align: 'center'
#| fig-width: 14
#| fig-height: 4.5
# Define a sequence of x-values (potential F-statistics) for the plot
x_values <- seq(0, 6, by = 0.01)

# --- Define Parameters for Four Different Scenarios ---

# Scenario 1: Base case (large sample, few restrictions)
q1 <- 2
n1 <- 100
k1 <- 5
# Denominator df = 100 - 5 - 1 = 94
density1 <- df(x_values, df1 = q1, df2 = n1 - k1 - 1)

# Scenario 2: More restrictions (q is larger)
q2 <- 10
n2 <- 100
k2 <- 5
# Denominator df = 100 - 5 - 1 = 94
density2 <- df(x_values, df1 = q2, df2 = n2 - k2 - 1)

# Scenario 3: Smaller sample size (n is smaller)
q3 <- 2
n3 <- 30
k3 <- 5
# Denominator df = 30 - 5 - 1 = 24
density3 <- df(x_values, df1 = q3, df2 = n3 - k3 - 1)

# Scenario 4: Fewer degrees of freedom (n is small relative to k)
q4 <- 2
n4 <- 15
k4 <- 10
# Denominator df = 15 - 10 - 1 = 4
density4 <- df(x_values, df1 = q4, df2 = n4 - k4 - 1)

# --- Create the Plot ---

# Set up the initial plot using the first distribution's data
plot(x_values, density1,
     type = "l",          # 'l' for line plot
     col = "blue",
     lwd = 2,             # Line width
     main = "F-Distributions for Different q, n, and k",
     xlab = "F-Statistic Value",
     ylab = "Probability Density",
     ylim = c(0, 1.0))    # Ensure all curves are visible

# Add the other distributions to the same plot
lines(x_values, density2, col = "red", lwd = 2, lty = 2)
lines(x_values, density3, col = "darkgreen", lwd = 2, lty = 3)
lines(x_values, density4, col = "purple", lwd = 2, lty = 4)

# Add a legend to explain which line is which
legend("topright",
       legend = c(
         paste0("q=", q1, ", n=", n1, ", k=", k1, "  (df=2, 94)"),
         paste0("q=", q2, ", n=", n2, ", k=", k2, "  (df=10, 94)"),
         paste0("q=", q3, ", n=", n3, ", k=", k3, "  (df=2, 24)"),
         paste0("q=", q4, ", n=", n4, ", k=", k4, "  (df=2, 4)")
       ),
       col = c("blue", "red", "darkgreen", "purple"),
       lwd = 2,
       lty = c(1, 2, 3, 4), # Line types
       title = "Parameters (q, n, k)")
```

:::


## Summary: $t$-test vs. $F$-test

- It's crucial to understand when to use each test. 
  - A group of variables can be jointly significant (F-test) even if no single variable is individually significant (t-tests).

:::{style="font-size: 1.2em;"}

| Feature | t-test | F-test |
| :--- | :--- | :--- |
| **Scope** | One coefficient at a time | Two or more coefficients at a time |
| **Typical Use**| Is *this specific variable* significant? | Is this *group of variables* jointly significant? OR Is the *model as a whole* useful?|
| **Null Hypothesis** | $H_0: \beta_j = 0$ | $H_0: \beta_1 = \beta_2 = \dots = 0$ |
| **Test Statistic**| $t = \frac{\hat{\beta}_j}{se(\hat{\beta}_j)}$ | Compares Restricted vs. Unrestricted sum of squares |
| **Key Question** | "Does education significantly affect wage, holding other factors constant?" | "Does a person's work experience, measured by `exper` and `exper^2`, jointly affect their wage?" |

:::

# Interactions and Dummies in MLR

## Dummies in Multiple Regression 

- The power of dummy variables becomes clear when we add other regressors. Let's add years of education (`Educ`) to our model:

  $$
    Wage_i = \beta_0 + \beta_1 Female_i + \beta_2 Educ_i + u_i
  $$

- We again analyze the regression equation for each group, now holding `Educ` constant.
  - **For Males ($Female_i = 0$):**
    - $E[Wage_i | Female_i=0, Educ_i] = \beta_0 + \beta_2 Educ_i$
    - This is the wage-education profile for males. It's a line with intercept $\beta_0$ and slope $\beta_2$.
  - **For Females ($Female_i = 1$):**
    - $E[Wage_i | Female_i=1, Educ_i] = \beta_0 + \beta_1(1) + \beta_2 Educ_i = (\beta_0 + \beta_1) + \beta_2 Educ_i$
    - This is the wage-education profile for females. It's a line with a **different intercept**, $(\beta_0 + \beta_1)$, but the **same slope**, $\beta_2$.

## Reinterpreting Dummies

- $\beta_0$: The intercept for the reference group (males). It is the predicted wage for a male with zero years of education.
- $\beta_1$: The difference in the intercept between females and males. It is the predicted wage difference between a female and a male *who have the same level of education*.
- $\beta_2$: The slope of the wage-education profile. It represents the change in wage for an additional year of education, and this model **constrains** that effect to be the same for both men and women.


## Visualization

- **Graphical Intuition:** This model generates two **parallel regression lines**. They have the same slope ($\beta_2$), but are separated vertically by a distance of $\beta_1$.


```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
# Load necessary libraries
library(ggplot2)
library(dplyr)

# For reproducibility
set.seed(42)

# 1. Define parameters for our data simulation
n <- 200 # Total number of observations
beta0 <- 20 # Base intercept (predicted wage for a male with 0 education)
beta1_gender <- -3 # The wage "penalty" for being female
beta2_educ <- 2.5 # The return to one year of education

# 2. Create the base data frame
wage_data <- tibble(
  # Create 100 males and 100 females
  gender = factor(rep(c("Male", "Female"), each = n/2)),
  # Give each person a random number of education years
  education = runif(n, min = 0, max = 8)
)

# 3. Create the dependent variable (wage)
# We build the "true" relationship into the data, plus some random noise
wage_data <- wage_data %>%
  mutate(
    # Create a numeric dummy variable for the formula
    is_female = ifelse(gender == "Female", 1, 0),
    
    # Calculate wage based on the PARALLEL lines model
    # Wage = b0 + b1*Female + b2*Educ + noise
    wage = beta0 + (beta1_gender * is_female) + (beta2_educ * education) + rnorm(n, mean = 0, sd = 4)
  )

model_parallel <- lm(wage ~ education + gender, data = wage_data)

ggplot(wage_data, aes(x = education, y = wage, color = gender)) +
  geom_point(alpha = 0.7) + # Plot the raw data points
  geom_smooth(method = "lm", se = FALSE, size = 1.2) + # Add the regression lines
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Dummy Variable as an Intercept Shift",
    subtitle = "Model: wage ~ education + gender",
    x = "Years of Education",
    y = "Predicted Wage",
    color = "Gender"
  ) +
  theme_minimal(base_size = 14)

```


## Interaction Effects

- The standard linear regression model  $\text{Wage}_i = \beta_0 + \beta_1 \text{Gender}_i + \beta_2 \text{Educ}_i + u_i$ assumes the effect of education on wages ($\beta_2$) is identical for men and women. 

- What if an extra year of education has a different return for females than for males? To allow for this, we must let the slope differ between the groups. We do this by adding an **interaction term**.

- The interaction term is simply the product of the dummy variable and the continuous variable.
$Wage_i = \beta_0 + \beta_1 Female_i + \beta_2 Educ_i + \beta_3 (Female_i \cdot Educ_i) + u_i$

- Once more, we derive the regression line for each group.
  - **For Males ($Female_i = 0$):** The dummy and interaction term are both zero.
    - $E[Wage_i | Female_i=0, Educ_i] = \beta_0 + \beta_2 Educ_i$
    - The intercept is $\beta_0$ and the slope is $\beta_2$. This is the baseline profile.

  - **For Females ($Female_i = 1$):**
    - $E[Wage_i | Female_i=1, Educ_i] = \beta_0 + \beta_1(1) + \beta_2 Educ_i + \beta_3 (1 \cdot Educ_i)$
    - Group the terms by the constant and the variable `Educ`:
    - $E[Wage_i | Female_i=1, Educ_i] = (\beta_0 + \beta_1) + (\beta_2 + \beta_3) Educ_i$
    - For females, the intercept is $(\beta_0 + \beta_1)$ and the slope is $(\beta_2 + \beta_3)$.

## Interpreting Interaction Terms

- The interpretation of the "main effects" ($\beta_1$ and $\beta_2$) changes fundamentally when an interaction term is present.

- **$\beta_0$ (Intercept):** The expected wage for the reference group (males) when `Educ` = 0.
- **$\beta_2$ (Main Effect of Continuous Variable):** The effect of an additional year of education on wages *for the reference group (males) only*.
- **$\beta_1$ (Main Effect of Dummy):** The difference in expected wages between females and males when `Educ` = 0. This is the difference in the intercepts. This coefficient is often not meaningful on its own if `Educ=0` is not a relevant value in the data.
- **$\beta_3$ (Interaction Coefficient):** The **difference in the slopes**. It measures by how much the effect of an additional year of education differs for females compared to males. This is often the coefficient of primary interest.

## Visualization

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
# Define a coefficient for the interaction effect
beta3_interaction <- -1.5 # Return to education is $0.50 lower for women

# Add a new wage variable to our data frame
wage_data <- wage_data %>%
  mutate(
    # Wage = b0 + b1*Female + b2*Educ + b3*Female*Educ + noise
    wage_interact = beta0 + (beta1_gender * is_female) + 
                    (beta2_educ * education) + 
                    (beta3_interaction * is_female * education) + 
                    rnorm(n, mean = 0, sd = 4)
  )

# Model 2: With an interaction term
# The `*` is a shortcut for `education + gender + education:gender`
model_interact <- lm(wage_interact ~ education * gender, data = wage_data)
ggplot(wage_data, aes(x = education, y = wage_interact, color = gender)) +
  geom_point(alpha = 0.7) + # Plot the raw data points
  geom_smooth(method = "lm", se = FALSE, size = 1.2) + # Add the regression lines
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Interaction Effects Allowing for Different Slopes",
    x = "Years of Education",
    y = "Predicted Wage",
    color = "Gender"
  ) +
  theme_minimal(base_size = 14)
```

## Total Effect Under Interaction 

- The effect of a variable is no longer a single coefficient but may be a function of another variable.

:::{.callout-tip title="Example: Education Gender Interaction"}
- **The marginal effect of Education for Females is:**
    $\frac{\partial E[wage_i | Female_i=1, Educ_i]}{\partial Educ_i} = \beta_2 + \beta_3$

- **The wage differential between Females and Males is:**
    $E[wage|F=1] - E[wage|F=0] = ((\beta_0 + \beta_1) + (\beta_2 + \beta_3)Educ_i) - (\beta_0 + \beta_2 Educ_i) = \beta_1 + \beta_3 Educ_i$. The wage gap is no longer constant; it depends on the level of education.
    
:::

- Hypothesis Testing: 
  - To test if education has a different effect for females, the null hypothesis is $H_0: \beta_3 = 0$.
  - To test if gender has any effect on wages at all, you must test if the lines are identical, which requires a joint test: $H_0: \beta_1 = 0 \text{ and } \beta_3 = 0$. This is done with an F-test.

# Understanding Statistical Output

## Understanding Statistical Output

- By now, we can understand virtually all of the standard statistical output from R/Python/Stata.
- Let's look at the full output from R/Python/Stata for our simple regression.

:::{.panel-tabset .scroll-container}

### R

```{r}
#| echo: true
#| collapse: true
#| code-fold: true

mlr_model <- lm(wage ~ educ + exper, data = dat_mlr)
summary(mlr_model)
```

### Python

```{python}
#| echo: true
#| collapse: true
#| code-fold: true

import statsmodels.api as sm
import pandas as pd
# Create a DataFrame for X (automatically keeps variable names)
X = pd.DataFrame({
    'educ': r.dat_mlr['educ'],
    'exper': r.dat_mlr['exper']
})

X = sm.add_constant(X)  # Adds 'const' column
y = r.dat_mlr['wage']
model = sm.OLS(y, X).fit()
print(model.summary())
```

### Stata

```{stata}
#| eval: false
#| echo: true
#| collapse: true
#| code-fold: true

reg wage educ expr
```

:::


## Understanding Statistical Output (Cont.)

- **Coefficients:**
  - `Estimate` or `coef`: These are $\hat{\beta}_0$ (Intercept), $\hat{\beta}_1$ (educ) and $\hat{\beta}_2$ (exper).
  - `Std. Error`: The standard errors of the estimates, $se(\hat{\beta}_j)$, which measure their sampling uncertainty.
  - `t value`: The t-statistic used for hypothesis testing (`Estimate` / `Std. Error`).
  - `Pr(>|t|)`: The p-value for the t-test.

- **Goodness-of-Fit:**
  - `Residual standard error` (R only): This is the **SER** (`r round(summary(mlr_model)$sigma, 2)`).
  - `R-squared`: This is our **R-squared** (`r round(summary(slr_model)$r.squared, 2)`). The model explains about 28% of the variation in wages.


## Interpretation with Controls

- educ: $\hat{\beta}_1 \approx 1.02$: Holding experience constant, one more year of education is associated with a €1.02/hr increase in wages, on average.
- exper: $\hat{\beta}_2 \approx 0.16$: Holding education constant, one more year of experience is associated with a €0.16/hr increase in wages, on average.
- The R-squared increased relative to the simple model, suggesting this model explains more variation in wages.
- The t-statistics tell us whether these coefficients are statistically distinguishable from zero.

# Robust Standard Errors

## OLS Variance

- Our goal is to find the variance of the OLS estimator, $Var(\hat{\beta}_1)$, which is the basis for all statistical inference. Conditional on the regressors $x_i$, we have:^[Strictly speaking, we are evaluating the _conditional variance_ here, i.e. $Var(\beta_1 | X_i)$. But for brevity and by convention, we use $Var(\beta_1)$. We also focus on the case with 1 regressor in this exposition. The formulae become more complicated with $k$ regressors, but the insights generalize.]

  $$
    Var(\hat{\beta}_1) = \frac{1}{\left(\sum (x_i - \bar{x})^2\right)^2} Var\left(\sum_{i=1}^N (x_i - \bar{x})u_i\right)
  $$

- To evaluate the variance of the sum, we must make assumptions about the error terms, $u_i$. The standard output in R/Python/Stata imposes two key assumptions on the errors:
  1.  **No serial correlation:** The errors are uncorrelated across observations. Mathematically, $Cov(u_i, u_j) = 0$ for all $i \neq j$.
  2.  **Homoskedasticity:** The errors have a constant variance. Mathematically, $Var(u_i) = \sigma^2$ for all $i$.

## OLS with Homoskedasticity

- Under the no-serial-correlation assumption, the variance of the sum is simply the sum of the variances:

  $$
    Var\left(\sum_{i=1}^N (x_i - \bar{x})u_i\right) = \sum_{i=1}^N Var\left((x_i - \bar{x})u_i\right) = \sum_{i=1}^N (x_i - \bar{x})^2 Var(u_i)
  $$

- Now, we impose the **homoskedasticity** assumption ($Var(u_i) = \sigma^2$). Since $\sigma^2$ is a constant, we can factor it out of the summation:

  $$
    = \sum_{i=1}^N (x_i - \bar{x})^2 \sigma^2 = \sigma^2 \sum_{i=1}^N (x_i - \bar{x})^2
  $$

- Plugging this back into our original expression for $Var(\hat{\beta}_1)$ gives the famous result:

  $$
    Var(\hat{\beta}_1) = \frac{\sigma^2 \sum (x_i - \bar{x})^2}{\left(\sum (x_i - \bar{x})^2\right)^2} = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}
  $$


## The Problem of Heteroskedasticity

- The assumption of homoskedasticity ("same scatter") is often violated in economic data. 
- **Heteroskedasticity** occurs when the variance of the error term is *not* constant across observations.

  -  **Formal Definition:** $Var(u_i) = \sigma_i^2$. The variance is indexed by `i`, meaning it can take on a different value for each observation.
  - **Intuitive Example:** Consider a regression of household food expenditure on household income. Low-income households have limited budgets, so their food spending will be tightly clustered around a certain amount (low variance). High-income households have more discretion; some may spend a lot on gourmet food while others spend relatively little, leading to a much wider spread of data points (high variance). The variance of the error term, which captures this deviation from the average, increases with income.

- If we ignore this and use the classical formula, our inference can be severely misleading.

## Derivation of HC SEs

- Let's re-derive the variance of $\hat{\beta}_1$ under the more realistic assumption of heteroskedasticity, while still maintaining the no-serial-correlation assumption.

- We return to the step just before we imposed homoskedasticity:

  $$
    Var(\hat{\beta}_1) = \frac{1}{\left(\sum (x_i - \bar{x})^2\right)^2} \sum_{i=1}^N (x_i - \bar{x})^2 Var(u_i)
  $$

- Now, we substitute $Var(u_i) = \sigma_i^2$:

  $$
    Var(\hat{\beta}_1) = \frac{\sum_{i=1}^N (x_i - \bar{x})^2 \sigma_i^2}{\left(\sum_{i=1}^N (x_i - \bar{x})^2\right)^2}
  $$
  
- This is the **true variance of the OLS estimator in the presence of heteroskedasticity**.

## Estimation of HC REs

- The challenge is that we cannot compute the true variance because we do not know the individual error variances, $\sigma_i^2$.

- The insight (White, 1980) is to use the squared OLS residual for each observation, $\hat{u}_i^2 = (y_i - \hat{y}_i)^2$, as an estimator for the unobserved error variance, $\sigma_i^2$. 

- We construct the estimator by taking the correct formula for the variance and "plugging in" $\hat{u}_i^2$ for $\sigma_i^2$. This gives the **heteroskedasticity-robust variance estimator**, also alled the **White estimator** or **HC estimator**:

  $$
    \widehat{Var}_{HC}(\hat{\beta}_1) = \frac{\sum_{i=1}^N (x_i - \bar{x})^2 \hat{u}_i^2}{\left(\sum_{i=1}^N (x_i - \bar{x})^2\right)^2}
  $$

- The square root of this value is the **heteroskedasticity-robust standard error** (or HC-robust SE).

## HC Robust Standard Errors in Software

- By default, you should estimate a regression and compute heteroskedastic standard errors. 
- Because this is not the default behavior in statistical packages, you need to do that manually.
  - In R: `feols(y ~ x1 + x2, data = df, vcov='hc1')`
  - In Python: `pf.feols("y ~ x1 + x2", data = df).vcov("HC1")`
  - In Stata: `reg y x1 x2, robust`

## Summary HC Robust SE

- **What it corrects for:** 
  - HC-robust standard errors correct for heteroskedasticity. They do **not** correct for serial correlation (which requires clustered SEs) or omitted variable bias.
- **When to use:**
  - In cross-sectional data analysis, heteroskedasticity is the rule rather than the exception. The modern consensus among econometricians is to use HC-robust standard errors **by default**.
  - The cost of using them when errors are truly homoskedastic is minimal in large samples, while the cost of failing to use them when heteroskedasticity is present is high (invalid inference).
- **Efficiency:** 
  - It's crucial to remember that OLS is still unbiased and consistent under heteroskedasticity. However, it is not efficient. HC-robust standard errors fix the *inference* (t-tests, p-values), but they do not make the OLS *estimator* itself more efficient. For that, you would need a different estimation method like Weighted Least Squares (WLS).

# Clustered Standard Errors

## OLS Variance Expansion

- We now express the variance of the OLS estimator, $Var(\hat{\beta}_1)$ in a more general form:

  $$
    Var(\hat{\beta}_1) = Var \left( \beta_1 + \frac{\sum_{i=1}^N (x_i - \bar{x})u_i}{\sum_{i=1}^N (x_i - \bar{x})^2} \right) = \frac{1}{\left(\sum (x_i - \bar{x})^2\right)^2} Var\left(\sum_{i=1}^N (x_i - \bar{x})u_i\right)
  $$

- The crucial step is calculating $Var(\sum (x_i - \bar{x})u_i)$. The variance of a sum of random variables is the sum of their variances plus twice the sum of all unique pairwise covariances:

  $$
    Var\left(\sum_{i=1}^N (x_i - \bar{x})u_i\right) = \sum_{i=1}^N (x_i - \bar{x})^2 Var(u_i) + \sum_{i \neq j} (x_i - \bar{x})(x_j - \bar{x}) Cov(u_i, u_j)
  $$

## Classical Assumptions

- The classical linear model makes two critical assumptions here:

  1.  **Homoskedasticity:** $Var(u_i) = \sigma^2$ for all $i$.
  2.  **No serial correlation:** $Cov(u_i, u_j) = 0$ for all $i \neq j$.

- Under these assumptions, the second term (the sum of covariances) vanishes entirely. The formula simplifies:

  $$
    Var(\hat{\beta}_1) = \frac{1}{\left(\sum (x_i - \bar{x})^2\right)^2} \sum_{i=1}^N (x_i - \bar{x})^2 \sigma^2 = \frac{\sigma^2 \sum (x_i - \bar{x})^2}{\left(\sum (x_i - \bar{x})^2\right)^2} = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}
  $$

##  Intra-Cluster Correlation

- In many economic datasets, the assumption that $Cov(u_i, u_j) = 0$ is unrealistic.   
- Examples are students nested within schools, individuals within states, or firms over time (panel data). 

- Unobserved factors at the group level can induce correlation among the error terms within that group.

- This structure is called **clustered data**.

## Notation Clustered SE's

- Let's introduce notation for clusters. Let $g$ index the cluster (e.g., a school) and $i$ index the individual within the cluster. An observation is denoted by $gi$. Our model is now:

  $$
    y_{gi} = \beta_0 + \beta_1 x_{gi} + u_{gi}
  $$

- The key feature of clustered data is the assumed error structure:
  - $Cov(u_{gi}, u_{gj}) \neq 0$ for $i \neq j$ (Correlation **within** a cluster $g$)
  - $Cov(u_{gi}, u_{g'j}) = 0$ for $g \neq g'$ (No correlation **between** clusters)

- If we ignore this structure and use the standard OLS formula, we are incorrectly zeroing out all the covariance terms in the variance calculation. 
  - If these covariance terms are, on average, positive (which is common), we will systematically underestimate the true variance of $\hat{\beta}_1$. This leads to standard errors that are too small. 

## Deriving the Cluster-Robust Variance

- Let's re-derive the variance of $\hat{\beta}_1$ under the clustering assumption. We start again from:

  $$
    Var(\hat{\beta}_1) = \frac{1}{\left(\sum_{g,i} (x_{gi} - \bar{x})^2\right)^2} \color{blue}{Var\left(\sum_{g=1}^G \sum_{i=1}^{N_g} (x_{gi} - \bar{x})u_{gi}\right)}
  $$  {#eq-numerator}

- where $G$ is the number of clusters and $N_g$ is the size of cluster $g$.

- Let's focus on the variance term (the blue part). We can rewrite the sum over individuals as a sum over clusters of within-cluster sums..

  $$
    Var\left(\sum_{g=1}^G \left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})u_{gi} \right) \right)
  $$
  

## Deriving Cluster-Robust Variance (Cont.)

- Since errors are uncorrelated *across* clusters, the variance of this sum is the sum of the variances:

  $$
   Var\left(\sum_{g=1}^G \left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})u_{gi} \right) \right) = \sum_{g=1}^G Var\left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})u_{gi} \right)
  $$

- Now, let's expand the variance term for a *single* cluster $g$. This is where the non-zero covariances appear:

  $$
    Var\left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})u_{gi} \right) = \sum_{i=1}^{N_g} (x_{gi} - \bar{x})^2 Var(u_{gi}) + \sum_{i \neq j \in g} (x_{gi} - \bar{x})(x_{gj} - \bar{x}) Cov(u_{gi}, u_{gj})
  $$

- This expression is the complete variance for cluster $g$. 
  - Notice that it includes all the pairwise covariance terms within the cluster. 
  - The total variance for the numerator of $\hat{\beta}_1$ is the sum of these terms over all $G$ clusters.

## The Cluster-Robust Estimator

- We cannot directly calculate the true variance because the error terms $u_{gi}$ and their variances/covariances are unknown. We must estimate it from the data using the OLS residuals, 
  
  $$
    \hat{u}_{gi} = y_{gi} - \hat{y}_{gi}
  $$

- Let's define a score for each observation: $u_{gi} = (x_{gi} - \bar{x})\hat{u}_{gi}$.
- Let the sum of these scores within a cluster be $u_g = \sum_{i=1}^{N_g} u_{gi} = \sum_{i=1}^{N_g} (x_{gi} - \bar{x})\hat{u}_{gi}$.

## The Cluster-Robust Estimator (Cont.)

- The variance expression we derived for a single cluster, $Var\left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})u_{gi} \right)$, can be thought of as the expected value of the squared sum, $E\left[ \left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})u_{gi} \right)^2 \right]$. ^[Since the expectation of the error term conditional on $X$ is zero, the variance is equal to the expectation squared.]

- A natural estimator for this quantity is simply the squared sum of the estimated scores for that cluster: $(u_g)^2 = \left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})\hat{u}_{gi} \right)^2$.

- By summing this quantity over all clusters, we get an estimate of the total variance of the numerator of $\hat{\beta}_1$^[The blue term in @eq-numerator.]:

  $$
    \widehat{Var}\left(\text{numerator}\right) = \sum_{g=1}^G \left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})\hat{u}_{gi} \right)^2
  $$

## The Cluster-Robust Estimator (Cont.)

- Plugging this back into our main variance formula for $\hat{\beta}_1$, we get the **cluster-robust variance estimator**:

  $$
    \widehat{Var}_C(\hat{\beta}_1) = \frac{1}{\left(\sum_{g,i} (x_{gi} - \bar{x})^2\right)^2} \left[ \sum_{g=1}^G \left( \sum_{i=1}^{N_g} (x_{gi} - \bar{x})\hat{u}_{gi} \right)^2 \right]
  $$
  
- A small-sample correction factor, $\frac{G}{G-1}\frac{N-k}{N-1}$, is typically applied, but the core formula above is the key insight.

## Comparison and Intuition

- Let's compare this to the **heteroskedasticity-robust estimator**:

  $$
    \widehat{Var}_W(\hat{\beta}_1) = \frac{1}{\left(\sum_i (x_i - \bar{x})^2\right)^2} \left[ \sum_{i=1}^N (x_i - \bar{x})^2 \hat{u}_i^2 \right]
  $$

- The HC estimator assumes observations are independent but allows their variances to differ. It estimates the variance contribution of each observation $i$ and sums them up.
- The Clustered estimator relaxes the independence assumption *within* clusters. 
  - It calculates a total score for each cluster (`sum inside`), squares that total (`square outside`), and then sums these squared cluster-level totals. 
  - This procedure implicitly accounts for all the covariance terms within each cluster without having to estimate each one separately.

## When to Cluster

- **When to Cluster:** 
  - Cluster standard errors when you suspect the error terms are correlated within groups. This is common with panel data, data from multi-stage surveys, or when a policy is applied at a group level (e.g., a state-level law).
- **What Level to Cluster at:** 
  - You should cluster at the level at which the unobserved components are shared. If you have students in classrooms within schools, and you suspect shocks at both the classroom and school level, you should cluster at the broader level (school), as this allows for correlation within classrooms *and* between classrooms within the same school.
- **Consequences of Failure to Cluster:** 
  - If intra-cluster correlation exists and you use standard OLS standard errors, your standard errors will be biased downwards. This will make you overly confident in your results and lead to spurious findings of statistical significance.
  
## Clustering in Software

- Similarly to HC-Robust standard errors, clustered standard errors need to be applied explicitly in statistical software. 
  - In R: `feols(y ~ x1 + x2, data = df, vcov=~cluster_variable)`
  - In Python: `pf.feols("y ~ x1 + x2", data = df).vcov({"CRV3": "cluster_variable"}).summary()`
  - In Stata: `reg y x1 x2, vce(cluster cluster_variable)`

# Summary

## What did we do?

- **Multiple Linear Regression**:
  - We introduced the method of multiple linear regression and provided ourselves with an interpretation of the estimated coefficients. 

- **Control Variables**: 
  - For OLS estimates to be unbiased (correct on average), a set of classical assumptions must hold. The most critical is the Zero Conditional Mean assumption, which states that unobserved factors (like innate ability) must not be correlated with the explanatory variable (like education). We discussed the logic of inclusion of control variables to minimize this kind of bias.

- **Hypothesis Testing**: 
  - After estimating a model, we use hypothesis testing to determine if the results are statistically significant.
  - The t-test is used to assess the significance of a single variable, while the F-test is used to assess the joint significance of multiple variables or the overall explanatory power of the model.

## What did we do? (Cont.)

- **Dummies and Interactions**:
  - We discussed the interpretation of dummy variables and interaction terms in a multiple linear regression context. 
  
- **Standard errors**:
  - We discussed the issue of standard errors. We saw that the standard errors statistical software computes by default are almost certainly incorrect. We saw two alternatives, the HC-robust standard error and the Cluster-robust standard error, and focused on their estimation. 
  
# The End


