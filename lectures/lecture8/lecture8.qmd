---
title: "Empirical Economics"
subtitle: "Lecture 8: Hands-on Econometrics"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 8- Hands-on Econometrics'
resources:
  - demo.pdf
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(tidyverse)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Introduction

## Course Overview

- Statistics and Probability - Basic Concepts
- Statistics and Probability - Hypothesis Testing
- The Linear Regression Model
- Time Series Data
- Panel Data (FE) and Control Variables
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Hands-on Econometrics in Practice

## Agenda 

- Data Wrangling and Cleaning
  - Importing Data
  - Pivoting
  - Merging Datasets
  - String Matching
- Summary Tables
- Regression Tables
  - Standard Errors and Fit Statistics
- Visualization
- Project and Code Organization

# Motivation

## Motivation: Why Data Cleaning? 


- In practice, you almost never get a dataset that is entirely ready to analyze. 
- Including for your MSc thesis, where you might have to query, put together, clean and analyze data from various sources. 
- To successfully put all of these data together, you need to know the basis of data cleaning in your preferred programming language. 
- In addition, you want your thesis to have **reliable results**: 
  - Dirty data (missing values, outliers, inconsistencies) can distort econometric models, leading to incorrect conclusions. Proper cleaning ensures robustness.
- Data cleaning also makes your thesis **reproducible**: we expect replicable analyses.
  - Clean, well-documented data allows others to verify your findings.

## Motivation: Why Project & Code Management?

- An empirical economics project such as a thesis is much easier and less frustrating if you think about project and code management up front. 
- Avoid Chaos: A well-organized folder structure (raw data, cleaned data, scripts, outputs) prevents lost files and version disasters.
- Collaboration & Transparency: clear code (with comments!) make it easier for advisors and others to help and for you to revisit your work weeks/months later.
- Scalability: Modular code (e.g., separate scripts for data prep, analysis, and visuals) lets you tweak one part without breaking everything.

# Data Wrangling & Cleaning

## Importing Data

- Most of the data you'll receive is in a structured format

- Pay attention to _delimiters_, if you parse something like this:

- It isn't because the data is corrupted. It's just because the function you have used doesn't recognize the data format

## Importing Data through API

- You can also think about importing data through dedicated libraries or through submitting requests to servers from Python or R 


# Pivoting

## Long and Wide Datasets

- There generally exist two often-ocurring formats for financial and economic data. 

- Long Format:
  - In a long format, each row represents a single observation for a specific variable. 
  - For instance, in a firm-year dataset, you might have separate rows for the revenue of Company A in 2023 and the revenue of the same company in 2024. This format is often easier for data collection and storage.

- Wide Format: 
  - In a wide format, each row represents a single subject (like a company), and the columns represent different variables or observations over time. 
  - For example, you would have a single row for Company A with separate columns for "Revenue 2023" and "Revenue 2024". 



## Standard (Long) Format

- Statistical packages usually assume your data is in the _long format_:

```{python}
#| echo: false
import pandas as pd

# Create a sample long-format dataset
data_long = {
    'Company': ['Company A', 'Company A', 'Company B', 'Company B', 'Company C', 'Company C'],
    'Year': [2023, 2024, 2023, 2024, 2023, 2024],
    'Revenue': [100, 110, 200, 210, 150, 160],
    'Expenses': [80, 85, 150, 155, 120, 125]
}
df_long = pd.DataFrame(data_long)
print(df_long)
```


## Pivoting Datasets

- It often occurs that your data is in a slightly different format. Most often, your data is in a _wide format_:

```{python}
#| echo: false
# Pivot from long to wide format
df_wide = df_long.pivot(index='Company', columns='Year', values=['Revenue', 'Expenses'])
df_wide.columns = ['_'.join(map(str, col)) for col in df_wide.columns]
df_wide.reset_index(inplace=True)
print(df_wide)
```

## Pivoting Wider and Longer

- Fortunately, it's possible to _reshape_ `DataFrames` so as to switch between these formats at your convenience. 

- From Wide to Long:

:::{.panel-tabset .hey}

### R

```{r}
#| echo: true
#| code-fold: true
#| collapse: true
library(tidyverse, quietly=TRUE)
print(py$df_wide)
long_condensed <- py$df_wide |>
  pivot_longer(cols = contains(c('2023', '2024')), 
               names_to = 'variable',
               values_to = 'value')

print(long_condensed)
long_final <- long_condensed |>
  separate_wider_delim(variable, delim="_", names=c('variable', 'year'))

print(long_final)
```

### Python

```{python}
#| echo: true
#| code-fold: true
#| collapse: true
print(df_wide)
df_melted = pd.melt(df_wide, id_vars=['Company'], var_name='Metric_Year', value_name='Value')
print(df_melted)
# Split the 'Metric_Year' column into 'Metric' and 'Year'
df_melted[['Metric', 'Year']] = df_melted['Metric_Year'].str.split('_', expand=True)
df_melted.drop(columns='Metric_Year', inplace=True)
print(df_melted)
```

### Stata

:::

:::{style="font-size: 0.9em;"}
- From Long to Wide:
:::

:::{.panel-tabset .hey}

### R 

```{r}
#| echo: true
#| collapse: true
#| code-fold: true
print(py$df_long)
py$df_long |>
  pivot_wider(names_from="Year",
              values_from=c("Revenue", "Expenses"))

```

### Python  

```{python}
#| echo: true
#| code-fold: true
#| collapse: true
print(df_long)
df_wide = df_long.pivot(index='Company', columns='Year', values=['Revenue', 'Expenses'])
print(df_wide)
df_wide.columns = ['_'.join(map(str, col)) for col in df_wide.columns]
df_wide.reset_index(inplace=True)
print(df_wide)

```

### Stata


:::

## Merging Datasets

- Another common operation you might have to perform is _merging datasets_, also referred to as _joining datasets_.
- If you acquire your data from different sources, you have to combine them somehow.
- Merging datasets involves combining two (or more) datasets on the basis of common **keys** or **identifiers**. 
- For example, you might have these two datasets, out of which you want to create a dataset including `name`, `band` and `instrument` :^[Borrowed from [here](https://dplyr.tidyverse.org/reference/mutate-joins.html)]

::::{.columns}

:::{.column}
```{r}
#| echo: false
band_members
```
:::

:::{.column}

```{r}
#| echo: false
band_instruments
```

:::

::::

## Different Types of Joins

- There are different types of "joins" you can perform. 

- `join` functions usually take (at least) two arguments, and add columns from the `y` dataframe (the second argument) to the `x` dataframe (the first argument), matching observations based on the keys. 

- The logic of this operation is **exactly the same**, no matter you do it in R, Python or Stata.

:::{.panel-tabset}

### R

::: {.scroll-container style="overflow-y: scroll; height: 300px; font-size: 0.7em;"}

In R, you'd use the `left_join` function from the `dplyr` package to join to datasets. `left_join` takes two arguments, data frame `x` and data frame `y`, where `x` is referred to as the left data frame, and `y` as the right data frame. 

The function `left_join` keeps _all observations_ in `x`, irrespective of whether they have a match in `y`:

```{r}
#| echo: true
#| collapse: true
left_join(band_members, band_instruments)
```

Good practice is to also specify another argument, `by`, to explicitly instrument on the basis of what keys the join should be performed. This works in this way: `by=c('key_in_x_df' = 'key_in_y_df')`. If the keys have the same variable name in both data frames, you could use the special case `by='key_in_bothdfs'`. 

```{r}
#| echo: true
#| collapse: true
left_join(band_members, band_instruments, by = 'name')
#left_join(band_members, band_instruments, by=c('name'='name')) would also work
```

:::

### Python

::: {.scroll-container style="overflow-y: scroll; height: 300px; font-size: 0.7em;"}

In Python, you'd use the `merge` function from the `pandas` package. This function includes a whole lot of options. The function merge takes at least three arguments, the `left_df` and the `right_df`. 

If the key columns have different names, you also specify the `left_on` and `right_on` arguments, as exemplified below. 

If not, you could use the `on` argument and specify it like `on='common_column_name'`. 

Finally, you should specify the `how` argument. If you specify `how='left'`, all observations from the left data frame will be kept. If you specify `how='inner'`, it'll only keep observations from the left data.frame for which a match can be found. For other options, check out the [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html). 

```{python}
#| echo: true
#| collapse: true
import pandas as pd

band_members = r.band_members
band_instruments = r.band_instruments
merged_df = pd.merge(band_members, band_instruments, on='name', how='left')
print(merged_df)
```

:::

### Stata

::: {.scroll-container style="overflow-y: scroll; height: 300px; font-size: 0.7em;"}

In Stata, a join can be accomplished using the `merge` command. 

```{stata}
#| echo: true
#| eval: false
merge 1:1 name using band_instruments, keep(master match) nogenerate
```

- `merge 1:1 name` specifies a one-to-one merge on the variable 'name'

- `using band_instruments` specifies the dataset to merge with (like `band_instruments` in R)

- `keep(master match)` keeps only observations from the master dataset (left join) and matched observations

- `nogenerate` suppresses creation of the _merge variable that tracks merge results

- When the variable names of the variable holding the keys are different, you'd use this:

```{stata}
#| echo: true
#| eval: false
merge 1:1 artist_name=name using band_instruments, keep(master match) nogenerate
```

- where now, `artist_name` is the name in the master (the left) dataframe and `name` is the name of the key in the `using` dataset (i.e. `band_instruments`). 

:::

:::

## Merging with Fuzzy Keys

- Sometimes, datasets have keys that are not entirely identical. 
- E.g. "United States" in one dataset vs. "United States of America" in another. 
- Usually, the best strategy in this case is to look for existing identifiers. 
  - E.g. companies usually have ISIN-numbers that are available in multiple databases. Countries have ISO codes. 
- If that does not work, you could attempt _fuzzy matching_ or _fuzzy joining_. 
- A fuzzy join (or approximate join) is a technique used to merge datasets based on inexact matching, such as similar strings.
- Fuzzy joins handle inconsistencies like typos, different formats, or slight variations in data.

## Implementation

- This is how that works in different languages. 

:::{.panel-tabset}

### R

::: {.scroll-container style="overflow-y: scroll; height: 300px; font-size: 0.7em;"}

- In R, you'd use the `fuzzyjoin` library in the following way:

```{r}
#| echo: true
#| code-fold: true
#| collapse: true

library(fuzzyjoin)
library(dplyr)

# Sample data
df1 <- data.frame(company = c("Apple Inc.", "Microsoft", "Google LLC"))
df2 <- data.frame(name = c("Apple Incorporated", "Microsoft Corp", "Google Inc"))

# Perform fuzzy join using string similarity (Levenshtein distance by default)
result <- stringdist_join(
  df1, 
  df2, 
  by = c("company" = "name"),
  mode = "left",
  method = "jw",  # Jaro-Winkler (good for company names)
  max_dist = 0.25, # Adjust threshold (lower = stricter)
  ignore_case = TRUE
) %>%
  select(company, name)  # Keep only relevant columns

print(result)
```

:::

### Python

::: {.scroll-container style="overflow-y: scroll; height: 300px; font-size: 0.7em;"}

- In Python, you'd use the `rapidfuzz` library in the following way:

```{python}
#| echo: true
#| code-fold: true
#| collapse: true

import pandas as pd
from rapidfuzz import process, fuzz

def fuzzy_join(left_df, right_df, left_col, right_col, threshold=40):
    matches = []
    for _, left_row in left_df.iterrows():
        # Get best match (returns (match, score, index) or None)
        match = process.extractOne(
            left_row[left_col], 
            right_df[right_col], 
            scorer=fuzz.token_set_ratio,
            score_cutoff=threshold
        )
        if match:
            matches.append((left_row[left_col], match[0], match[1]))
        else:
            matches.append((left_row[left_col], None, None))
    
    # Create DataFrame with matches
    matched_df = pd.DataFrame(matches, columns=[left_col, right_col, "score"])
    
    # Merge results
    return pd.merge(
        left_df,
        matched_df[[left_col, right_col]],
        on=left_col,
        how="left"
    ).merge(
        right_df,
        left_on=right_col,
        right_on=right_col,
        how="left"
    )

# Example usage
df1 = pd.DataFrame({"company": ["Apple Inc.", "Microsoft", "Google LLC", "Netflix"]})
df2 = pd.DataFrame({"name": ["Apple Incorporated", "Microsoft Corp", "Google Inc."]})

result = fuzzy_join(df1, df2, "company", "name")
print(result)
```

:::

### Stata

::: {.scroll-container style="overflow-y: scroll; height: 300px; font-size: 0.7em;"}

- Stata has the `reclink` package (documentation [here](http://fmwww.bc.edu/repec/bocode/r/reclink.html)) that allows you to use fuzzy string matching to merge datasets:

```{stata}
#| echo: true
#| eval: false
#| code-fold: true

* Install the reclink command from ssc
ssc install reclink

// Example with two datasets
use dataset1, clear
save temp1, replace

use dataset2, clear
save temp2, replace

reclink keyvar using temp1, idmaster(id1) idusing(id2) gen(matchscore)
```

:::

:::

- An alternative you could exploit is use LLM's and provide them with the instruction to return a matched dataset.
- There also exists a Python library called `recordlinkage`, available [here](https://recordlinkage.readthedocs.io/en/latest/installation.html). 


# Regression Tables


## Regression Tables

- Regression tables will probably be some of the most important stuff in your thesis.

- They often form your central evidence, confirming or rejecting your hypothesis, answering your research question. 

- Anyone who reads your thesis farther than the introduction will probably look at them. 

- This is why **regression tables have to be formatted nicely**. 

- Fortunately, R/Python/Stata all have nice libraries allowing you to generate regression tables. 

- Hence, **you should never manually create a regression table, ever**. 

## Example Format

::::{.columns}

:::{.column width="50%"}
- Regression tables should always be formulated like this:

```{r}
#| echo: false
library(modelsummary); library(fixest); library(tinytable)
model <- feols(mpg ~ drat, data=mtcars)
model2 <- feols(mpg ~ drat + hp, data = mtcars)
model3 <- feols(mpg ~ drat + hp, vcov='hc1', data=mtcars)

modelsummary(list(model, model2, model3), gof_map=c("nobs", "r.squared"), output = 'tinytable') |>
  theme_tt("resize", width=1)
```

:::

:::{.column width="50%"}
- And they should never be raw output like this:

:::{style="font-size: 0.7em;"}
```{r}
#| echo: false
summary(model)
```
:::

:::

::::


## Regression Tables (Cont.)

- Regression tables should have an informative title. 
- Variable names should be properly and informatively formatted
  - In human-readable text, so "GDP per capita", not `gdp_per_capita`. 
- If you use fixed effects, these should be incorporated in the table in a specific way (see examples that follow). 
- As a rule, you should also report the number of observations and (some equivalent of an) R-squared. 
- Tables should have notes briefly describing what the regressions do. It should detail which method is used, how/which standard errors are computed and, traditionally, indicate statistical significance. 

## Regression Tables in R/Python/Stata

- Here are three full-fledged examples of nicely formatted regression tables. We'll go through all of the characteristics and their implementation.

:::{.panel-tabset}

### R

::: {.scroll-container style="overflow-y: scroll; height: 400px; font-size: 0.7em;"}

- In R, the best option is to use the `modelsummary` and `tinytable` libraries to create regression tables and descriptive statistics. The documentation for these libraries is available [here](https://modelsummary.com/) and [here](https://vincentarelbundock.github.io/tinytable/) respectively. These both also work very well with the `fixest` package. 

```{r}
#| echo: true
library(fixest)
library(tibble)
library(modelsummary)
library(tinytable)
# Estimate 6 models at once with "vs" fixed effects
models <- feols(c(carb, mpg) ~ csw(cyl, disp, cyl:disp) | vs, data=mtcars)
table1 <- modelsummary(models,
             stars=c("*"=0.1, "**"=0.05, "***"=0.01),
             gof_map = tibble(
                raw=c("nobs", "r2.within", "FE: vs"),
                clean=c("N", "Within R sq.", "VS FE"),
                fmt=c(0, 2, 0))
             ) |> 
  setNames(c("", rep("carb", 3), rep("mpg", 3)))

table1
```

- This table can be saved (exported to e.g. Word) by using the `save_tt()` function:

```{r}
#| echo: true
#| eval: false
save_tt(table1, "table1.docx")
```

:::

### Python

::: {.scroll-container style="overflow-y: scroll; height: 400px; font-size: 0.7em;"}

- In Python, the best option is to use the `pyfixest` package for regression tables and descriptive statistics. The documentation for regression tables is available [here](https://py-econometrics.github.io/pyfixest/table-layout.html). 

```{python}
#| echo: true
import pyfixest as pf
data = r.mtcars
models = pf.feols("carb + mpg ~ csw(cyl, disp, cyl:disp) | vs", data=data)
table = pf.etable(models,
  signif_code=[0.01, 0.05, 0.1],
  digits=3,
  show_se_type=False,
  labels={
    "carb": "Carb",
    "mpg": "Miles Per Gallon",
    "cyl": "Cyl",
    "disp": "Disp", 
    "vs": "VS FE"
}
  )

table
```

- This can also be exported in various ways, for example, to Word, using the `GT` library:

```{python}
#| echo: true
#| eval: false
from great_tables import GT
GT.write_raw_html(table, filename="table1.html")
```


:::


### Stata

::: {.scroll-container style="overflow-y: scroll; height: 400px; font-size: 0.7em;"}


- Stata uses the `estout` library (documentation [here](http://repec.org/bocode/e/estout/hlp_esttab.html)) to construct regression tables. 

```{stata}
#| echo: true
#| eval: false
#| code-fold: true
* Load and prepare data
sysuse auto, clear  // Stata doesn't have mtcars, so using auto dataset
encode rep78, gen(rep78_fe)  // Create fixed effect variable

* Run regressions
eststo clear
eststo: reg price mpg weight
eststo: reg price mpg weight i.rep78_fe  // With fixed effects
eststo: reg price mpg weight i.rep78_fe i.foreign  // Multiple FEs

* Create publication-ready table
esttab using "regression_table.rtf", ///
    replace label b(3) se(3) ///
    star(* 0.10 ** 0.05 *** 0.01) ///
    title("Automobile Price Regressions") ///
    addnotes("Standard errors in parentheses" "*** p<0.01, ** p<0.05, * p<0.10") ///
    indicate("Rep78 FE = *.rep78_fe*" "Foreign FE = *.foreign*")

```

:::

:::

  
# Visualization

## What should we not show?

- Don't show, or incorporate into your thesis, any raw plotted figure you used to inspect your data
- I.e. a scatterplot like this is a no-go: 

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
mtcars |>
  ggplot(aes(x = drat, y = mpg)) + geom_point()
```

## What should we show?

- Always add a **title** and **clear labels** to your plot.
  - Make sure they're large enough.
- If you're unsure, ask somebody else to look at your plot.
  - If they don't get it, you probably created a bad plot.
- Pay attention to ticks and intercepts.
  
```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8

mtcars |>
  ggplot(aes(x=drat, y=mpg)) + geom_point() + 
  xlab("Rear axle ratio (drat)") +
  ylab("Miles/(US) Gallon (mpg)") + 
  ggtitle("Relationship between Rear axle ratio and Miles per gallon") +
  theme_bw()
```

## Use colors strategically

Here is some code that helps you set-up some basic acceptable scatter plots. 


## In what format should you save your graphics?

- Vector graphics are composed of formulas or paths.
  - "Draw a straight line from (0, 0) to (13, 4)."
  - Infinitely zoom-able. Preserves all underlying information.
  - May be slow to load when complex.
  - Extensions .pdf or .svg.

- Raster graphics are composed of pixels (a grid of squares with color information).
  - Only an approximation to the underlying shapes or points.
  - Work better with Microsoft Office and HTML.
  - Usually best: .png. Also .jpeg, .gif.

# Project Organization

## General principes

- A Master's thesis is likely the largest, most complex independent project you've ever managed.

-  The difference between a smooth, relatively low-stress process and a chaotic, last-minute scramble often comes down to the organization you set up at the very beginning.

- Adhering to these principles will make your thesis (hopefully) much less painful

## File Organization

- Create ONE main folder for your entire thesis.
- Use a numbered prefix system to keep folders in a logical, non-alphabetical order.

```
\master_thesis\
      \1_code\
        \1_download_data.py
        \2_bring_different_sources.py
        \3_rename_and_reshape_data.py
      \2_data\
        \1_stockprice.csv
        \2_accounting_info.json
        \intermediate_data\
          \stockprices_winsorized.csv
      \3_figures\
        \event_study.pdf
        \scatterplot.pdf
      \4_tables\
        \descriptive_statistics.tex
        \ols_baseline.tex
        \ols_robust_se.tex
        \autoregressive_1.tex
      \5_document\
        \thesis.docx
```

## File Organization (Cont.)

- Why numbers? Because it forces the folders into an order that *you* decide is logical, not the alphabet. 'Admin' won't end up at the top, and 'Writing' won't be at the bottom. 

- Keep your raw data in a separate folder and never touch it—always work on a copy. Make a backup of this folder. Set up a cloud sync service today. 
  - You can use things like [Google Drive](http://drive.google.com), but also [DropBox](www.dropbox.com) and other commercial providers are available.
  
- Seperate files into inputs and outputs. 

- Don't use file names or directory names with spaces. 
  - Computer languages don't handle spaces well

## Organizing Literature

- Manual citation is a recipe for disaster. 
- Pick a tool **now** and use it for every paper you read.
  - Popular choices are Zotero, EndNote and Mendeley
  - You can also use [Overleaf](www.overleaf.com) if you're familiar with LaTeX

- Every time you download a paper, add it to your reference manager immediately. 

- When you're writing, you can insert citations directly from the manager.

## Code Organization

- Break programs into short scripts or functions, each of which conducts a single task.

- Make names distinctive and meaningful.

- Be consistent with code style and formatting.
  - Pay attention to things like file naming conventions and variable naming conventions

- Use relative filepaths ("../input/data.csv" instead of "C:/build/input/data.csv").
  - In VSCode, the working directory is always the **folder you have opened**. See [here](https://stackoverflow.com/questions/38623138/how-to-set-the-working-directory-for-debugging-a-python-program-in-vs-code) if you want to change that.
  - In RStudio, if you set up an [Rproj](https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects), your working directory will also be the directory in which the `.Rproj` file is situated. 
  - In Stata, you can use `cd` to change your working directory. 
  
- Make incremental changes in your code, and test them as you go.


## Code Organization (Cont.)


- Save new copies of your scripts:
  - Every time you make a substantial revision.
  - If you make any edits to a script you haven't touched in more than 1-2 weeks.

- Comment often to explain the purpose of your code
  - Use descriptive variable names, e.g. `income_percapita` rather than `income_pc` or `inc_pc`

- Don't use spaces when naming variables. 
  - R: The `janitor` package has a function called `clean_names()`, which automatically converts column names into something compatible with R

- In Python, you could use something like this:

:::{style="font-size: 0.7em;"}
```{python}
#| echo: true
#| code-fold: true
#| eval: false
import re

def clean_col_names(col_name):
    """A more robust function to clean column names."""
    # Convert to lowercase
    new_col = col_name.lower()
    # Replace any non-alphanumeric characters with an underscore
    new_col = re.sub(r'[^A-Za-z0-9]+', '_', new_col)
    # Remove leading/trailing underscores
    new_col = new_col.strip('_')
    return new_col

# Apply the function to all columns
df.columns = [clean_col_names(col) for col in df_messy_copy.columns]
```
:::


## Version Control

- We've all been guilty of the 'Final_Final' file name. 
  - This creates confusion and risk. 
  - A simple, robust naming convention using dates in the `YEAR-MONTH-DAY` format is a great start because it sorts chronologically.
  
- If you're working a lot with code or LaTeX, I strongly encourage you to look into Git.
  - It's the professional standard for a reason. 
  - For Word documents, leverage the version history that's built into tools like Google Docs or OneDrive.


# Summary

## What did we do?

## Other Resources

- [This](https://github.com/uo-ec607/lectures)
- [This](https://github.com/msu-econ-data-analytics/course-materials?tab=readme-ov-file)


