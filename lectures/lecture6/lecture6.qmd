---
title: "Empirical Economics"
subtitle: "Lecture 6: Binary Outcome Data"
format:
  revealjs: 
    theme: [default, theme.scss]
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 6 - Binary Outcome Data'
---


```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Outline

## Course Overview

- Linear Model I
- Linear Model II
- Time Series and Prediction
- Panel Data I
- Panel Data II
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Instrumental Variables

## What do we do today?

- Introduce binary outcome data and a straightforward way of analyzing it: the **linear probability model** (LPM).

- Talk about various shortcomings of the LPM, and introduces several alternatives, in particular, **logistic regression** and the **probit model**. 

- Introduce an alternative method to OLS, namely **maximum likelihood estimation** (MLE). 

- Discuss and contrast pitfalls of these models. 

- Talk about more general ways to deal with **truncated** and **selected data**, such as the Tobit model.

# Introduction

## Binary Outcomes

- Many interesting economic and social questions have a binary (0/1) outcome. We need special tools to model these.

:::{.callout-tip title="Examples: Binary Outcomes"}

*   **Labor Economics:** Does a person participate in the labor force? (Yes=1, No=0)
*   **Finance:** Does a company default on its loan? (Default=1, No Default=0)
*   **Marketing:** Does a consumer purchase a product after seeing an ad? (Purchase=1, No Purchase=0)
*   **Health Economics:** Does a patient's insurance status affect whether they receive a certain treatment? (Treatment=1, No Treatment=0)
*   **Political Science:** Does a person vote for a specific candidate? (Vote=1, Don't Vote=0)

:::

- Our dependent variable, $y$, can only take two values: 0 and 1.

## Linear Probability Model (LPM)

- What if we just use what we know? Ordinary Least Squares (OLS).

- When we apply OLS to a binary dependent variable, we call it the **Linear Probability Model (LPM)**.

:::{.callout-note title="Definition: Linear Probability Model"}

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \epsilon_i$$

Where $y_i$ is either 0 or 1.

:::

- **Key Insight:** The expected value of a binary variable is the probability that it equals 1.
$E[y_i | X_i] = 1 \cdot P(y_i=1 | X_i) + 0 \cdot P(y_i=0 | X_i) = P(y_i=1 | X_i)$
- This makes interpretation very appealing...

## LPM Interpretation

- Since $E[y_i | X_i] = P(y_i=1 | X_i)$, the LPM becomes:
$P(y_i=1 | X_i) = \beta_0 + \beta_1 x_{1i} + ...$

- **Interpretation of Coefficients:**
  - $\beta_k$ is the change in the *probability* that $y=1$ for a one-unit change in $x_k$, holding other factors constant.
  -  **Simplicity:** Easy to estimate (just OLS) and coefficients are incredibly easy to interpret as changes in probability.

:::{.callout-tip title="Example: Interpretation of LPM"}
Suppose we have a model like: `employed = 0.20 + 0.15 * education_years`

Each additional year of education is associated with a 0.15 (or 15 percentage point) increase in the probability of being employed.
:::

## Problems with LPM: Out-of-Bounds Predictions

- The model is linear, but probability is bounded by $[0, 1]$. The LPM doesn't know this.

  $$P(y=1|X) = \hat{\beta}_0 + \hat{\beta}_1 X$$

- Nothing in the OLS mechanics prevents the predicted value, $\hat{y}$, from being less than 0 or greater than 1 for certain values of X.
  - Interpretation of a predicted probability of 1.2 or -0.1 is nonsensical. 
  
## Illustration

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 7
#| # Step 1: Load necessary libraries
# install.packages(c("ggplot2", "dplyr")) # Run this if you don't have them
library(ggplot2)
library(dplyr)

# Step 2: Simulate data
# We'll create data where the probability of an event (y=1) is strongly
# related to a continuous variable (x). We'll make the range of x wide
# enough to guarantee the out-of-bound problem becomes visible.

set.seed(42) # for reproducibility

# Create a wide range for the independent variable 'x'
x <- seq(-15, 25, by = 0.5)

# Define the true probability using a logistic (S-shaped) function.
# This is a more realistic way to model probabilities.
# z = -2 + 0.3*x  (This is the log-odds)
true_prob <- 1 / (1 + exp(-(-2 + 0.3 * x)))

# Generate the binary outcome 'y' (0 or 1) based on this true probability
y <- rbinom(n = length(x), size = 1, prob = true_prob)

# Combine into a data frame
sim_data <- data.frame(x, y, true_prob)

# Step 3: Fit the models
# Model 1: The Linear Probability Model (LPM)
# This is just a standard linear regression (lm) on a binary outcome.
lpm_model <- lm(y ~ x, data = sim_data)


# Step 4: Generate predictions for visualization
# We create a grid of x-values to get smooth prediction lines.
prediction_grid <- data.frame(x = seq(min(sim_data$x), max(sim_data$x), length.out = 200))

# Get predictions from the LPM
prediction_grid$lpm_pred <- predict(lpm_model, newdata = prediction_grid)

# Get predictions from the Logit model.
# type = "response" gives us probabilities (P(Y=1))

# Step 5: Visualize the results with ggplot
ggplot(sim_data, aes(x = x, y = y)) +
  
  # Add horizontal lines at 0 and 1 to show the valid probability boundaries
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
  
  # Plot the raw data points (0s and 1s)
  # Jittering helps to see the density of points
  geom_point(position = position_jitter(width = 0, height = 0.03), alpha = 0.5) +
  
  # Plot the prediction line from the Linear Probability Model (LPM)
  geom_line(data = prediction_grid, aes(x = x, y = lpm_pred, color = "LPM"), size = 1.2) +
  
  # Improve the aesthetics
  scale_color_manual(name = "Model",
                     values = c("LPM" = "dodgerblue"),
                     labels = c("Linear Probability Model")) +
  labs(
    title = "Out-of-Bound Predictions in a Linear Probability Model",
    subtitle = "The LPM (blue) predicts probabilities < 0 and > 1",
    x = "Independent Variable (X)",
    y = "Predicted Probability / Actual Outcome (Y)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

##  Problems with LPM: The Error Term

- The assumptions of the Classical Linear Model are violated.
- Inherent Heteroskedasticity:
  - The variance of the error term depends on the values of the independent variables.
  - $Var(\epsilon_i | X_i) = p_i(1-p_i)$, where $p_i = P(y_i=1|X_i)$
  - Since $p_i$ depends on X, the variance is not constant. This is heteroskedasticity.
  - **Consequence:** OLS standard errors are biased. We could use robust standard errors to fix this problem. 
  
## LPM Summary

- **Pros:** Simple to interpret. It is also easy to incorporate fixed effects in an LPM.
- **Cons:** Nonsensical predictions, violates key OLS assumptions.

- We need a model that constrains the predicted probability to be between 0 and 1. We need a non-linear model.

- This requires a different way of thinking about the choice process.

# Probability Reminder

## PDF

- The **PDF** describes the *relative likelihood* that a continuous random variable $X$ takes on a given value.

- For a continuous variable, the probability of it being *exactly* one value is zero ($P(X=x) = 0$). Instead, the PDF gives us a function where the **area under the curve** over an interval corresponds to the probability of the variable falling within that interval.
  - A higher value of $f_X(x)$ means that values in that immediate vicinity are more likely to occur.

- **Formal Properties:**

  1.  $f_X(x) \ge 0$ for all $x$. The density is never negative.
  2.  $\int_{-\infty}^{\infty} f_X(x) dx = 1$. The total area under the curve is one.
  3.  $P(a \le X \le b) = \int_{a}^{b} f_X(x) dx$. The probability of $X$ being between $a$ and $b$ is the area under the PDF from $a$ to $b$.

## PDF Visualization 

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 7
# Create a sequence of x-values
x_vals <- seq(-4, 4, by = 0.01)

# Calculate the PDF values using dnorm() for the normal distribution
pdf_vals <- dnorm(x_vals, mean = 0, sd = 1)

# Plot the PDF
plot(x_vals, pdf_vals, type = "l", lwd = 2, col = "blue",
     main = "PDF of a Standard Normal Distribution N(0,1)",
     xlab = "x", ylab = "Density: f(x)")
abline(h = 0, col = "grey")
```


## CDF 

- The **CDF** $F(x)$ gives the probability that a random variable $X$ is *less than or equal to* a particular value $x$.

- Think of the CDF as a "running total" of the probability. As you move from left to right along the x-axis, it accumulates all the probability up to that point. This is why its value always goes from 0 to 1.

- **Formal Properties:**

  1.  $F_X(x) = P(X \le x)$. This is the definition.
  2.  The CDF is a non-decreasing function of $x$.
  3.  $\lim_{x \to -\infty} F_X(x) = 0$ and $\lim_{x \to \infty} F_X(x) = 1$.
  4.  For a continuous variable, $P(a < X \le b) = F_X(b) - F_X(a)$.

## CDF Visualization

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 7
# Use the same sequence of x-values
x_vals <- seq(-4, 4, by = 0.01)

# Calculate the CDF values using pnorm()
cdf_vals <- pnorm(x_vals, mean = 0, sd = 1)

# Plot the CDF
plot(x_vals, cdf_vals, type = "l", lwd = 2, col = "darkgreen",
     main = "CDF of a Standard Normal Distribution N(0,1)",
     xlab = "x", ylab = "Cumulative Probability: F(x)")
abline(h = c(0, 1), col = "grey", lty = 2) # Add lines at 0 and 1
```


## Relationship Between PDF and CDF

- The PDF and CDF are intrinsically linked. This relationship is foundational for many econometric concepts, including **Maximum Likelihood Estimation**.

- In particular, **the PDF is the derivative of the CDF.**
    $$ f_X(x) = \frac{dF_X(x)}{dx} $$
    - **Intuition:** The height of the PDF at a point $x$ represents the *slope* (or rate of change) of the CDF at that same point. Where the PDF is high (e.g., at the mean), the CDF is steepest, as probability is accumulating most rapidly.

- Less importantly here, **the CDF is the integral of the PDF.**

## Why This Matters in Econometrics

- The S-shape of the CDF is crucial for modeling probabilities in models like **Probit** and **Logit**. 

- The CDF naturally constrains the output to be between 0 and 1, which is exactly what a probability requires. 

- The choice of distribution (e.g., Normal for Probit, Logistic for Logit) determines the specific shape of this S-curve.

- A important property of both the Normal and Logistic CDF: $1 - F(-x) = F(x)$

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 6
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Choose a specific value for x for the illustration
x_val = 1.28

# --- Create the data for the bell curve ---
# Create a range of x-values for the plot
x_range = np.linspace(-4, 4, 1000)
# Calculate the y-values (PDF) for the standard normal distribution
y_pdf = norm.pdf(x_range, 0, 1)

# --- Create the two subplots ---
# fig is the whole figure, (ax1, ax2) are the two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
fig.suptitle('Why Φ(x) = 1 - Φ(-x)', fontsize=16)

# --- Plot 1: Illustrating Phi(x) ---
ax1.plot(x_range, y_pdf, 'black')
ax1.set_title(f'Area for Φ({x_val})', fontsize=14)
ax1.set_xlabel('x')
ax1.set_ylabel('Probability Density')

# Create the condition for shading: from the start of the range up to x_val
shade_condition_1 = x_range < x_val
# Shade the area under the curve for Phi(x)
ax1.fill_between(x_range[shade_condition_1], y_pdf[shade_condition_1], color='skyblue', alpha=0.7)
# Add a text annotation
ax1.text(x_val - 2.5, 0.1, f'Area = {norm.cdf(x_val):.4f}', fontsize=12)
ax1.grid(True, linestyle='--', alpha=0.6)


# --- Plot 2: Illustrating 1 - Phi(-x) ---
ax2.plot(x_range, y_pdf, 'black')
ax2.set_title(f'Area for 1 - Φ({-x_val})', fontsize=14)
ax2.set_xlabel('x')

# Create the condition for shading: from -x_val to the end of the range
shade_condition_2 = x_range > -x_val
# Shade the area under the curve for 1 - Phi(-x)
ax2.fill_between(x_range[shade_condition_2], y_pdf[shade_condition_2], color='skyblue', alpha=0.7)
# Add a text annotation showing the calculation
area_val = 1 - norm.cdf(-x_val)
ax2.text(-x_val + 0.5, 0.1, f'Area = {area_val:.4f}', fontsize=12)
ax2.grid(True, linestyle='--', alpha=0.6)

# --- Display the plot ---
plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for suptitle
plt.show()
```


# Logit and Probit Models

## The Latent Variable Framework

- Let's imagine the binary choice is driven by an unobserved, underlying continuous variable, $y^*$.

- $y^*$ can be thought of as the "net utility," "propensity," or "tendency" to choose 1.

:::{.callout-note title="Definition: Latent Variable Framework"}

$y_i^* = \beta_0 + \beta_1 x_i + \epsilon_i$

We don't observe $y^*$. We only observe the outcome, *y*, based on a threshold (usually normalized to 0):

- If $y_i^* > 0$, then we choose Yes ($y_i = 1$)
- If $y_i^* \le 0$, then we choose No ($y_i = 0$)

The probability that $y_i=1$ is the probability that $y_i^*$ is greater than 0.

  $$
    P(y_i=1) = P(y_i^* > 0) = P(\beta_0 + \beta_1 x_i + \epsilon_i > 0) = P(\epsilon_i > -(\beta_0 + \beta_1 x_i))
  $$

:::


## From Latent Variables to Probit & Logit

- This links the probability of the observed outcome to the distribution of the unobserved error term, $\epsilon_i$.

- The final step depends on what we assume about the distribution of $\epsilon_i$.

  $$
    P(y_i=1) = P(\epsilon_i > -(\beta_0 + \beta_1 x_i)) = 1 - F(-(\beta_0 + \beta_1 x_i))
  $$

- where *F* is the Cumulative Distribution Function (CDF) of $\epsilon_i$.

## Probit and Logit

- Two common choices for $F$ are the normal distribution (Probit model) and the logistic distribution (Logit model)

:::{.callout-note title="Definition: Probit and Logit Models"}

**Probit Model:** Assumes $\epsilon_i$ follows a **Standard Normal** distribution.

- $P(y_i=1 | X_i) = 1 - \Phi (-(\beta_0 + \beta_1 x_i)) =  \Phi(\beta_0 + \beta_1 x_i)$
- where $\Phi(\cdot)$ is the Standard Normal CDF.

**Logit Model:** Assumes $\epsilon_i$ follows a **Standard Logistic** distribution.

- $P(y_i=1 | X_i) = 1 - \Lambda(-(\beta_0 + \beta_1 x_i)) = \Lambda(\beta_0 + \beta_1 x_i) = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}$
- where $\Lambda(\cdot)$ is the Standard Logistic CDF.
  
In both cases, $1-F(-(\beta_0 + \beta_1 x_i)) = F(\beta_0 + \beta_1 x_i)$, since both distributions are symmetric.  Both CDFs produce the S-shaped curve we need. 

:::

## Visualization Logit and Probit Models

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 10

library(ggplot2)
library(dplyr)

# Set a seed for reproducibility
set.seed(42)

# Number of data points
n <- 200

# Create a predictor variable 'x'
x <- runif(n, min = -10, max = 10)

# Define a "true" underlying relationship (log-odds)
# log-odds = 0.5 (intercept) + 0.8 * x (slope)
true_log_odds <- 0.5 + 0.8 * x

# Convert log-odds to probabilities using the logistic function
true_prob <- 1 / (1 + exp(-true_log_odds))

# Generate the binary outcome 'y' based on these probabilities
y <- rbinom(n, size = 1, prob = true_prob)

# Create our final data frame
sim_data <- data.frame(x, y)

# Create a grid of x-values for plotting the smooth curves
x_grid <- seq(min(sim_data$x), max(sim_data$x), length.out = 200)

# --- Define Model A's coefficients and calculate probabilities ---
b0_A <- 0.5  # Intercept
b1_A <- 0.8  # Slope
prob_A <- 1 / (1 + exp(-(b0_A + b1_A * x_grid)))

# --- Define Model B's coefficients and calculate probabilities ---
b0_B <- 4.0  # Higher intercept (shifts left)
b1_B <- 2.5  # Steeper slope
prob_B <- 1 / (1 + exp(-(b0_B + b1_B * x_grid)))

# Combine the curves into a single data frame for plotting with ggplot
curves_df <- data.frame(
  x = x_grid,
  Probability_A = prob_A,
  Probability_B = prob_B
) %>%
  # Pivot to a long format, which is ideal for ggplot
  tidyr::pivot_longer(
    cols = c(Probability_A, Probability_B),
    names_to = "Model",
    values_to = "Probability"
  ) %>%
  # Make the model names prettier for the legend
  mutate(Model = recode(Model,
    "Probability_A" = "Model A: log-odds = 0.5 + 0.8x",
    "Probability_B" = "Model B: log-odds = 4.0 + 2.5x"
  ))

ggplot() +
  # 1. Plot the raw data points
  # We use jitter to prevent overplotting, especially at y=0 and y=1
  geom_point(data = sim_data, aes(x = x, y = y), alpha = 0.4,
             position = position_jitter(width = 0.1, height = 0.05)) +

  # 2. Plot the logistic curves from our two models
  geom_line(data = curves_df, aes(x = x, y = Probability, color = Model), size = 1.2) +

  # 3. Add labels and improve aesthetics
  labs(
    title = "Visualizing Logit Models with Different Coefficients",
    subtitle = "How the sigmoid curve moves based on the intercept and slope",
    x = "Predictor (x)",
    y = "Probability of Outcome (y=1)",
    color = "Model Specification" # Legend title
  ) +
  scale_y_continuous(breaks = seq(0, 1, 0.25), limits = c(-0.1, 1.1)) +
  scale_color_manual(values = c("steelblue", "darkred")) + # Custom colors for the lines
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")

```

# Estimation of Logit and Probit

## Estimation: Maximum Likelihood (MLE)

- Remember that in OLS, we take $\sum (y_i - \hat{y_i} (\beta, x_i))^2$ and set the derivative to zero to express the optimal $\beta$ coefficients. 
  - These conditions **may not** lead to a unique expression for the $\beta$-coefficients in Logit/Probit. 
  - Hence we can't use OLS. Instead, we use **Maximum Likelihood Estimation (MLE)**.

:::{.callout-note title="Definition: Maximum Likelihood Estimation"}

MLE finds the parameter values ($\beta$) that *maximize the probability of observing the actual data we collected*.
:::

- In other words: "Given our data, what are the most likely parameter values that could have generated it?"

## Simple MLE Example: A Biased Coin

:::{.callout-tip title="Example: A Biased Coin"}

Imagine you flip a coin 10 times and get 7 Heads (H) and 3 Tails (T).

- Data: {H, H, T, H, H, T, H, H, T, H}
- Question: What is your best guess for *p*, the probability of getting a Head?

**The Likelihood Function $L(p | \text{Data})$:** The probability of observing this specific sequence is: 
  $$L(p) = p \cdot p \cdot (1-p) \cdot p \cdot p \cdot (1-p) \cdot p \cdot p \cdot (1-p) \cdot p = p^7 (1-p)^3$$

The goal is to find the value of *p* that maximizes this function.

- **Intuition:** Your gut says p = 0.7.
- **Optimization:** We can take lots of values of $p$ and calculate the likelihood, and see which value of $p$ gives us the highest likelihood. 
- The result is indeed $\hat{p}_{MLE} = 0.7$.

This is the value of *p* that makes the data we saw "most likely."

:::

## MLE Visualization

:::{.callout-tip title="MLE Visualization: Biased Coin"}

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
#| fig-height: 4

library(ggplot2)
library(tidyr)
# 1. Define the parameters from the data
num_heads <- 7
num_flips <- 10
num_tails <- num_flips - num_heads

# 2. Create a fine grid of possible values for the parameter p
# We avoid exactly 0 and 1 to prevent log(0) = -Inf issues.
p_grid <- seq(from = 0.001, to = 0.999, length.out = 1000)

# 3. Calculate the log-likelihood for each p in the grid
log_likelihood <- num_heads * log(p_grid) + num_tails * log(1 - p_grid)

# 4. Create a data frame for plotting
plot_data <- data.frame(
  p = p_grid,
  log_likelihood = log_likelihood
)

# 5. Define the Maximum Likelihood Estimate (MLE) for annotation
p_mle <- num_heads / num_flips

# Find the maximum value of the log-likelihood to place our annotation point
max_log_likelihood <- max(plot_data$log_likelihood)


# 6. Create the plot
ggplot(plot_data, aes(x = p, y = log_likelihood)) +
  
  # Draw the log-likelihood curve
  geom_line(color = "dodgerblue", size = 1.2) +
  
  # Add a vertical dashed line marking the MLE
  geom_vline(xintercept = p_mle, linetype = "dashed", color = "red", size = 1) +
  
  # Add a point at the maximum of the curve
  geom_point(aes(x = p_mle, y = max_log_likelihood), color = "red", size = 4) +
  
  # Add a text label for the MLE. We position it slightly below the peak.
  annotate(
    geom = "text",
    x = p_mle,
    y = max_log_likelihood - 0.4, # Adjust vertical position
    label = paste("MLE: p =", p_mle),
    color = "red",
    size = 5,
    hjust = 0.4  # Adjust horizontal position relative to the line
  ) +
  
  # Add informative labels and a title
  labs(
    title = "Log-Likelihood for Coin Flip Data (7 Heads, 3 Tails)",
    subtitle = "The peak of the curve identifies the most likely value for p.",
    x = "Parameter p (Probability of a Head)",
    y = "Log-Likelihood: 7*log(p) + 3*log(1-p)"
  ) +
  
  # Use a clean theme for a professional look
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "gray30")
  )
```

:::

## MLE for Probit/Logit Models

- For our regression models, the principle is the same, the calculations are just more complex.

- The **Likelihood Function** is the product of the probabilities of each individual observation:
$L(\beta) = \prod_{i=1}^{N} [P(y_i=1|X_i)]^{y_i} \cdot [1 - P(y_i=1|X_i)]^{1-y_i}$
  - If $y_i=1$, we use $P(y_i=1|X_i)$
  - If $y_i=0$, we use $1-P(y_i=1|X_i) = P(y_i=0|X_i)$

- We plug in the Probit or Logit formula for $P(y_i=1|X_i)$ and use a computer to find the $\beta$ vector that maximizes this function (or more commonly, the log of this function, the **Log-Likelihood**).

## MLE: Probit Example

:::{.callout-tip title="Example: MLE for Probit"}

- **Likelihood Function:**
  $$
    \mathcal{L}(\beta) = \prod_{i=1}^n \Phi(\beta_0 + \beta_1 x_i)^{Y_i} [1 - \Phi(\beta_0 + \beta_1 x_i)]^{1-Y_i}
  $$

- **Log-Likelihood Function:**
  $$
    \ln\mathcal{L}(\beta) = \sum_{i=1}^n \left\{ Y_i \ln\Phi(\beta_0 + \beta_1 x_i) + (1-Y_i)\ln[1-\Phi(\beta_0 + \beta_1 x_i)] \right\}
  $$

- **First-Order Conditions (FOC):**
  $$
    \frac{\partial \ln\mathcal{L}(\beta_0, \dots, \beta_k)}{\partial \beta_j} = \sum_{i=1}^n \left[ \frac{Y_i \phi(\beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik})}{\Phi(\beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik})} - \frac{(1-Y_i)\phi(\beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik})}{1-\Phi(\beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik})} \right] x_{ij} = 0
  $$

:::

## Interpreting Coefficients

- In Probit and Logit models, the estimated coefficients ($\hat{\beta}$) are **NOT** marginal effects.

  $$
    P(y=1|X) = F(\beta x_i )
  $$

- A one-unit change in $x_k$ changes the *argument* $\beta x_i$ by $\beta_k$.
- But the change in the *probability* depends on the slope of the S-curve, which depends on the values of *all* X variables:

  $$
    \frac{\partial P(y=1 | x_i)}{\partial x_i} = F'(\beta x_i) \cdot \beta = f(\beta x_i) \cdot \beta
  $$

- This means that the change in the probability does not only depend on $\beta$, but also on the values of the independent variables $x_i$ and the function $F'$. 
- So, how do we get meaningful interpretations?

## Interpretation Method 1: Marginal Effect at the Mean

- We simply calculate the change in predicted probability for a change in an x-variable at a particular value.

:::{.callout-note title="Marginal Effect at the Mean"}

The marginal effect _at the mean_ equals: 

  $$
    \frac{\partial P(y=1|X)}{\partial x_k} = f(\beta \bar{x}) \cdot \beta_k
  $$
  
where $f(\cdot)$ is the PDF, the derivative of the CDF $F$.

In other words, we simply calculate the MEs with all X variables set to their sample means.

*Problem:* No single observation in the data might actually have all mean values. "The average person" doesn't exist.
    
:::

## Interpretation Method 2: Average Marginal Effect

- **This is the modern, preferred standard.** It gives the best summary of the effect for the population in the sample.

:::{.callout-note title="Average Marginal Effect"}

The Average Marginal Effect (AME) equals:

  $$
    \frac{\partial P(y=1|X)}{\partial x_k} = \frac{1}{N} \sum_{i=1}^N f(\beta x_i) \cdot \beta_k
  $$

In other words, we compute the marginal effect using the values of each observations, and then take the average. 

:::


## Logit and Probit in Software

:::{.callout-tip title="Example: Bertrand and Mullainathan (2004)"}
The paper "Are Emily and Greg More Employable Than Lakisha and Jamal?" is a Field Experiment on Labor Market Discrimination".

They sent thousands of fictitious resumes to real job openings in Boston and Chicago, randomly assigning either a "White-sounding" name or an "African American-sounding" name to otherwise identical resumes.

The study found that resumes with "White-sounding" names received 50% more callbacks for interviews than those with "African American-sounding" names. This racial gap in callbacks was consistent across various occupations, industries, and employer sizes. 

The paper provides strong evidence of persistent racial discrimination in the hiring process.
:::

## Logit and Probit in Software - Example

- R/Python/Stata have excellent options to run both logistic regression and probit models. Below are some examples on the basis of the Bertrand and Mullainathan (2004) dataset. 

:::{.panel-tabset .hey}

### R

```{r}
#| echo: true
#| code-fold: true
#| collapse: true

df <- haven::read_dta('../../tutorials/datafiles/lakisha_aer.dta')
# LPM
model <- lm(call~ race + sex + C(occupbroad), data=df)
summary(model)
# Logit
model <- glm(call ~ race + sex + C(occupbroad), data=df, family=binomial(link="logit"))
summary(model)
# Probit
model <- glm(call ~ race + sex + C(occupbroad), data=df, family=binomial(link="probit"))
summary(model)
```

### Python

```{python}
#| echo: true
#| code-fold: true
#| collapse: true

import pandas as pd
import statsmodels.formula.api as smf

df = pd.read_stata('../../tutorials/datafiles/lakisha_aer.dta')
# LPM
model = smf.ols(formula="call ~ race + sex + C(occupbroad)", data=df)
results = model.fit()
print(results.summary())

# Logit
model = smf.logit(formula="call ~ race + sex + C(occupbroad)", data=df)
results = model.fit()
print(results.summary())

# Probit
model = smf.probit(formula="call ~ race + sex + C(occupbroad)", data=df)
results = model.fit()
print(results.summary())
```

### Stata

```{stata}
#| echo: true
#| eval: false
#| code-fold: true
#| collapse: true
* Load the dataset
use "../../tutorials/datafiles/lakisha_aer.dta", clear

* LPM (Linear Probability Model)
regress call race sex i.occupbroad

* Logit
logit call race sex i.occupbroad

* Probit
probit call race sex i.occupbroad
```

:::

## Marginal Effects in Software

- R/Python/Stata have libraries that allow you to calculate the marginal effects in these two ways easily.

:::{.panel-tabset}

### R

```{r}
#| echo: true
#| collapse: true
#| code-fold: true
library(marginaleffects)
# Model as before
# Logit
model <- glm(call ~ race + sex + C(occupbroad), data=df, family=binomial(link="logit"))

# Using the marginaleffects package
avg_slopes(model, variables = c('race', 'sex'))
```

### Python

```{python}
#| echo: true
#| collapse: true
#| code-fold: true
import statsmodels.formula.api as smf
# Logit
model = smf.logit(formula="call ~ race + sex + C(occupbroad)", data=df)
results = model.fit()

# Using the `get.margeff` method:
results.get_margeff(at='overall').summary_frame()
results.get_margeff(at='mean').summary_frame()
```

### Stata

```{stata}
#| echo: true
#| eval: false
#| code-fold: true

* Logit
logit call race sex i.occupbroad

* Calculate the Average Marginal Effect for all variables
margins, dydx(*)

* Calculate the Marginal Effect at the Mean for all variables
margins, dydx(*) atmeans
* The interpretation is similar to the AME, but it applies specifically to an observation with average characteristics rather than being an average of individual effects.
```

:::


## Goodness-of-Fit and Testing

- How well does our model fit the data?

- **Percent Correctly Predicted:**
  - If $\hat{p}_i > 0.5$, predict $y=1$. If $\hat{p}_i \le 0.5$, predict $y=0$.
  - Compare predictions to actual outcomes. What percentage did we get right?
  - Intuitive, but sensitive to the 0.5 cutoff.

- **Pseudo R-Squared:**
  - Several versions exist, like **McFadden's R-squared**.
  - $R^2_{McF} = 1 - \frac{\ln L_{full}}{\ln L_{null}}$ (where $L_{null}$ is from a model with only an intercept).
  - Ranges from 0 to 1, but values are much lower than OLS R-squared. A value of 0.2-0.4 can indicate a very good fit. **Do not compare to OLS R-squared!**

- **Likelihood Ratio (LR) Test:**
  - Tests the joint significance of a set of variables (like the F-test in OLS).
  - Compares the log-likelihood of the restricted model to the unrestricted (full) model.
  - $LR = 2(\ln L_{full} - \ln L_{restricted})$ which follows a $\chi^2$ distribution.
  
## Predictive Accuracy: The Confusion Matrix 

- This is the most intuitive method for assessing a model's performance. It's based on how well the model classifies individual cases.
  1.  The model calculates a predicted probability, $\hat{p}_i$, for each observation.
  2.  We choose a classification threshold (commonly 0.5).
  3.  We classify the prediction: if $\hat{p}_i > 0.5$, predict 1 (e.g., "Yes"); if $\hat{p}_i \le 0.5$, predict 0 (e.g., "No").
  4.  We compare these predictions to the actual outcomes in a "confusion matrix."

:::{style="font-size: 1.2em;"}

|                 | **Predicted: No (0)** | **Predicted: Yes (1)** |
| :-------------- | :-------------------- | :--------------------- |
| **Actual: No (0)** | True Negatives (TN)   | False Positives (FP)   |
| **Actual: Yes (1)**| False Negatives (FN)  | True Positives (TP)    |
:::

- Percent Correctly Predicted (Accuracy) = $\tfrac{TN + TP}{\text{Total Observations}}$
  *   **Pro:** Very easy to understand and communicate.
  *   **Con:** It is highly sensitive to the 0.5 cutoff. If the model is for a rare event (e.g., disease diagnosis), a different threshold might be more appropriate. It also treats all misclassifications as equal.


## Calculating Percent Correctly Predicted

- Let's see how to implement this in code. We will fit a model and then evaluate its accuracy using a confusion matrix.

:::{.panel-tabset}

### R

```{r cm}
#| echo: true
#| collapse: true
#| code-fold: true
#| 
# Load necessary library for more detailed metrics
# install.packages("caret")
library(caret)

# Create some sample data
set.seed(123) # for reproducibility
sample_data <- data.frame(
  predictor = rnorm(100),
  outcome = rbinom(100, 1, 0.5)
)

# Fit the logistic regression model
logit_model <- glm(outcome ~ predictor, data = sample_data, family = "binomial")

# Get predicted probabilities
predicted_probs <- predict(logit_model, type = "response")

# Convert probabilities to classes (0 or 1) using a 0.5 threshold
predicted_class <- factor(ifelse(predicted_probs > 0.5, 1, 0))
actual_class <- factor(sample_data$outcome)

# Generate the confusion matrix
# 'positive="1"' tells the function which outcome is the "success" class
conf_matrix <- confusionMatrix(data = predicted_class, reference = actual_class, positive="1")

print(conf_matrix)
```

### Python

```{python cm_p}
#| echo: true
#| collapse: true
#| code-fold: true

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report

# 1. Create some sample data
np.random.seed(123) # for reproducibility
sample_data = pd.DataFrame({
    'predictor': np.random.randn(100),
    'outcome': np.random.binomial(1, 0.5, 100)
})

# Define predictor (X) and outcome (y) variables
# We need to reshape the predictor to be a 2D array for scikit-learn
X = sample_data[['predictor']]
y = sample_data['outcome']

# 2. Fit the logistic regression model
logit_model = LogisticRegression()
model_fit = logit_model.fit(X, y)

# 3. Get predicted classes
# .predict() automatically uses a 0.5 probability threshold
predicted_class = model_fit.predict(X)

# 4. Generate the confusion matrix and classification report
# The labels parameter ensures the matrix is in the order [0, 1]
# for TN, FP, FN, TP
conf_matrix = confusion_matrix(y_true=y, y_pred=predicted_class)
print(conf_matrix)
# The classification_report provides precision, recall, f1-score, etc.
class_report = classification_report(y_true=y, y_pred=predicted_class)
print(class_report)
```

### Stata

```{stata}
#| echo: true
#| eval: false
#| code-fold: true

* Clear any existing data and set seed for reproducibility
clear all
set seed 123

* 1. Create some sample data
* Set the number of observations to 100
set obs 100

* Generate the predictor variable from a standard normal distribution
gen predictor = rnormal()

* Generate the binary outcome variable from a binomial distribution
* This creates a variable that is 0 or 1, with a 60% probability of being 1
gen outcome = rbinomial(1, 0.6)


* 2. Fit the logistic regression model
* The command 'logit' runs the model with 'outcome' as the dependent variable
logit outcome predictor


* 3. Generate the confusion matrix and classification report
* The 'estat classification' command is run immediately after the regression.
* It automatically uses a 0.5 probability threshold to classify outcomes.
estat classification
```

:::

- The output from the confusion matrix gives a detailed breakdown, including the confusion matrix itself, and key metrics like:

  *   **Accuracy:** The overall "Percent Correctly Predicted."
  *   **Sensitivity:** The proportion of actual positives that were correctly identified.
  *   **Specificity:** The proportion of actual negatives that were correctly identified.

## Explanatory Power: Pseudo R-Squared 

- In linear regression, $R^2$ tells us the proportion of variance explained by the model. We can't use the same metric for logistic regression, but we have alternatives called **Pseudo R-Squareds**.

- **McFadden's $R^2$** is a common choice. It is based on the log-likelihood of two models:

  1.  **The Full Model ($L_{full}$):** The model with all of our chosen predictors.
  2.  **The Null Model ($L_{null}$):** A basic model with only an intercept. This model essentially predicts the same probability (the overall sample proportion) for every observation.

  $$
    R^2_{McF} = 1 - \frac{\ln L_{full}}{\ln L_{null}}
  $$

- **Interpretation:**

  *   **Ranges from 0 to 1:** A value of 0 means your predictors are useless. A value of 1 would mean a perfect fit.
  *   **Values are much lower than OLS R-squared:** Do not make a direct comparison! A McFadden's $R^2$ between 0.2 and 0.4 can indicate a very good model fit.
  *   It represents the improvement in the log-likelihood of the full model compared to the null model.

## The Likelihood Ratio (LR) Test

- The LR test is the equivalent of the F-test in OLS. 
  - It tests the **joint significance** of a set of variables by comparing the goodness-of-fit of two **nested** models.
  - One model (the restricted model) is a special case of the other (the full model). For example, a model with predictors $X_1$ is nested within a model with predictors $(X_1, X_2)$.
  - The test determines if the additional variables in the full model provide a statistically significant improvement in fit compared to the simpler, restricted model.
  $LR = 2(\ln L_{full} - \ln L_{restricted})$

- This statistic follows a **Chi-Squared ($\chi^2$) distribution**, with degrees of freedom equal to the number of variables excluded from the restricted model.

   - **$H_0$ (Null Hypothesis):** The restricted model is the true model. The coefficients of the extra variables in the full model are all equal to zero.
   - **$H_A$ (Alternative Hypothesis):** The full model is the true model. At least one of the extra variables has a non-zero coefficient.
  - A small p-value (< 0.05) suggests that the full model is a significant improvement, and the additional variables are jointly significant.

## Performing a Likelihood Ratio Test

- A common use of the LR test is to check if the full model is a significant improvement over the null (intercept-only) model.

:::{.panel-tabset}

### R

```{r}
#| echo: true
#| code-fold: true
#| collapse: true
# Load the library for the LR test
# install.packages("lmtest")
library(lmtest)

# Create the restricted (null) model
null_model <- glm(outcome ~ 1, data = sample_data, family = "binomial")

# Compare the two models using the Likelihood Ratio Test
lr_test_result <- lrtest(null_model, logit_model)

print(lr_test_result)
```

### Python

```{python}
#| echo: true
#| code-fold: true
#| collapse: true
import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy.stats import chi2

# --- Setup: Re-creating the data from the previous step ---
# This ensures the script is self-contained.
np.random.seed(123) # for reproducibility
sample_data = pd.DataFrame({
    'predictor': np.random.randn(100),
    'outcome': np.random.binomial(1, 0.5, 100)
})

# Define predictor (X) and outcome (y) variables
y = sample_data['outcome']
X = sample_data[['predictor']]

# In statsmodels, you need to manually add a constant (intercept) to the model
X_with_const = sm.add_constant(X)

# --- Step 1: Fit the full (unrestricted) logistic regression model ---
# This is the same as 'logit_model' from the R code
full_model = sm.Logit(y, X_with_const).fit(disp=0) # disp=0 suppresses convergence messages

# --- Step 2: Fit the restricted (null) model ---
# This model only includes an intercept (outcome ~ 1)
# The predictor is just a column of ones (the constant)
X_null = sm.add_constant(pd.DataFrame({'dummy': np.ones(len(y))})) # Creates just an intercept
null_model = sm.Logit(y, X_null).fit(disp=0)

# --- Step 3: Compare the two models using the Likelihood Ratio Test ---
from scipy.stats.distributions import chi2
def likelihood_ratio(llmin, llmax):
    return(2*(llmax-llmin))

LR = likelihood_ratio(null_model.llf, full_model.llf)
p = chi2.sf(LR, 1) # L2 has 1 DoF more than L1

print('p: %.30f' % p)
```

### Stata

```{stata}
#| eval: false
#| echo: true
#| code-fold: true

* Clear any existing data and set seed for reproducibility
clear all
set seed 123

* --- Setup: Re-creating the data from the previous step ---
* This ensures the script is self-contained.
set obs 100
gen predictor = rnormal()
gen outcome = rbinomial(1, 0.6)


* --- Step 1: Fit the full (unrestricted) logistic regression model ---
* This is the model including our variable of interest.
logit outcome predictor

* --- Step 2: Store the estimation results of the full model ---
* The 'estimates store' command saves all the results (like the log-likelihood)
* of the most recent model under a name you choose.
estimates store full_model


* --- Step 3: Fit the restricted (null) model ---
* To fit an intercept-only model, you just specify the dependent variable.
logit outcome


* --- Step 4: Compare the two models using the Likelihood Ratio Test ---
* The 'lrtest' command compares the currently active model (the null model)
* with a previously stored model (full_model).
lrtest full_model
```

:::

  - **LogLik:** The log-likelihood values for the null (Model 1) and full (Model 2) models.
  - **Chisq:** This is the LR test statistic (0.5065).
  - **Pr(>Chisq):** This is the p-value (0.4766).

- Since the p-value is large, we do not reject the null hypothesis. This indicates that our predictor does not provide a statistically significant improvement over a model that simply predicts the mean.

## Choosing Between Probit and Logit

- **In practice, the choice rarely matters much.**
  - The Normal and Logistic distributions are very similar, except the Logistic has slightly "fatter tails" (it's less sensitive to outliers).
  - Predicted probabilities from both models are usually very close.

- **Rule of Thumb:**
  - Logit coefficients are larger than Probit coefficients by a factor of ~1.6 - 1.8.
  - Logit Marginal Effects $\approx$ Probit Marginal Effects.

  
# Prediction

## Prediction on Unseen Data 

- Once a model is estimated, its primary purpose is often to make predictions on new data where the outcome is unknown.

- The LPM is simply an OLS linear regression model where the dependent variable (Y) is binary (0 or 1).

  $$
    Y = \beta_0 + \beta_1X_1 + \dots + \beta_kX_k + \epsilon
  $$

- The predicted value, $\hat{Y}$, is interpreted as the predicted probability of the event occurring ($P(Y=1)$).
  - To predict on new data, you simply plug the new values of the predictor variables ($X_i$) into the estimated regression equation.
  - Because the LPM is a simple linear model, its predictions are not bounded between 0 and 1. It is possible—and common—to get predicted "probabilities" that are negative or greater than 1, which are nonsensical.

## Predicting in Software

:::{.panel-tabset}

### R

::: {.scroll-container style="overflow-y: scroll; height: 200px; font-size: 0.7em;"}

```{r}
#| echo: true
#| collapse: true
#| code-fold: true
# Create some sample data for training
set.seed(42)
age = sample(20:70, 100, replace = TRUE)

train_data <- data.frame(
  age = age,
  purchased = rbinom(100, 1, prob = age/70)
)

# Fit the Linear Probability Model on the training data
lpm_model <- lm(purchased ~ age, data = train_data)
summary(lpm_model)

# Create new, "unseen" data for which we want to make predictions
unseen_data <- data.frame(
  age = c(18, 25, 45, 68, 85) # Note the ages outside the original range
)

# Use the predict() function to get predicted probabilities
unseen_data$predicted_prob_lpm <- predict(lpm_model, newdata = unseen_data)

print(unseen_data)
```

:::

### Python

::: {.scroll-container style="overflow-y: scroll; height: 200px; font-size: 0.7em;"}

```{python}
#| echo: true
#| collapse: true
#| code-fold: true

import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf

# 1. Create some sample data for training
np.random.seed(42) # for reproducibility

# R's rbinom recycles the probability vector. We replicate that behavior here.
age = np.random.randint(20, 71, 100) # np.randint is exclusive of the high end
prob_vector = [np.random.binomial(1, i/70, 1)[0] for i in age]# creates a vector of 50 probabilities

train_data = pd.DataFrame({
  'age': age,
  'purchased': prob_vector
})

# 2. Fit the Linear Probability Model (LPM) on the training data
# The formula syntax 'purchased ~ age' is identical to R's
lpm_model = smf.ols('purchased ~ age', data=train_data).fit()

# Print a summary of the model, similar to R's summary()
print("--- Linear Probability Model Summary ---")
print(lpm_model.summary())
print("\n" + "="*80 + "\n") # Separator for clarity

# 3. Create new, "unseen" data for which we want to make predictions
unseen_data = pd.DataFrame({
  'age': [18, 25, 45, 68, 85]
})

# 4. Use the .predict() method to get predicted probabilities
# This is the direct equivalent of predict(model, newdata=...)
unseen_data['predicted_prob_lpm'] = lpm_model.predict(unseen_data)

# 5. Print the final predictions
print("--- Predictions on Unseen Data ---")
print(unseen_data)
```

:::

### Stata

::: {.scroll-container style="overflow-y: scroll; height: 200px; font-size: 0.7em;"}

```{stata}
#| eval: false
#| echo: true
#| code-fold: true
// 1. Create some sample data for training
clear // Clears any existing data from memory
set obs 100 // Sets the number of observations to 100

// Set the seed for reproducibility, equivalent to np.random.seed(42)
set seed 42

// Generate a variable 'age' with random integers between 20 and 70.
generate age = runiformint(20, 70)

// Generate the 'purchased' variable based on a binomial distribution.

generate purchased = rbinomial(1, age/70)


// 2. Fit the Linear Probability Model (LPM) on the training data
// The 'regress' command fits an OLS model. The dependent variable
// comes first, followed by the independent variable(s).
regress purchased age

// We can store the estimation results for later use if needed.
estimates store lpm_model


// 3. Create new, "unseen" data for which we want to make predictions
clear // Clear the training data to create the new dataset

// Input the new "unseen" data for the 'age' variable.
input age
    18
    25
    45
    68
    85
end // Finalizes the data input


// 4. Use the .predict() command to get predicted probabilities
predict predicted_prob_lpm

// 5. Print the final predictions
list age predicted_prob_lpm
```


:::

:::

- This output clearly illustrates the main weakness of the LPM: for plausible but extreme values of the predictor variables (like an age of 85), the model can predict probabilities outside of the logical range.


## Prediction with Logit

- The Logit model solves the prediction problem of the LPM by using a logistic function to ensure that predicted probabilities are always between 0 and 1.

- The model predicts the natural logarithm of the odds of the event occurring:

  $$
    \ln\left(\frac{P(Y=1)}{1-P(Y=1)}\right) = \beta_0 + \beta_1X_1 + \dots + \beta_kX_k
  $$

- To get the predicted probability, we solve for $P(Y=1)$.

  1.  **Predict the Log-Odds:** The model first calculates the predicted log-odds (the "link") for the new data.
  2.  **Convert to Probability:** The log-odds are then transformed into a probability using the logistic function:
      $P(Y=1) = \frac{e^{\text{log-odds}}}{1 + e^{\text{log-odds}}}$

- In software, this happens automatically.

## Logit Prediction in Software

:::{.panel-tabset}

### R

::: {.scroll-container style="overflow-y: scroll; height: 200px; font-size: 0.7em;"}

```{r}
#| echo: true
#| code-fold: true
#| collapse: true
# Use the same training data as before
# train_data <- data.frame(...)

# Fit the logistic regression model on the training data
logit_model <- glm(purchased ~ age, data = train_data, family = "binomial")

# Use the same "unseen" data
# unseen_data <- data.frame(age = c(18, 25, 45, 68, 85))

# Predict the log-odds (the default)
unseen_data$predicted_log_odds <- predict(logit_model, newdata = unseen_data, type = "link")

# Predict the probability directly
# The 'type = "response"' argument handles the conversion for us
unseen_data$predicted_prob_logit <- predict(logit_model, newdata = unseen_data, type = "response")

# We can also generate a class prediction using a 0.5 threshold
unseen_data$predicted_class <- ifelse(unseen_data$predicted_prob_logit > 0.5, "Purchase", "No Purchase")

print(unseen_data)
```

:::

### Python

::: {.scroll-container style="overflow-y: scroll; height: 200px; font-size: 0.7em;"}

```{python}
#| echo: true
#| code-fold: true
#| collapse: true
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf

# 1. Estimate logit model
model = smf.logit("purchased ~ age", data=train_data).fit()

# 2. Create the unseen data
# --- 2. Use the same "unseen" data ---
unseen_data = pd.DataFrame({
  'age': [18, 25, 45, 68, 85]
})

# 3. Put the data inside an Array for Prediction
X_unseen = unseen_data[['age']]

# 4. Use the predict method to find the predicted probabilities
predicted_probabilities = model.predict(X_unseen)
unseen_data['predicted_prob_logit'] = predicted_probabilities

# 5. Make the prediction on the basis of the 0.5 cutoff
unseen_data['predicted_class'] = np.where(predicted_probabilities > 0.5, "Purchase", "No Purchase")
print(unseen_data)
```


:::

### Stata

::: {.scroll-container style="overflow-y: scroll; height: 200px; font-size: 0.7em;"}

```{stata}
#| code-fold: true
#| eval: false
#| echo: true

* --- 1. Fit the logistic regression model on the training data ---
logit purchased age

* --- 2. Create new, "unseen" data for which we want to make predictions ---

* Clear the training data (it's safe because of the 'preserve' command)
clear

* Input the new ages for the unseen data directly
input age
18
25
45
68
85
end

* --- 3. Use the 'predict' command to find the predicted probabilities ---

* 'predict' uses the coefficients from the last model that was fit (the logit model).
* The 'pr' option specifies that we want the probability of a positive outcome.
predict predicted_prob_logit, pr

* --- 4. Make the prediction on the basis of the 0.5 cutoff ---

* The cond() function is a direct equivalent of Python's np.where() or R's ifelse().
* It creates a new string variable based on the condition.
gen predicted_class = cond(predicted_prob_logit > 0.5, "Purchase", "No Purchase")

* --- 5. Print the final results ---
* 'list' displays the data currently in memory, now containing the predictions.
* We drop the probability column to exactly match the final Python output.
drop predicted_prob_logit
display "" // Add a blank line for readability
display "--- Logistic Regression Predictions on Unseen Data ---"
list, noobs // The 'noobs' option hides observation numbers for a cleaner look

* --- (Optional but good practice) Restore the original training data ---
restore

```

:::

:::

- Notice how even for the extreme age of 85, the predicted probability from the logit model is 0.99—very high, but logically bounded within the range. 
  - This makes the logit model a much more robust choice for prediction in binary outcome scenarios.


# Limited Dependent Variable Models

## Dealing with Truncated and Selected Data

- In many situations, we might have a **biased sample**. 
  - Standard Ordinary Least Squares (OLS) relies on the crucial assumption that the expected value of the error term, conditional on the explanatory variables, is zero: $E[u | X] = 0$. 
  - This assumption is violated when our sample is not randomly drawn from the population of interest. 
- This can happen in two common ways:
  1.  **Censoring:** 
    - The dependent variable has a "floor" or "ceiling." We observe the explanatory variables for everyone, but the dependent variable is clustered at a limit value (e.g., zero) for a part of the sample.
  2.  **Sample Selection (or Incidental Truncation):** 
    - We are missing data on the dependent variable for a non-random subset of our observations. We don't even see the "zero"—the data is simply not there.

- In both cases, running OLS on the observed data leads to biased and inconsistent parameter estimates. 
  - We need models that explicitly account for the non-random nature of the sample.

## The Tobit Model (Censored Dependent Variable)

- The Tobit model is used when the dependent variable is continuous over a certain range but has a significant number of observations "piled up" at a lower (or upper) limit.

:::{.callout-tip title="Example: Censored Data"}
Annual hours worked. Many individuals choose not to work, so their hours are 0. For those who do work, the hours are positive and continuous.

Household expenditure on a durable good (e.g., a car).

A firm's R&D spending.
:::

## Latent Variable Framework 

- The Tobit model assumes a single underlying decision process. Just as in the logit/probit case, we model a **latent (unobserved) variable**, $y^*$, which represents the true desired level or propensity.

  $$
    y_i^* = β_0 + β_1X_{1i} + β_2X_{2i} + ... + u_i, \text{ where } u_i \sim N(0, \sigma^2)
  $$

- $y_i^*$ can be thought of as the "desired hours of work" or "propensity to spend." It can be negative.

- **Observation Rule:** We only observe the actual outcome, $y_i$, which is a censored version of $y_i^*$.
  - $y_i = y_i^*$   if   $y_i^* > 0$
  - $y_i = 0$       if   $y_i^* \leq 0$

## Why OLS Fails

- **OLS on positives only:** If we drop the zero-outcome observations, we induce selection bias. 
  - We are only looking at a group for whom the error term $u_i$ was large enough to push $y_i^*$ above zero. 
  - This means $E[u_i | y_i > 0] \neq 0$, biasing the coefficients.
- **OLS on all data (including zeros):** 
  - The relationship between $X$ and the *observed* $y_i$ is non-linear. The conditional expectation $E[y_i | X]$ is a complex function, not the simple linear form $\beta_0 + \beta_1 X_1 + \dots$. 
  - OLS will incorrectly estimate the linear relationship, typically attenuating the coefficients towards zero.

## Solution: Maximum Likelihood Estimation

- We construct a likelihood function that accounts for the two types of observations:
  - For an observation where **$y_i = 0$**, its contribution to the likelihood is the probability that $y_i^* \leq 0$.
  - $P(y_i^* \leq 0) = P(u_i \leq -(\beta_0 + \beta_1 X_{1i} + ...)) = \Phi(-\frac{\beta_0 + \beta_1 X_{1i} + ...}{\sigma})$
  - For an observation where **$y_i > 0$**, its contribution is the probability density of observing that specific value of $y_i$.
  - This is the standard normal probability density function (PDF), evaluated at $y_i$.

- Hence, the likelihood is:

    $$
      \prod_{i=1}^N \Phi \left(-\frac{\beta_0 + \beta_1 X_{1i} + \dots}{\sigma}\right)^{I(Y_i = 0)} \cdot \phi \left(\frac{Y_j - [\beta_0 + \beta_1 X_{i1} + \dots]}{\sigma}\right)^{I(Y_i >0)}
    $$
  
- MLE finds the parameter values ($\beta_0, \beta_1, ..., \sigma$) that maximize the joint probability (the product of these individual likelihoods) of observing our actual sample.

## Interpretation 

- The estimated $β$ coefficients measure the effect of an X-variable on the *latent variable* $y^*$, **not** on the observed outcome $y$. 
- For observations above the censoring point, the marginal effect is:
    $$
      \frac{\partial E(y_i∣y_i>0,x_i)}{\partial x_j}=\beta_j \cdot Pr(y_i > 0| x_i)
    $$

  - This means the effect of $x_j$ on the observed $y_i$ (for uncensored data) is attenuated by the probability of being uncensored.
- Effect on the Expected Value of the Observed Variable ($E[y_i]$):
  - The overall marginal effect (for both censored and uncensored observations) is:
  $\frac{\partial E[y_i | x_i]}{x_j} =\beta_j \Phi(\frac{\beta_0 + \beta_1 x_{i1} + \dots}{\sigma})$
  - Hence, to find the effect on the observed outcome, one must calculate marginal effects, which are more complex.
  
## Visualization

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.optimize import minimize
from scipy.stats import norm

# --- 1. Set Simulation Parameters ---
np.random.seed(42) # for reproducibility
n_obs = 200        # Number of observations
beta_0 = -5        # True intercept
beta_1 = 10        # True slope
sigma = 5          # Standard deviation of the error term
censor_point = 0   # The point at which the data is left-censored/truncated

# --- 2. Simulate Data ---
# Generate independent variable X
X = np.random.uniform(low=0, high=2, size=n_obs)
X_with_const = sm.add_constant(X) # Add a constant for matrix multiplication

# Generate the error term
epsilon = np.random.normal(loc=0, scale=sigma, size=n_obs)

# Generate the latent (unobserved) dependent variable y*
# This is the true relationship we want to uncover
y_latent = beta_0 + beta_1 * X + epsilon

# Generate the observed dependent variable y by censoring it at 0
# Any value of y_latent below the censor_point becomes the censor_point
y_observed = np.maximum(censor_point, y_latent)

# Create a pandas DataFrame for easier handling
df = pd.DataFrame({
    'X': X,
    'y_latent': y_latent,
    'y_observed': y_observed
})

# --- 3. Estimate OLS on Truncated Data ---
# To simulate truncated data, we simply drop all censored observations
df_truncated = df[df['y_observed'] > censor_point].copy()

# Estimate the OLS model
ols_model = sm.OLS(df_truncated['y_observed'], sm.add_constant(df_truncated['X']))
ols_results = ols_model.fit()
ols_beta_0, ols_beta_1 = ols_results.params

# --- 4. Estimate Tobit Model using Maximum Likelihood Estimation (MLE) ---
# The Tobit log-likelihood function has two parts:
# 1. For non-censored observations (y > 0): the log of the normal PDF
# 2. For censored observations (y = 0): the log of the normal CDF

def tobit_log_likelihood(params, X, y, censor_point):
    # Unpack parameters
    beta = params[:-1]
    # We estimate log(sigma) to ensure sigma is always positive
    sigma_mle = np.exp(params[-1])

    # Separate observed and censored data
    observed_mask = y > censor_point
    censored_mask = ~observed_mask
    
    # Linear prediction
    y_pred = X @ beta

    # Log-likelihood for observed part
    ll_observed = np.sum(norm.logpdf(y[observed_mask], loc=y_pred[observed_mask], scale=sigma_mle))
    
    # Log-likelihood for censored part
    ll_censored = np.sum(norm.logcdf(censor_point, loc=y_pred[censored_mask], scale=sigma_mle))
    
    # Total log-likelihood (we return the negative because we are minimizing)
    return -(ll_observed + ll_censored)

# Initial guess for the optimization (using OLS results can be a good start)
initial_guess = [ols_beta_0, ols_beta_1, np.log(np.std(df_truncated['y_observed']))]

# Minimize the negative log-likelihood function
# We use the full (censored) dataset for Tobit
tobit_results = minimize(
    fun=tobit_log_likelihood,
    x0=initial_guess,
    args=(X_with_const, df['y_observed'], censor_point),
    method='L-BFGS-B' # A robust optimization algorithm
)

# Extract estimated Tobit parameters
tobit_beta_0, tobit_beta_1 = tobit_results.x[:2]
tobit_sigma = np.exp(tobit_results.x[2])

# --- 5. Plot the Results ---
plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(12, 8))

# Scatter plot of the OBSERVED data
# Color the censored points differently to make them visible
ax.scatter(df['X'][df['y_observed'] > censor_point], df['y_observed'][df['y_observed'] > censor_point], 
           alpha=0.6, label='Observed Data (y > 0)')
ax.scatter(df['X'][df['y_observed'] == censor_point], df['y_observed'][df['y_observed'] == censor_point], 
           color='gray', alpha=0.6, label='Censored Data (y = 0)')

# Create a range of X values for plotting the lines
x_plot = np.linspace(df['X'].min(), df['X'].max(), 100)

# Plot the TRUE underlying relationship
y_true = beta_0 + beta_1 * x_plot
ax.plot(x_plot, y_true, 'k--', linewidth=2.5, label=f'True Relationship (Latent)')

# Plot the OLS regression line
y_ols = ols_beta_0 + ols_beta_1 * x_plot
ax.plot(x_plot, y_ols, 'r-', linewidth=2, label='OLS on Truncated Data (Biased)')

# Plot the Tobit regression line (which estimates the latent relationship)
y_tobit = tobit_beta_0 + tobit_beta_1 * x_plot
ax.plot(x_plot, y_tobit, 'g-', linewidth=2, label='Tobit Model (Unbiased)')

# Add a horizontal line at the censoring point
ax.axhline(censor_point, color='black', linestyle=':', linewidth=1)

# Formatting
ax.set_title('Tobit vs. OLS on Left-Truncated Data', fontsize=16)
ax.set_xlabel('Independent Variable (X)', fontsize=12)
ax.set_ylabel('Dependent Variable (Y)', fontsize=12)
ax.legend(loc='upper left', fontsize=11)
ax.set_ylim(df['y_observed'].min() - 5, df['y_observed'].max() + 5);
ax.set_xlim(df['X'].min() - 0.1, df['X'].max() + 0.1);

plt.show()
```

# Summary

## What did we do?

- **Binary outcomes are everywhere.** 
  - Standard OLS (the LPM) is a simple starting point but is flawed (out-of-range predictions, bad errors) in some ways, but good (interaction effects, integration with panel data) in others.

- **Probit and Logit**:
  - These are the standard solutions. They are derived from a **latent variable** model and use a non-linear CDF to constrain predictions between 0 and 1.

- **Maximum Likelihood Estimation (MLE)**: 
  - We looked at these models are estimated, MLE, which finds the parameters that make the observed data most probable.

- **Interpretation**: 
  - We found that raw coefficients are not marginal effects, we can use **Average Marginal Effects (AMEs)** to talk about changes in probability. In logit, we can use use **Odds Ratios** for Logit models for a multiplicative interpretation.
  
## What did we do? (Cont.)

- **Goodness-of-fit**: 
  - We discussed various ways of evaluating binary outcome models, and discussed their pros and cons. 
  
- **Prediction**:
  - We talked about prediction on unseen data using different methods. 


# The End

