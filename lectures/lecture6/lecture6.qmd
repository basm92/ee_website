---
title: "Empirical Economics"
subtitle: "Lecture 6: Binary Outcome Data"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Lecture 6 - Binary Outcome Data'
resources:
  - demo.pdf
---


```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Outline

## Course Overview

- Statistics and Probability - Basic Concepts
- Statistics and Probability - Hypothesis Testing
- The Linear Regression Model
- Time Series Data
- Panel Data (FE) and Control Variables
- Binary Outcome Data
- Potential Outcomes and Difference-in-differences
- Hands-on Econometrics in Practice

## What do we do today?

- Introduce binary outcome data and a straightforward way of analyzing it: the **linear probability model** (LPM).

- Talk about various shortcomings of the LPM, and introduces several alternatives, in particular, **logistic regression** and the **probit model**. 

- Introduce an alternative method to OLS, namely **maximum likelihood estimation** (MLE). 

- Discuss and contrast pitfalls of these models. 

- Talk about more general ways to deal with **truncated** and **selected data**, such as the Tobit and Heckman models. 

# Introduction

## Binary Outcomes

- Many interesting economic and social questions have a binary (0/1) outcome. We need special tools to model these.

:::{.callout-tip title="Examples: Binary Outcomes"}

*   **Labor Economics:** Does a person participate in the labor force? (Yes=1, No=0)
*   **Finance:** Does a company default on its loan? (Default=1, No Default=0)
*   **Marketing:** Does a consumer purchase a product after seeing an ad? (Purchase=1, No Purchase=0)
*   **Health Economics:** Does a patient's insurance status affect whether they receive a certain treatment? (Treatment=1, No Treatment=0)
*   **Political Science:** Does a person vote for a specific candidate? (Vote=1, Don't Vote=0)

:::

- Our dependent variable, $y$, can only take two values: 0 and 1.

## Linear Probability Model (LPM)

- What if we just use what we know? Ordinary Least Squares (OLS).

- When we apply OLS to a binary dependent variable, we call it the **Linear Probability Model (LPM)**.

:::{.callout-note title="Definition: Linear Probability Model"}

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \epsilon_i$$

Where $y_i$ is either 0 or 1.

:::

- **Key Insight:** The expected value of a binary variable is the probability that it equals 1.
$E[y_i | X_i] = 1 \cdot P(y_i=1 | X_i) + 0 \cdot P(y_i=0 | X_i) = P(y_i=1 | X_i)$
- This makes interpretation very appealing...

## LPM Interpretation

- Since $E[y_i | X_i] = P(y_i=1 | X_i)$, the LPM becomes:
$P(y_i=1 | X_i) = \beta_0 + \beta_1 x_{1i} + ...$

- **Interpretation of Coefficients:**
  - $\beta_k$ is the change in the *probability* that $y=1$ for a one-unit change in $x_k$, holding other factors constant.
  -  **Simplicity:** Easy to estimate (just OLS) and coefficients are incredibly easy to interpret as changes in probability.

:::{.callout-tip title="Example: Interpretation of LPM"}
Suppose we have a model like: `employed = 0.20 + 0.15 * education_years`

Each additional year of education is associated with a 0.15 (or 15 percentage point) increase in the probability of being employed.
:::

## Problems with LPM: Out-of-Bounds Predictions

- The model is linear, but probability is bounded by $[0, 1]$. The LPM doesn't know this.

  $$P(y=1|X) = \hat{\beta}_0 + \hat{\beta}_1 X$$

- Nothing in the OLS mechanics prevents the predicted value, $\hat{y}$, from being less than 0 or greater than 1 for certain values of X.
  - Interpretation of a predicted probability of 1.2 or -0.1 is nonsensical. 
  
## Illustration

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 7
#| # Step 1: Load necessary libraries
# install.packages(c("ggplot2", "dplyr")) # Run this if you don't have them
library(ggplot2)
library(dplyr)

# Step 2: Simulate data
# We'll create data where the probability of an event (y=1) is strongly
# related to a continuous variable (x). We'll make the range of x wide
# enough to guarantee the out-of-bound problem becomes visible.

set.seed(42) # for reproducibility

# Create a wide range for the independent variable 'x'
x <- seq(-15, 25, by = 0.5)

# Define the true probability using a logistic (S-shaped) function.
# This is a more realistic way to model probabilities.
# z = -2 + 0.3*x  (This is the log-odds)
true_prob <- 1 / (1 + exp(-(-2 + 0.3 * x)))

# Generate the binary outcome 'y' (0 or 1) based on this true probability
y <- rbinom(n = length(x), size = 1, prob = true_prob)

# Combine into a data frame
sim_data <- data.frame(x, y, true_prob)

# Step 3: Fit the models
# Model 1: The Linear Probability Model (LPM)
# This is just a standard linear regression (lm) on a binary outcome.
lpm_model <- lm(y ~ x, data = sim_data)


# Step 4: Generate predictions for visualization
# We create a grid of x-values to get smooth prediction lines.
prediction_grid <- data.frame(x = seq(min(sim_data$x), max(sim_data$x), length.out = 200))

# Get predictions from the LPM
prediction_grid$lpm_pred <- predict(lpm_model, newdata = prediction_grid)

# Get predictions from the Logit model.
# type = "response" gives us probabilities (P(Y=1))

# Step 5: Visualize the results with ggplot
ggplot(sim_data, aes(x = x, y = y)) +
  
  # Add horizontal lines at 0 and 1 to show the valid probability boundaries
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
  
  # Plot the raw data points (0s and 1s)
  # Jittering helps to see the density of points
  geom_point(position = position_jitter(width = 0, height = 0.03), alpha = 0.5) +
  
  # Plot the prediction line from the Linear Probability Model (LPM)
  geom_line(data = prediction_grid, aes(x = x, y = lpm_pred, color = "LPM"), size = 1.2) +
  
  # Improve the aesthetics
  scale_color_manual(name = "Model",
                     values = c("LPM" = "dodgerblue"),
                     labels = c("Linear Probability Model")) +
  labs(
    title = "Out-of-Bound Predictions in a Linear Probability Model",
    subtitle = "The LPM (blue) predicts probabilities < 0 and > 1",
    x = "Independent Variable (X)",
    y = "Predicted Probability / Actual Outcome (Y)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

##  Problems with LPM: The Error Term

- The assumptions of the Classical Linear Model are violated.
- Inherent Heteroskedasticity:
  - The variance of the error term depends on the values of the independent variables.
  - $Var(\epsilon_i | X_i) = p_i(1-p_i)$, where $p_i = P(y_i=1|X_i)$
  - Since $p_i$ depends on X, the variance is not constant. This is heteroskedasticity.
  - **Consequence:** OLS standard errors are biased. We could use robust standard errors to fix this problem. 
  
## LPM Summary

- **Pros:** Simple to interpret. It is also easy to incorporate fixed effects in an LPM.
- **Cons:** Nonsensical predictions, violates key OLS assumptions.

- We need a model that constrains the predicted probability to be between 0 and 1. We need a non-linear model.

- This requires a different way of thinking about the choice process.


# Logit and Probit Models

## The Latent Variable Framework

- Let's imagine the binary choice is driven by an unobserved, underlying continuous variable, $y^*$.

- $y^*$ can be thought of as the "net utility," "propensity," or "tendency" to choose 1.

:::{.callout-note title="Definition: Latent Variable Framework"}

$y_i^* = \beta_0 + \beta_1 x_i + \epsilon_i$

We don't observe $y^*$. We only observe the outcome, *y*, based on a threshold (usually normalized to 0):

- If $y_i^* > 0$, then we choose Yes ($y_i = 1$)
- If $y_i^* \le 0$, then we choose No ($y_i = 0$)

The probability that $y_i=1$ is the probability that $y_i^*$ is greater than 0.

  $$
    P(y_i=1) = P(y_i^* > 0) = P(\beta_0 + \beta_1 x_i + \epsilon_i > 0) = P(\epsilon_i > -(\beta_0 + \beta_1 x_i))
  $$

:::


## From Latent Variables to Probit & Logit

- This links the probability of the observed outcome to the distribution of the unobserved error term, $\epsilon_i$.

- The final step depends on what we assume about the distribution of $\epsilon_i$.

  $$
    P(y_i=1) = P(\epsilon_i > -X_i'\beta) = 1 - F(-X_i'\beta)
  $$

- where *F* is the Cumulative Distribution Function (CDF) of $\epsilon_i$.

## Probit and Logit

- Two common choices for $F$ are the normal distribution (Probit model) and the logistic distribution (Logit model)

:::{.callout-note title="Definition: Probit and Logit Models"}

**Probit Model:** Assumes $\epsilon_i$ follows a **Standard Normal** distribution.

- $P(y_i=1 | X_i) = 1 - \Phi (-\beta x_i) =  \Phi(\beta x_i)$
- where $\Phi(\cdot)$ is the Standard Normal CDF.

**Logit Model:** Assumes $\epsilon_i$ follows a **Standard Logistic** distribution.

- $P(y_i=1 | X_i) = 1 - \Lambda(-\beta x_i) = \Lambda(\beta x_i) = \frac{e^{\beta x_i}}{1 + e^{\beta x_i}}$
- where $\Lambda(\cdot)$ is the Standard Logistic CDF.
  
In both cases, $1-F(-\beta x_i) = F(\beta x_i)$, since both distributions are symmetric.  Both CDFs produce the S-shaped curve we need. 

:::

## Visualization Logit and Probit Models

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 10

library(ggplot2)
library(dplyr)

# Set a seed for reproducibility
set.seed(42)

# Number of data points
n <- 200

# Create a predictor variable 'x'
x <- runif(n, min = -10, max = 10)

# Define a "true" underlying relationship (log-odds)
# log-odds = 0.5 (intercept) + 0.8 * x (slope)
true_log_odds <- 0.5 + 0.8 * x

# Convert log-odds to probabilities using the logistic function
true_prob <- 1 / (1 + exp(-true_log_odds))

# Generate the binary outcome 'y' based on these probabilities
y <- rbinom(n, size = 1, prob = true_prob)

# Create our final data frame
sim_data <- data.frame(x, y)

# Create a grid of x-values for plotting the smooth curves
x_grid <- seq(min(sim_data$x), max(sim_data$x), length.out = 200)

# --- Define Model A's coefficients and calculate probabilities ---
b0_A <- 0.5  # Intercept
b1_A <- 0.8  # Slope
prob_A <- 1 / (1 + exp(-(b0_A + b1_A * x_grid)))

# --- Define Model B's coefficients and calculate probabilities ---
b0_B <- 4.0  # Higher intercept (shifts left)
b1_B <- 2.5  # Steeper slope
prob_B <- 1 / (1 + exp(-(b0_B + b1_B * x_grid)))

# Combine the curves into a single data frame for plotting with ggplot
curves_df <- data.frame(
  x = x_grid,
  Probability_A = prob_A,
  Probability_B = prob_B
) %>%
  # Pivot to a long format, which is ideal for ggplot
  tidyr::pivot_longer(
    cols = c(Probability_A, Probability_B),
    names_to = "Model",
    values_to = "Probability"
  ) %>%
  # Make the model names prettier for the legend
  mutate(Model = recode(Model,
    "Probability_A" = "Model A: log-odds = 0.5 + 0.8x",
    "Probability_B" = "Model B: log-odds = 4.0 + 2.5x"
  ))

ggplot() +
  # 1. Plot the raw data points
  # We use jitter to prevent overplotting, especially at y=0 and y=1
  geom_point(data = sim_data, aes(x = x, y = y), alpha = 0.4,
             position = position_jitter(width = 0.1, height = 0.05)) +

  # 2. Plot the logistic curves from our two models
  geom_line(data = curves_df, aes(x = x, y = Probability, color = Model), size = 1.2) +

  # 3. Add labels and improve aesthetics
  labs(
    title = "Visualizing Logit Models with Different Coefficients",
    subtitle = "How the sigmoid curve moves based on the intercept and slope",
    x = "Predictor (x)",
    y = "Probability of Outcome (y=1)",
    color = "Model Specification" # Legend title
  ) +
  scale_y_continuous(breaks = seq(0, 1, 0.25), limits = c(-0.1, 1.1)) +
  scale_color_manual(values = c("steelblue", "darkred")) + # Custom colors for the lines
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")

```

# Estimation of Logit and Probit

## Estimation: Maximum Likelihood (MLE)

- Remember that in OLS, we take $\sum (y_i - \hat{y_i} (\beta, x_i))^2$ and set the derivative to zero to express the optimal $\beta$ coefficients. 
  - These conditions **do not** lead to a unique expression for the $\beta$-coefficients in Logit/Probit. 
  - Hence we can't use OLS. Instead, we use **Maximum Likelihood Estimation (MLE)**.

:::{.callout-note title="Definition: Maximum Likelihood Estimation"}

MLE finds the parameter values ($\beta$) that *maximize the probability of observing the actual data we collected*.
:::

- In other words: "Given our data, what are the most likely parameter values that could have generated it?"

## Simple MLE Example: A Biased Coin

:::{.callout-tip title="Example: A Biased Coin"}

Imagine you flip a coin 10 times and get 7 Heads (H) and 3 Tails (T).

- Data: {H, H, T, H, H, T, H, H, T, H}
- Question: What is your best guess for *p*, the probability of getting a Head?

**The Likelihood Function $L(p | \text{Data})$:** The probability of observing this specific sequence is: 
  $$L(p) = p \cdot p \cdot (1-p) \cdot p \cdot p \cdot (1-p) \cdot p \cdot p \cdot (1-p) \cdot p = p^7 (1-p)^3$$

The goal is to find the value of *p* that maximizes this function.

- **Intuition:** Your gut says p = 0.7.
- **Optimization:** We can take lots of values of $p$ and calculate the likelihood, and see which value of $p$ gives us the highest likelihood. 
- The result is indeed $\hat{p}_{MLE} = 0.7$.

This is the value of *p* that makes the data we saw "most likely."

:::

## MLE Visualization

:::{.callout-tip title="MLE Visualization: Biased Coin"}

```{r}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
#| fig-height: 4

library(ggplot2)
library(tidyr)
# 1. Define the parameters from the data
num_heads <- 7
num_flips <- 10
num_tails <- num_flips - num_heads

# 2. Create a fine grid of possible values for the parameter p
# We avoid exactly 0 and 1 to prevent log(0) = -Inf issues.
p_grid <- seq(from = 0.001, to = 0.999, length.out = 1000)

# 3. Calculate the log-likelihood for each p in the grid
log_likelihood <- num_heads * log(p_grid) + num_tails * log(1 - p_grid)

# 4. Create a data frame for plotting
plot_data <- data.frame(
  p = p_grid,
  log_likelihood = log_likelihood
)

# 5. Define the Maximum Likelihood Estimate (MLE) for annotation
p_mle <- num_heads / num_flips

# Find the maximum value of the log-likelihood to place our annotation point
max_log_likelihood <- max(plot_data$log_likelihood)


# 6. Create the plot
ggplot(plot_data, aes(x = p, y = log_likelihood)) +
  
  # Draw the log-likelihood curve
  geom_line(color = "dodgerblue", size = 1.2) +
  
  # Add a vertical dashed line marking the MLE
  geom_vline(xintercept = p_mle, linetype = "dashed", color = "red", size = 1) +
  
  # Add a point at the maximum of the curve
  geom_point(aes(x = p_mle, y = max_log_likelihood), color = "red", size = 4) +
  
  # Add a text label for the MLE. We position it slightly below the peak.
  annotate(
    geom = "text",
    x = p_mle,
    y = max_log_likelihood - 0.4, # Adjust vertical position
    label = paste("MLE: p =", p_mle),
    color = "red",
    size = 5,
    hjust = 0.4  # Adjust horizontal position relative to the line
  ) +
  
  # Add informative labels and a title
  labs(
    title = "Log-Likelihood for Coin Flip Data (7 Heads, 3 Tails)",
    subtitle = "The peak of the curve identifies the most likely value for p.",
    x = "Parameter p (Probability of a Head)",
    y = "Log-Likelihood: 7*log(p) + 3*log(1-p)"
  ) +
  
  # Use a clean theme for a professional look
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "gray30")
  )
```

:::

## MLE for Probit/Logit Models

- For our regression models, the principle is the same but more complex.

- The **Likelihood Function** is the product of the probabilities of each individual observation:
$L(\beta) = \prod_{i=1}^{N} [P(y_i=1|X_i)]^{y_i} \cdot [1 - P(y_i=1|X_i)]^{1-y_i}$
  - If $y_i=1$, we use $P(y_i=1|X_i)$
  - If $y_i=0$, we use $1-P(y_i=1|X_i) = P(y_i=0|X_i)$

- We plug in the Probit or Logit formula for $P(y_i=1|X_i)$ and use a computer to find the $\beta$ vector that maximizes this function (or more commonly, the log of this function, the **Log-Likelihood**).

## MLE: Probit Example

:::{.callout-tip title="Example: MLE for Probit"}
**Likelihood Function:**
$$
\mathcal{L}(\beta) = \prod_{i=1}^n \Phi(X_i\beta)^{Y_i} [1 - \Phi(X_i\beta)]^{1-Y_i}
$$

**Log-Likelihood Function:**
$$
\ln\mathcal{L}(\beta) = \sum_{i=1}^n \left\{ Y_i \ln\Phi(X_i\beta) + (1-Y_i)\ln[1-\Phi(X_i\beta)] \right\}
$$

**First-Order Condition (FOC):**
$$
\frac{\partial \ln\mathcal{L}(\beta)}{\partial \beta} = \sum_{i=1}^n \left[ \frac{Y_i \phi(X_i\beta)}{\Phi(X_i\beta)} - \frac{(1-Y_i)\phi(X_i\beta)}{1-\Phi(X_i\beta)} \right] X_i = 0
$$
:::

## Interpreting Coefficients

- In Probit and Logit models, the estimated coefficients ($\hat{\beta}$) are **NOT** marginal effects.

  $$
    P(y=1|X) = F(\beta x_i )
  $$

- A one-unit change in $x_k$ changes the *argument* $\beta x_i$ by $\beta_k$.
- But the change in the *probability* depends on the slope of the S-curve, which depends on the values of *all* X variables:

  $$
    \frac{\partial P(y=1 | x_i)}{\partial x_i} = F'(\beta x_i) \cdot \beta = f(\beta x_i) \cdot \beta
  $$

- This means that the change in the probability does not only depend on $\beta$, but also on the values of the independent variables $x_i$ and the function $F'$. 
- So, how do we get meaningful interpretations?

## Interpretation Method 1: Marginal Effect at the Mean

- We simply calculate the change in predicted probability for a change in an x-variable at a particular value.

:::{.callout-note title="Marginal Effect at the Mean"}

The marginal effect _at the mean_ equals: 

  $$
    \frac{\partial P(y=1|X)}{\partial x_k} = f(\beta \bar{x}) \cdot \beta_k
  $$
  
where $f(\cdot)$ is the PDF, the derivative of the CDF $F$.

In other words, we simply calculate the MEs with all X variables set to their sample means.

*Problem:* No single observation in the data might actually have all mean values. "The average person" doesn't exist.
    
:::

## Interpretation Method 2: Average Marginal Effect

- **This is the modern, preferred standard.** It gives the best summary of the effect for the population in the sample.

:::{.callout-note title="Average Marginal Effect"}

The Average Marginal Effect (AME) equals:

  $$
    \frac{\partial P(y=1|X)}{\partial x_k} = \frac{1}{N} \sum_{i=1}^N f(\beta x_i) \cdot \beta_k
  $$

In other words, we compute the marginal effect using the values of each observations, and then take the average. 

:::

## Interpretation Method 3: Odds Ratios (Logit Only)

- For the Logit model, we can interpret results using **Odds Ratios**.

:::{.callout-note title="Definition: Odds Ratio (OR)"}
The odds are defined as: $\text{Odds}=\frac{P(y=1)}{P(y=0)} = \frac{P}{1-P}$.

For example, if P=0.8, Odds = 0.8 / 0.2 = 4 (or "4 to 1"). If P=0.5, Odds = 0.5 / 0.5 = 1.

:::

- The Logit model can be written as:
  $$
    \ln(\frac{P}{1-P}) = \ln(\text{Odds}) = \beta_0 + \beta_1 x_1 + ...
  $$ {#eq-lodds}
  
- The model is linear in the *log-odds*.

## Interpretation Method: Odds Ratio (Cont.)

- Exponentiating both sides of [@eq-lodds] gives: $\text{Odds}=e^{\beta_0 + \beta_1 X_1 + \dots}$
- The _Odds Ratio_ compares the odds for two different values of a predictor $x_j$. Suppose we increase $x_j$ by 1 unit while holding other predictors constant:
  $$
    \text{OR}_j = \frac{\text{Odds} (X_j + 1)}{\text{Odds} (X_j)} = e^{\beta_j}
  $$
  
## Odds Ratios Interpretation

*   **Interpretation:** A one-unit increase in $x_k$ multiplies the odds of success by a factor of $e^{\beta_k}$.

:::{.callout-tip title="Example: Odds Ratios"}

If $\hat{\beta}_k = 0.2$, then $e^{0.2} \approx 1.22$. A one-unit increase in $x_k$ increases the odds of the outcome by 22%.

If $\hat{\beta}_k = -0.5$, then $e^{-0.5} \approx 0.61$. A one-unit increase in $x_k$ decreases the odds by about 39% (multiplies them by 0.61).

:::



## Interaction Terms

- Interaction effects in probit/logit models are also not straightforward: 

- Suppose our model has two independent variables and an interaction effect, i.e. $P(y | x_1, x_2) = F(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2)$. 

- We already know the marginal effect (ME) of $x_1$ on the conditional probability is the partial derivative of $P(y=1 | X)$ with respect to $x_1$: 

  $$
    ME_{x_1} = f(\beta_0+\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2) \cdot (\beta_1 + \beta_3 x_2)
  $$

- Note that the marginal effect of $x_1$ is not constant; it depends on the value of $x_2$ (due to the $\beta_3 x_2$ term) and on the values of *all* variables through the $f(\cdot)$ term.


## Interaction Terms (Cont.)

- The interaction effect is defined as the change in the marginal effect of one variable, $x_1$, as the other variable, $x_2$, changes. Hence, we have to look for the **cross-partial derivative**:

  $$
    \text{Interaction Effect (IE)} = \frac{\partial (ME_{x_1})}{\partial x_2} = \frac{\partial^2 P(Y|x_1, x_2)}{\partial x_2 \partial x_1}
  $$

- We differentiate $ME(x_1)$ with respect to $x_2$ using the product rule:

  $$
    \frac{\beta_1 + \beta_3 x_2} {\partial x_2} = \beta_3
  $$
  
- and:

  $$
    \frac{\partial f(\beta_0+\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2)}{\partial x_2} = f'(\beta_0+\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2) \cdot (\beta_2 + \beta_3 x_1) 
  $$

- Hence, according to the product rule, 
  
  $$
  \begin{align}
    \text{IE} = [f'(\beta_0+\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2) \cdot (\beta_2 + \beta_3 x_1)] (\beta_1 + \beta_3 x_2) + \\ f(\beta_0+\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2) \beta_3.
  \end{align}
  $$

## Analysis of the result

- This formal result is profoundly different from the simple coefficient $\beta_3$ in the LPM. It consists of two parts:

  1.  **$f(.)\beta_3$:**
    - The "direct" effect, which is the interaction coefficient $\beta_3$ scaled by the PDF `f(.)`. This part's sign is determined by the sign of $\beta_3$, but its magnitude depends on all covariates. 

  2.  **$f'(.)(\beta_1 + \beta_3 x_2)(\beta_2 + \beta_3 x_1)$:** 
    - The "non-linearity" or "functional form" effect. This term arises purely because $f(.)$ is not a constant. **Critically, this term exists even if $\beta_3= 0$**. In that case, the interaction effect would be $f'(.) \beta_1 \beta_2$, which is generally non-zero. This means that an interaction effect on the probabilities is an inherent feature of non-linear models, regardless of whether an interaction term is included in the linear index $X$. 

- **Conclusion:** The sign, magnitude, and even statistical significance of the true interaction effect on the probability can differ from that of the coefficient $\beta_3$. One cannot simply inspect $\beta_3$ to understand the interaction.

## Contrast with the Linear Probability Model (LPM)

- This insight was originally discovered by Ai and Norton (2003). 

- For the LPM, the model is $P(y=1 | X) = \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_1 x_2$. 
- In the LPM, the interaction effect on the probability is precisely and unambiguously equal to the coefficient on the interaction term, $\alpha_3$.
  - This starkly contrasts with the result from the probit/logit model, highlighting the interpretational complexity introduced by non-linearity.


## Goodness-of-Fit and Testing

- How well does our model fit the data?

- **Percent Correctly Predicted:**
  - If $\hat{p}_i > 0.5$, predict $y=1$. If $\hat{p}_i \le 0.5$, predict $y=0$.
  - Compare predictions to actual outcomes. What percentage did we get right?
  - Intuitive, but sensitive to the 0.5 cutoff.

- **Pseudo R-Squared:**
  - Several versions exist, like **McFadden's R-squared**.
  - $R^2_{McF} = 1 - \frac{\ln L_{full}}{\ln L_{null}}$ (where $L_{null}$ is from a model with only an intercept).
  - Ranges from 0 to 1, but values are much lower than OLS R-squared. A value of 0.2-0.4 can indicate a very good fit. **Do not compare to OLS R-squared!**

- **Likelihood Ratio (LR) Test:**
  - Tests the joint significance of a set of variables (like the F-test in OLS).
  - Compares the log-likelihood of the restricted model to the unrestricted (full) model.
  - $LR = 2(\ln L_{full} - \ln L_{restricted})$ which follows a $\chi^2$ distribution.

## Choosing Between Probit and Logit

- **In practice, the choice rarely matters much.**
  - The Normal and Logistic distributions are very similar, except the Logistic has slightly "fatter tails" (it's less sensitive to outliers).
  - Predicted probabilities from both models are usually very close.

- **Rule of Thumb:**
  - Logit coefficients are larger than Probit coefficients by a factor of ~1.6 - 1.8.
  - Logit Marginal Effects $\approx$ Probit Marginal Effects.
  - **Logit is often preferred** due to the simpler interpretation of coefficients as log-odds and the direct calculation of odds ratios.

## Logit and Probit in Software

To do


# Limited Dependent Variable Models

## Dealing with Truncated and Selected Data

- In many situations, we might have a **biased sample**. 
  - Standard Ordinary Least Squares (OLS) relies on the crucial assumption that the expected value of the error term, conditional on the explanatory variables, is zero: $E[u | X] = 0$. 
  - This assumption is violated when our sample is not randomly drawn from the population of interest. 
- This can happen in two common ways:
  1.  **Censoring:** 
    - The dependent variable has a "floor" or "ceiling." We observe the explanatory variables for everyone, but the dependent variable is clustered at a limit value (e.g., zero) for a part of the sample.
  2.  **Sample Selection (or Incidental Truncation):** 
    - We are missing data on the dependent variable for a non-random subset of our observations. We don't even see the "zero"—the data is simply not there.

- In both cases, running OLS on the observed data leads to biased and inconsistent parameter estimates. 
  - We need models that explicitly account for the non-random nature of the sample.

## The Tobit Model (Censored Dependent Variable)

- The Tobit model is used when the dependent variable is continuous over a certain range but has a significant number of observations "piled up" at a lower (or upper) limit.

:::{.callout-tip title="Example: Censored Data"}
Annual hours worked. Many individuals choose not to work, so their hours are 0. For those who do work, the hours are positive and continuous.

Household expenditure on a durable good (e.g., a car).

A firm's R&D spending.
:::

## Latent Variable Framework 

- The Tobit model assumes a single underlying decision process. Just as in the logit/probit case, we model a **latent (unobserved) variable**, $y^*$, which represents the true desired level or propensity.

  $$
    y_i^* = β_0 + β_1X_{1i} + β_2X_{2i} + ... + u_i, \text{ where } u_i \sim N(0, \sigma^2)
  $$

- $y_i^*$ can be thought of as the "desired hours of work" or "propensity to spend." It can be negative.

- **Observation Rule:** We only observe the actual outcome, $y_i$, which is a censored version of $y_i^*$.
  - $y_i = y_i^*$   if   $y_i^* > 0$
  - $y_i = 0$       if   $y_i^* \leq 0$

## Why OLS Fails

- **OLS on positives only:** If we drop the zero-outcome observations, we induce selection bias. 
  - We are only looking at a group for whom the error term $u_i$ was large enough to push $y_i^*$ above zero. 
  - This means $E[u_i | y_i > 0] \neq 0$, biasing the coefficients.
- **OLS on all data (including zeros):** 
  - The relationship between $X$ and the *observed* $y_i$ is non-linear. The conditional expectation $E[y_i | X]$ is a complex function, not the simple linear form $\beta_0 + \beta_1 X_1 + \dots$. 
  - OLS will incorrectly estimate the linear relationship, typically attenuating the coefficients towards zero.

## Solution: Maximum Likelihood Estimation

- We construct a likelihood function that accounts for the two types of observations:
  - For an observation where **$y_i = 0$**, its contribution to the likelihood is the probability that $y_i^* \leq 0$.
  - $P(y_i^* \leq 0) = P(u_i \leq -(\beta_0 + \beta_1 X_{1i} + ...)) = \Phi(-\frac{\beta_0 + \beta_1 X_{1i} + ...}{\sigma})$
  - For an observation where **$y_i > 0$**, its contribution is the probability density of observing that specific value of $y_i$.
  - This is the standard normal probability density function (PDF), evaluated at $y_i$.

- Hence, the likelihood is:

  $$
    \prod_{i=1}^N \Phi \left(-\frac{\beta_0 + \beta_1 X_{1i}}{\sigma}\right)^{I(Y_i = 0)} \cdot \phi \left(\frac{Y_j - [\beta_0 + \beta_1 X_{i1} + \dots]}{\sigma}\right)^{I(Y_i >0)}
  $$
  
- MLE finds the parameter values ($\beta_0, \beta_1, ..., \sigma$) that maximize the joint probability (the product of these individual likelihoods) of observing our actual sample.

## Interpretation 

- The estimated $β$ coefficients measure the effect of an X-variable on the *latent variable* $y^*$, **not** on the observed outcome $y$. 
- For observations above the censoring point, the marginal effect is:
    $$
      \frac{\partial E(y_i∣y_i>0,x_i)}{\partial x_j}=\beta_j \cdot Pr(y_i > 0| x_i)
    $$

  - This means the effect of $x_j$ on the observed $y_i$ (for uncensored data) is attenuated by the probability of being uncensored.
- Effect on the Expected Value of the Observed Variable ($E[y_i]$):
  - The overall marginal effect (for both censored and uncensored observations) is:
  $\frac{\partial E[y_i | x_i]}{x_j} =\beta_j \Phi(\frac{\beta_0 + \beta_1 x_{i1} + \dots}{\sigma})$
  - Hence, to find the effect on the observed outcome, one must calculate marginal effects, which are more complex.
  
## Visualization

```{python}
#| echo: false
#| fig-align: 'center'
#| fig-width: 8
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.optimize import minimize
from scipy.stats import norm

# --- 1. Set Simulation Parameters ---
np.random.seed(42) # for reproducibility
n_obs = 200        # Number of observations
beta_0 = -5        # True intercept
beta_1 = 10        # True slope
sigma = 5          # Standard deviation of the error term
censor_point = 0   # The point at which the data is left-censored/truncated

# --- 2. Simulate Data ---
# Generate independent variable X
X = np.random.uniform(low=0, high=2, size=n_obs)
X_with_const = sm.add_constant(X) # Add a constant for matrix multiplication

# Generate the error term
epsilon = np.random.normal(loc=0, scale=sigma, size=n_obs)

# Generate the latent (unobserved) dependent variable y*
# This is the true relationship we want to uncover
y_latent = beta_0 + beta_1 * X + epsilon

# Generate the observed dependent variable y by censoring it at 0
# Any value of y_latent below the censor_point becomes the censor_point
y_observed = np.maximum(censor_point, y_latent)

# Create a pandas DataFrame for easier handling
df = pd.DataFrame({
    'X': X,
    'y_latent': y_latent,
    'y_observed': y_observed
})

# --- 3. Estimate OLS on Truncated Data ---
# To simulate truncated data, we simply drop all censored observations
df_truncated = df[df['y_observed'] > censor_point].copy()

# Estimate the OLS model
ols_model = sm.OLS(df_truncated['y_observed'], sm.add_constant(df_truncated['X']))
ols_results = ols_model.fit()
ols_beta_0, ols_beta_1 = ols_results.params

# --- 4. Estimate Tobit Model using Maximum Likelihood Estimation (MLE) ---
# The Tobit log-likelihood function has two parts:
# 1. For non-censored observations (y > 0): the log of the normal PDF
# 2. For censored observations (y = 0): the log of the normal CDF

def tobit_log_likelihood(params, X, y, censor_point):
    # Unpack parameters
    beta = params[:-1]
    # We estimate log(sigma) to ensure sigma is always positive
    sigma_mle = np.exp(params[-1])

    # Separate observed and censored data
    observed_mask = y > censor_point
    censored_mask = ~observed_mask
    
    # Linear prediction
    y_pred = X @ beta

    # Log-likelihood for observed part
    ll_observed = np.sum(norm.logpdf(y[observed_mask], loc=y_pred[observed_mask], scale=sigma_mle))
    
    # Log-likelihood for censored part
    ll_censored = np.sum(norm.logcdf(censor_point, loc=y_pred[censored_mask], scale=sigma_mle))
    
    # Total log-likelihood (we return the negative because we are minimizing)
    return -(ll_observed + ll_censored)

# Initial guess for the optimization (using OLS results can be a good start)
initial_guess = [ols_beta_0, ols_beta_1, np.log(np.std(df_truncated['y_observed']))]

# Minimize the negative log-likelihood function
# We use the full (censored) dataset for Tobit
tobit_results = minimize(
    fun=tobit_log_likelihood,
    x0=initial_guess,
    args=(X_with_const, df['y_observed'], censor_point),
    method='L-BFGS-B' # A robust optimization algorithm
)

# Extract estimated Tobit parameters
tobit_beta_0, tobit_beta_1 = tobit_results.x[:2]
tobit_sigma = np.exp(tobit_results.x[2])

# --- 5. Plot the Results ---
plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(12, 8))

# Scatter plot of the OBSERVED data
# Color the censored points differently to make them visible
ax.scatter(df['X'][df['y_observed'] > censor_point], df['y_observed'][df['y_observed'] > censor_point], 
           alpha=0.6, label='Observed Data (y > 0)')
ax.scatter(df['X'][df['y_observed'] == censor_point], df['y_observed'][df['y_observed'] == censor_point], 
           color='gray', alpha=0.6, label='Censored Data (y = 0)')

# Create a range of X values for plotting the lines
x_plot = np.linspace(df['X'].min(), df['X'].max(), 100)

# Plot the TRUE underlying relationship
y_true = beta_0 + beta_1 * x_plot
ax.plot(x_plot, y_true, 'k--', linewidth=2.5, label=f'True Relationship (Latent)')

# Plot the OLS regression line
y_ols = ols_beta_0 + ols_beta_1 * x_plot
ax.plot(x_plot, y_ols, 'r-', linewidth=2, label='OLS on Truncated Data (Biased)')

# Plot the Tobit regression line (which estimates the latent relationship)
y_tobit = tobit_beta_0 + tobit_beta_1 * x_plot
ax.plot(x_plot, y_tobit, 'g-', linewidth=2, label='Tobit Model (Unbiased)')

# Add a horizontal line at the censoring point
ax.axhline(censor_point, color='black', linestyle=':', linewidth=1)

# Formatting
ax.set_title('Tobit vs. OLS on Left-Truncated Data', fontsize=16)
ax.set_xlabel('Independent Variable (X)', fontsize=12)
ax.set_ylabel('Dependent Variable (Y)', fontsize=12)
ax.legend(loc='upper left', fontsize=11)
ax.set_ylim(df['y_observed'].min() - 5, df['y_observed'].max() + 5);
ax.set_xlim(df['X'].min() - 0.1, df['X'].max() + 0.1);

plt.show()
```

## The Heckman Model (Sample Selection Bias)

- The Heckman model is used when the dependent variable is missing for a group of observations due to a systematic selection process.

:::{.callout-tip title="Example: Missing DV"}

Wage determination. We only observe wages ($y_i$) for individuals who are employed. The decision to work is likely not random and may be correlated with the factors that determine wages.

The returns to a college education (we only observe wages for graduates); the effect of a training program on productivity (we only observe productivity for those who completed the program).
:::

## Model Set-Up

- The Heckman model assumes **two distinct processes**: a selection process and an outcome process.

- **Selection Equation (Probit Model):** A latent variable $s_i^*$ determines whether we observe the outcome.
  - $s_i^* = γ_0 + γ_1Z_{1i} + γ_2Z_{2i} + ... + v_i$, where $v_i \sim N(0, 1)$
  - We observe the outcome $y_i$ only if $s_i^* > 0$ (e.g., the offered wage exceeds the reservation wage).

- **Outcome Equation:** The model for the variable of interest, $y_i$.
  - $y_i = β_0 + β_1X_{1i} + β_2X_{2i} + ... + u_i$, where $u_i \sim N(0, σ^2)$
  - This equation is only estimated for the selected sample (i.e., when $s_i^* > 0$).

## Why OLS Fails

- The bias arises if the unobserved factors in the two equations are correlated: $Corr(u_i, v_i) = \rho \neq 0$. 
  - If $\rho>0$, people with unobservably high wage potential ($u_i > 0$) are also more likely to work ($v_i > 0$).
- OLS on the selected sample (workers) suffers from omitted variable bias. The conditional expectation of the error is no longer zero:
  - $E[y_i | X_i, \text{sample is selected}] = β_0 + β_1X_{1i} + ... + E[u_i | s_i^* > 0]$
  - The term $E[u_i | s_i^* > 0]$ is non-zero if $\rho \neq 0$ and is a function of the $Z$ variables.

## Solution: Heckman's Two-Step Procedure

**Step 1: Estimate the Selection Equation.**
*   Run a Probit model of the selection decision (e.g., working vs. not working) on the **full sample**. The regressors are the Z variables.
*   From the Probit results, calculate the **Inverse Mills Ratio (IMR)** for each observation, often denoted $\lambda_i$.
    *   $\lambda_i = \frac{\phi(γ_0 + γ_1Z_{1i} + ...)}{\Phi(γ_0 + γ_1Z_{1i} + ...)}$ (where $\phi$ is the normal PDF, $\Phi$ is the normal CDF).
    *   The IMR is a measure of the "selection hazard" — the likelihood of being selected into the sample, given the Z variables. It acts as a proxy for the omitted term $E[u_i | s_i^* > 0]$.

**Step 2: Estimate the Corrected Outcome Equation.**
*   Run OLS on the outcome equation using **only the selected sample** (those for whom we observe $y_i$).
*   Include the calculated IMR ($\lambda_i$) as an additional explanatory variable:
    $y_i = β_0 + β_1X_{1i} + ... + β_{\lambda}\lambda_i + \text{error}$
*   The coefficient on the IMR, $β_{\lambda}$, is an estimate of $ρ \cdot σ_u$. A statistically significant $t$-test on this coefficient is a direct test for sample selection bias.

**Crucial Point - The Exclusion Restriction:** For robust identification, there must be at least one variable in the selection equation (Z) that is **not** in the outcome equation (X). This variable should influence the selection decision but not the outcome itself (e.g., number of young children may affect the decision to work but not the potential wage rate).

---
### **Slide 4: Tobit vs. Heckman - A Summary**

| Feature                   | Tobit Model                                                                                             | Heckman Model                                                                                                      |
| ------------------------- | ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| **The Problem**           | **Censoring**                                                                                           | **Selection Bias** (Incidental Truncation)                                                                         |
| **Dependent Variable ($y_i$)** | Observed for all individuals, but "piled up" at a limit value (e.g., 0) for a subset of the sample.  | **Missing** for a non-random subset of the sample.                                                                 |
| **Underlying Process**    | **One decision process.** The same factors (X variables) and parameters ($β$) determine both the probability of being at the limit and the value if above the limit. | **Two distinct processes.** One process for selection (driven by Z variables) and another for the outcome (driven by X variables). |
| **Model Structure**       | One equation for a latent variable $y^*$.                                                               | Two equations: a **selection equation** (Probit) and an **outcome equation** (linear model).                             |
| **Key Question to Ask**   | Are the zeros a legitimate outcome of the same choice? (e.g., "I choose to spend $0 on a car").          | Do the zeros (or missing values) represent a separate barrier preventing me from observing the outcome? (e.g., "I can't observe the wage of someone who isn't working"). |
| **Identification**        | Identified by the non-linearity of the likelihood function (normality assumption).                      | Relies heavily on an **exclusion restriction**: a variable in the selection equation that is not in the outcome equation. |
| **Estimation**            | Maximum Likelihood Estimation (MLE).                                                                    | Two-step procedure (Probit then OLS with IMR) or Full Information Maximum Likelihood (FIML).                     |

# Summary

## What did we do?

- **Binary outcomes are everywhere.** 
  - Standard OLS (the LPM) is a simple starting point but is flawed (out-of-range predictions, bad errors) in some ways, but good (interaction effects, integration with panel data) in others.

- **Probit and Logit**:
  - These are the standard solutions. They are derived from a **latent variable** model and use a non-linear CDF to constrain predictions between 0 and 1.

- **Maximum Likelihood Estimation (MLE)**: 
  - We looked at these models are estimated, MLE, which finds the parameters that make the observed data most probable.

- **Interpretation**: 
  - We found that raw coefficients are not marginal effects, we can use **Average Marginal Effects (AMEs)** to talk about changes in probability. In logit, we can use use **Odds Ratios** for Logit models for a multiplicative interpretation.


# The End

