---
title: "Empirical Economics"
subtitle: "Tutorial 2: Statistics and Probability"
mainfont: Lato
monofont: Ubuntu Mono
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: logo.svg
    css: styles.css
    footer: 'Empirical Economics: Tutorial 2 - Statistics and Probability'
resources:
  - demo.pdf
---

```{r setup}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(gridExtra)
library(reticulate)
use_python("/home/bas/anaconda3/bin/python")
```

# Questions

## Wage Determinants

:::{style="font-size: 0.7em;"}

You are provided with a dataset named `SLEEP75.DTA`. Your task is to investigate the relationship between hourly wages and several explanatory variables using multiple linear regression. The variables of interest are:`lhrwage`- the natural logarithm of the hourly wage, `educ` - years of education, `exper` - years of potential work experience, `union`: a binary variable, where 1 indicates union membership and 0 indicates non-membership, and `male`: a binary variable, where 1 indicates male and 0 indicates female.

  1. Estimate a multiple linear regression model where `lhrwage` is the dependent variable and `educ`, `exper`, `union`, and `male` are the independent variables.
  2. Present the summary of your regression model, including the estimated coefficients, standard errors, t-statistics, and p-values.
  3. Based on your regression output, provide a clear and concise interpretation for the `educ` and `union` coefficients.
  4. Perform a hypothesis test for the `exper` coefficient to determine if it is statistically significant at the 5% significance level. State your null and alternative hypotheses, report the relevant test statistic and p-value from your model output, and conclude whether you reject or fail to reject the null hypothesis. What does this conclusion imply about the relationship between experience and the logarithm of hourly wage in this model?
  
:::

## Heteroskedasticity

:::{style="font-size: 0.7em;"}

Using the same multiple linear regression model from the previous question, where `lhrwage` is regressed on `educ`, `exper`, `union`, and `male`, you will now investigate the presence and consequences of heteroskedasticity.

1. After estimating the OLS model, obtain the residuals.^[In R, you can use the `model$residuals`. In Python, it's `model.resid()`. In Stata, it's `predict res, residuals`. ]

2. Create a scatter plot with the values of `educ` on the x-axis and the squared residuals on the y-axis.

3. Examine the plot. Does the spread of the squared residuals appear to change as the fitted values decrease? Describe the pattern you see and explain why it might suggest the presence of heteroskedasticity.

4. Re-estimate the model, but this time calculate **heteroskedasticity-robust standard errors**. 

5. Compare the "normal" OLS standard errors with the robust standard errors for each of the four coefficients (`educ`, `exper`, `union`, and `male`).

:::

## Omitted Variable Bias 

:::{style="font-size: 0.7em;"}
This is a challenging but crucial derivation. Suppose the *true* population model is a multiple regression:
  $$
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u
  $$
However, you mistakenly estimate a simple regression, omitting $x_2$:
    $$
      y = \gamma_0 + \gamma_1 x_1 + v
    $$
Let $\hat{\gamma}_1$ be the OLS estimate from your incorrect (short) regression. Show that the expected value of this estimator is:

  $$
    E(\hat{\gamma}_1) = \beta_1 + \beta_2 \cdot \delta_1
  $$
  
where $\delta_1$ is the slope coefficient from an auxiliary regression of the omitted variable ($x_2$) on the included variable ($x_1$): $x_2 = \delta_0 + \delta_1 x_1 + \text{error}$.

- *(Hint: Start with the formula for $\hat{\gamma}_1$, substitute the true model for y, and then take the expectation. The term $\beta_2 \cdot \delta_1$ represents the omitted variable bias.)*
- *(Hint: $\frac{\sum(x_{1i}-\bar{x}_1)x_{2i}}{\sum(x_{1i}-\bar{x}_1)^2} = \frac{\sum(x_{1i}-\bar{x}_1)(x_{2i}-\bar{x}_2)}{\sum(x_{1i}-\bar{x}_1)^2}$)*

:::


## Perfect Multicollinearity

The variance of a coefficient estimator in a multiple regression model with two variables ($x_1, x_2$) is given by:

  $$
    Var(\hat{\beta}_1) = \frac{\sigma^2}{SST_1 (1 - R_1^2)}
  $$

where $R_1^2$ is the R-squared from a regression of $x_1$ on $x_2$.

(a) What does it mean for $x_1$ and $x_2$ to have *perfect multicollinearity* in terms of their relationship?

(b) Analytically, what happens to the value of $R_1^2$ under perfect multicollinearity?

(c) Using the variance formula, explain mathematically why it is impossible to calculate the OLS estimate $\hat{\beta}_1$ in this scenario. What happens to the variance of the estimator?
    
## Zero Conditional Mean

The lecture states that the **Zero Conditional Mean assumption** ($E(u|x) = 0$) is the most crucial assumption for causality.

(a) Explain in your own words what this assumption means.

(b) Using the lecture's example of `wage` on `education`, explain why "innate ability" is a potential unobserved factor ($u$) that likely violates this assumption.

(c) If higher ability is positively correlated with both education and wages, in which direction will the OLS estimate for the effect of education on wages ($\hat{\beta}_1$) be biased? Explain your reasoning.


## Mitigating Omitted Variable Bias 

The lecture introduces multiple regression as a way to control for other factors and mitigate omitted variable bias. Let's return to the `wage` on `education` model.

Besides experience (which was added in the lecture), what are two or three other variables you would want to include in the model to get a more credible estimate of the true return to education?

What practical challenges might you face in obtaining data for these variables?
